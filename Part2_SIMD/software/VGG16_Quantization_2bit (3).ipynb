{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os, time, shutil\n",
        "from models import *"
      ],
      "metadata": {
        "id": "8NDC0lV3j125"
      },
      "id": "8NDC0lV3j125",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device Selection\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cudnn.benchmark = True\n",
        "torch.backends.cudnn.fastest = True   # maximize RTX 3080 throughput\n",
        "print(f\"=> Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDEPEaplj4mC",
        "outputId": "07d04886-497a-46ef-f78e-261ede6fe271"
      },
      "id": "eDEPEaplj4mC",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Parameters\n",
        "batch_size = 128\n",
        "epochs = 200\n",
        "lr = 0.1"
      ],
      "metadata": {
        "id": "izeHy1Fbj6jF"
      },
      "id": "izeHy1Fbj6jF",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc = 0\n",
        "save_dir = \"result/VGG16_quant\"\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "LTYfHkaokB4U"
      },
      "id": "LTYfHkaokB4U",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VGG16_quant().to(device)\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"=> Using {torch.cuda.device_count()} GPUs\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "# Fix: Reinitialize the classifier layer with the correct input features\n",
        "# Based on VGG16_quant architecture and CIFAR10 input (32x32 with 5 max-pooling layers),\n",
        "# the spatial dimension becomes 1x1, and the last conv block outputs 16 channels.\n",
        "num_features_after_flattening = 16\n",
        "model.classifier = nn.Linear(num_features_after_flattening, 10).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.7, weight_decay=5e-4)\n",
        "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[25, 40], gamma=0.1)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "scaler = torch.amp.GradScaler(device='cuda') # Updated to recommended syntax\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447],\n",
        "                                 std=[0.247, 0.243, 0.262])\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=256,              # 128 â†’ 256 (fits in 10 GB easily)\n",
        "    shuffle=True,\n",
        "    num_workers=os.cpu_count(),  # use all CPU cores\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True,\n",
        "    prefetch_factor=4,           # overlap data loading with compute\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=512,              # eval can use larger batch\n",
        "    shuffle=False,\n",
        "    num_workers=os.cpu_count(),\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True,\n",
        ")\n",
        "\n",
        "class AverageMeter:\n",
        "    def __init__(self): self.reset()\n",
        "    def reset(self): self.val=self.avg=self.sum=self.count=0\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val; self.sum += val*n; self.count += n; self.avg = self.sum/self.count\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    maxk = max(topk)\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / target.size(0)))\n",
        "    return res\n",
        "\n",
        "def save_checkpoint(state, is_best, fdir):\n",
        "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
        "    torch.save(state, filepath)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    losses, top1 = AverageMeter(), AverageMeter()\n",
        "    start = time.time()\n",
        "\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs = inputs.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        # accuracy in full precision is fine (small overhead)\n",
        "        prec1 = accuracy(outputs, targets)[0]\n",
        "        losses.update(loss.item(), inputs.size(0))\n",
        "        top1.update(prec1.item(), inputs.size(0))\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}] [{i}/{len(train_loader)}] \"\n",
        "                f\"LR {current_lr:.5e}  \"\n",
        "                f\"Loss {losses.val:.4f} ({losses.avg:.4f})  \"\n",
        "                f\"Acc {top1.val:.2f}% ({top1.avg:.2f}%)\"\n",
        "            )\n",
        "\n",
        "    print(\n",
        "        f\" Epoch {epoch} done in {time.time()-start:.1f}s | \"\n",
        "        f\"LR: {current_lr:.5e} | Train Acc: {top1.avg:.2f}% | Loss: {losses.avg:.4f}\"\n",
        "    )\n",
        "\n",
        "def validate(val_loader, model, criterion, epoch):\n",
        "    model.eval()\n",
        "    losses, top1 = AverageMeter(), AverageMeter()\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, targets) in enumerate(val_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            prec1 = accuracy(outputs, targets)[0]\n",
        "            losses.update(loss.item(), inputs.size(0))\n",
        "            top1.update(prec1.item(), inputs.size(0))\n",
        "    print(f\"Validation Epoch {epoch}: Acc {top1.avg:.2f}% | Loss {losses.avg:.4f}\")\n",
        "    return top1.avg"
      ],
      "metadata": {
        "id": "gl9Dj9Q_jmKT"
      },
      "id": "gl9Dj9Q_jmKT",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "for epoch in range(1, epochs+1):\n",
        "    train(train_loader, model, criterion, optimizer, epoch)\n",
        "    val_acc = validate(test_loader, model, criterion, epoch)\n",
        "    scheduler.step()\n",
        "\n",
        "    is_best = val_acc > best_acc\n",
        "    best_acc = max(val_acc, best_acc)\n",
        "\n",
        "    save_checkpoint({\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'best_acc': best_acc,\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }, is_best, save_dir)\n",
        "\n",
        "    print(f\"Epoch {epoch} complete | Best Acc: {best_acc:.2f}%\\n\")\n",
        "\n",
        "print(\"Training completed. Best accuracy: {:.2f}%\".format(best_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hbb9TaX7jYEy",
        "outputId": "32ae8662-5f0a-4df7-c232-b4eb05dea543"
      },
      "id": "Hbb9TaX7jYEy",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2022107039.py:90: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1] [0/196] LR 1.00000e-01  Loss 2.4543 (2.4543)  Acc 12.50% (12.50%)\n",
            "Epoch [1] [100/196] LR 1.00000e-01  Loss 1.9829 (2.1364)  Acc 25.39% (19.06%)\n",
            " Epoch 1 done in 57.7s | LR: 1.00000e-01 | Train Acc: 23.10% | Loss: 2.0231\n",
            "Validation Epoch 1: Acc 28.97% | Loss 2.0224\n",
            "Epoch 1 complete | Best Acc: 28.97%\n",
            "\n",
            "Epoch [2] [0/196] LR 9.99938e-02  Loss 1.8712 (1.8712)  Acc 30.86% (30.86%)\n",
            "Epoch [2] [100/196] LR 9.99938e-02  Loss 1.8153 (1.8319)  Acc 28.52% (30.94%)\n",
            " Epoch 2 done in 21.1s | LR: 9.99938e-02 | Train Acc: 31.95% | Loss: 1.8109\n",
            "Validation Epoch 2: Acc 28.65% | Loss 2.0222\n",
            "Epoch 2 complete | Best Acc: 28.97%\n",
            "\n",
            "Epoch [3] [0/196] LR 9.99753e-02  Loss 1.8150 (1.8150)  Acc 33.20% (33.20%)\n",
            "Epoch [3] [100/196] LR 9.99753e-02  Loss 1.7309 (1.7349)  Acc 41.41% (36.55%)\n",
            " Epoch 3 done in 20.3s | LR: 9.99753e-02 | Train Acc: 37.51% | Loss: 1.7128\n",
            "Validation Epoch 3: Acc 34.74% | Loss 1.8825\n",
            "Epoch 3 complete | Best Acc: 34.74%\n",
            "\n",
            "Epoch [4] [0/196] LR 9.99445e-02  Loss 1.6215 (1.6215)  Acc 42.58% (42.58%)\n",
            "Epoch [4] [100/196] LR 9.99445e-02  Loss 1.6810 (1.6189)  Acc 38.28% (40.88%)\n",
            " Epoch 4 done in 19.7s | LR: 9.99445e-02 | Train Acc: 42.28% | Loss: 1.5875\n",
            "Validation Epoch 4: Acc 38.27% | Loss 1.7405\n",
            "Epoch 4 complete | Best Acc: 38.27%\n",
            "\n",
            "Epoch [5] [0/196] LR 9.99013e-02  Loss 1.5682 (1.5682)  Acc 47.27% (47.27%)\n",
            "Epoch [5] [100/196] LR 9.99013e-02  Loss 1.4291 (1.4837)  Acc 47.27% (46.43%)\n",
            " Epoch 5 done in 20.5s | LR: 9.99013e-02 | Train Acc: 47.10% | Loss: 1.4602\n",
            "Validation Epoch 5: Acc 42.21% | Loss 1.6138\n",
            "Epoch 5 complete | Best Acc: 42.21%\n",
            "\n",
            "Epoch [6] [0/196] LR 9.98459e-02  Loss 1.3770 (1.3770)  Acc 47.27% (47.27%)\n",
            "Epoch [6] [100/196] LR 9.98459e-02  Loss 1.4023 (1.3767)  Acc 50.39% (50.69%)\n",
            " Epoch 6 done in 20.7s | LR: 9.98459e-02 | Train Acc: 51.62% | Loss: 1.3544\n",
            "Validation Epoch 6: Acc 51.00% | Loss 1.4012\n",
            "Epoch 6 complete | Best Acc: 51.00%\n",
            "\n",
            "Epoch [7] [0/196] LR 9.97781e-02  Loss 1.4644 (1.4644)  Acc 45.70% (45.70%)\n",
            "Epoch [7] [100/196] LR 9.97781e-02  Loss 1.2481 (1.2938)  Acc 59.38% (53.75%)\n",
            " Epoch 7 done in 20.8s | LR: 9.97781e-02 | Train Acc: 54.54% | Loss: 1.2731\n",
            "Validation Epoch 7: Acc 54.81% | Loss 1.2984\n",
            "Epoch 7 complete | Best Acc: 54.81%\n",
            "\n",
            "Epoch [8] [0/196] LR 9.96980e-02  Loss 1.2268 (1.2268)  Acc 56.25% (56.25%)\n",
            "Epoch [8] [100/196] LR 9.96980e-02  Loss 1.2938 (1.2074)  Acc 53.12% (56.80%)\n",
            " Epoch 8 done in 20.2s | LR: 9.96980e-02 | Train Acc: 57.58% | Loss: 1.1923\n",
            "Validation Epoch 8: Acc 54.30% | Loss 1.2940\n",
            "Epoch 8 complete | Best Acc: 54.81%\n",
            "\n",
            "Epoch [9] [0/196] LR 9.96057e-02  Loss 1.1379 (1.1379)  Acc 64.84% (64.84%)\n",
            "Epoch [9] [100/196] LR 9.96057e-02  Loss 1.0648 (1.1485)  Acc 61.33% (59.32%)\n",
            " Epoch 9 done in 19.8s | LR: 9.96057e-02 | Train Acc: 60.26% | Loss: 1.1335\n",
            "Validation Epoch 9: Acc 56.24% | Loss 1.3087\n",
            "Epoch 9 complete | Best Acc: 56.24%\n",
            "\n",
            "Epoch [10] [0/196] LR 9.95012e-02  Loss 0.9137 (0.9137)  Acc 67.97% (67.97%)\n",
            "Epoch [10] [100/196] LR 9.95012e-02  Loss 1.0699 (1.0749)  Acc 61.33% (62.50%)\n",
            " Epoch 10 done in 20.1s | LR: 9.95012e-02 | Train Acc: 62.56% | Loss: 1.0709\n",
            "Validation Epoch 10: Acc 58.91% | Loss 1.1969\n",
            "Epoch 10 complete | Best Acc: 58.91%\n",
            "\n",
            "Epoch [11] [0/196] LR 9.93844e-02  Loss 1.1171 (1.1171)  Acc 58.20% (58.20%)\n",
            "Epoch [11] [100/196] LR 9.93844e-02  Loss 1.0157 (1.0240)  Acc 66.41% (64.58%)\n",
            " Epoch 11 done in 20.3s | LR: 9.93844e-02 | Train Acc: 64.58% | Loss: 1.0147\n",
            "Validation Epoch 11: Acc 56.46% | Loss 1.3172\n",
            "Epoch 11 complete | Best Acc: 58.91%\n",
            "\n",
            "Epoch [12] [0/196] LR 9.92555e-02  Loss 0.9607 (0.9607)  Acc 65.62% (65.62%)\n",
            "Epoch [12] [100/196] LR 9.92555e-02  Loss 0.9741 (0.9786)  Acc 64.06% (66.21%)\n",
            " Epoch 12 done in 20.3s | LR: 9.92555e-02 | Train Acc: 66.41% | Loss: 0.9731\n",
            "Validation Epoch 12: Acc 63.56% | Loss 1.0722\n",
            "Epoch 12 complete | Best Acc: 63.56%\n",
            "\n",
            "Epoch [13] [0/196] LR 9.91144e-02  Loss 0.9030 (0.9030)  Acc 69.53% (69.53%)\n",
            "Epoch [13] [100/196] LR 9.91144e-02  Loss 1.0557 (0.9362)  Acc 65.23% (68.01%)\n",
            " Epoch 13 done in 20.1s | LR: 9.91144e-02 | Train Acc: 68.07% | Loss: 0.9303\n",
            "Validation Epoch 13: Acc 62.83% | Loss 1.1220\n",
            "Epoch 13 complete | Best Acc: 63.56%\n",
            "\n",
            "Epoch [14] [0/196] LR 9.89611e-02  Loss 0.9910 (0.9910)  Acc 65.23% (65.23%)\n",
            "Epoch [14] [100/196] LR 9.89611e-02  Loss 0.9458 (0.8936)  Acc 66.41% (69.42%)\n",
            " Epoch 14 done in 19.6s | LR: 9.89611e-02 | Train Acc: 69.63% | Loss: 0.8904\n",
            "Validation Epoch 14: Acc 62.54% | Loss 1.1669\n",
            "Epoch 14 complete | Best Acc: 63.56%\n",
            "\n",
            "Epoch [15] [0/196] LR 9.87958e-02  Loss 0.8036 (0.8036)  Acc 71.88% (71.88%)\n",
            "Epoch [15] [100/196] LR 9.87958e-02  Loss 0.8799 (0.8552)  Acc 72.66% (71.01%)\n",
            " Epoch 15 done in 20.2s | LR: 9.87958e-02 | Train Acc: 70.73% | Loss: 0.8608\n",
            "Validation Epoch 15: Acc 67.32% | Loss 0.9928\n",
            "Epoch 15 complete | Best Acc: 67.32%\n",
            "\n",
            "Epoch [16] [0/196] LR 9.86185e-02  Loss 0.8665 (0.8665)  Acc 70.31% (70.31%)\n",
            "Epoch [16] [100/196] LR 9.86185e-02  Loss 0.7735 (0.8226)  Acc 71.48% (71.91%)\n",
            " Epoch 16 done in 20.9s | LR: 9.86185e-02 | Train Acc: 71.82% | Loss: 0.8235\n",
            "Validation Epoch 16: Acc 64.72% | Loss 1.0629\n",
            "Epoch 16 complete | Best Acc: 67.32%\n",
            "\n",
            "Epoch [17] [0/196] LR 9.84292e-02  Loss 0.7918 (0.7918)  Acc 74.61% (74.61%)\n",
            "Epoch [17] [100/196] LR 9.84292e-02  Loss 0.7661 (0.8069)  Acc 70.70% (72.55%)\n",
            " Epoch 17 done in 20.2s | LR: 9.84292e-02 | Train Acc: 72.73% | Loss: 0.8018\n",
            "Validation Epoch 17: Acc 65.26% | Loss 1.0519\n",
            "Epoch 17 complete | Best Acc: 67.32%\n",
            "\n",
            "Epoch [18] [0/196] LR 9.82279e-02  Loss 0.8591 (0.8591)  Acc 72.27% (72.27%)\n",
            "Epoch [18] [100/196] LR 9.82279e-02  Loss 0.7472 (0.7752)  Acc 75.39% (73.56%)\n",
            " Epoch 18 done in 20.6s | LR: 9.82279e-02 | Train Acc: 73.59% | Loss: 0.7742\n",
            "Validation Epoch 18: Acc 69.13% | Loss 0.9321\n",
            "Epoch 18 complete | Best Acc: 69.13%\n",
            "\n",
            "Epoch [19] [0/196] LR 9.80147e-02  Loss 0.6319 (0.6319)  Acc 76.95% (76.95%)\n",
            "Epoch [19] [100/196] LR 9.80147e-02  Loss 0.7428 (0.7289)  Acc 75.39% (75.41%)\n",
            " Epoch 19 done in 19.7s | LR: 9.80147e-02 | Train Acc: 74.79% | Loss: 0.7422\n",
            "Validation Epoch 19: Acc 70.19% | Loss 0.9044\n",
            "Epoch 19 complete | Best Acc: 70.19%\n",
            "\n",
            "Epoch [20] [0/196] LR 9.77897e-02  Loss 0.6904 (0.6904)  Acc 75.39% (75.39%)\n",
            "Epoch [20] [100/196] LR 9.77897e-02  Loss 0.7956 (0.7159)  Acc 72.27% (75.64%)\n",
            " Epoch 20 done in 20.0s | LR: 9.77897e-02 | Train Acc: 75.36% | Loss: 0.7232\n",
            "Validation Epoch 20: Acc 69.78% | Loss 0.9346\n",
            "Epoch 20 complete | Best Acc: 70.19%\n",
            "\n",
            "Epoch [21] [0/196] LR 9.75528e-02  Loss 0.7959 (0.7959)  Acc 74.22% (74.22%)\n",
            "Epoch [21] [100/196] LR 9.75528e-02  Loss 0.7414 (0.7022)  Acc 74.22% (76.23%)\n",
            " Epoch 21 done in 20.0s | LR: 9.75528e-02 | Train Acc: 75.96% | Loss: 0.7086\n",
            "Validation Epoch 21: Acc 72.53% | Loss 0.8189\n",
            "Epoch 21 complete | Best Acc: 72.53%\n",
            "\n",
            "Epoch [22] [0/196] LR 9.73043e-02  Loss 0.6765 (0.6765)  Acc 79.30% (79.30%)\n",
            "Epoch [22] [100/196] LR 9.73043e-02  Loss 0.7233 (0.6910)  Acc 74.61% (76.56%)\n",
            " Epoch 22 done in 20.8s | LR: 9.73043e-02 | Train Acc: 76.45% | Loss: 0.6963\n",
            "Validation Epoch 22: Acc 67.06% | Loss 1.0104\n",
            "Epoch 22 complete | Best Acc: 72.53%\n",
            "\n",
            "Epoch [23] [0/196] LR 9.70440e-02  Loss 0.7855 (0.7855)  Acc 75.39% (75.39%)\n",
            "Epoch [23] [100/196] LR 9.70440e-02  Loss 0.6079 (0.6777)  Acc 80.08% (76.91%)\n",
            " Epoch 23 done in 20.6s | LR: 9.70440e-02 | Train Acc: 76.92% | Loss: 0.6772\n",
            "Validation Epoch 23: Acc 66.97% | Loss 1.0712\n",
            "Epoch 23 complete | Best Acc: 72.53%\n",
            "\n",
            "Epoch [24] [0/196] LR 9.67722e-02  Loss 0.6941 (0.6941)  Acc 76.56% (76.56%)\n",
            "Epoch [24] [100/196] LR 9.67722e-02  Loss 0.6137 (0.6547)  Acc 80.08% (77.78%)\n",
            " Epoch 24 done in 20.3s | LR: 9.67722e-02 | Train Acc: 77.87% | Loss: 0.6545\n",
            "Validation Epoch 24: Acc 72.92% | Loss 0.8141\n",
            "Epoch 24 complete | Best Acc: 72.92%\n",
            "\n",
            "Epoch [25] [0/196] LR 9.64888e-02  Loss 0.7358 (0.7358)  Acc 74.22% (74.22%)\n",
            "Epoch [25] [100/196] LR 9.64888e-02  Loss 0.6550 (0.6297)  Acc 79.69% (78.54%)\n",
            " Epoch 25 done in 19.8s | LR: 9.64888e-02 | Train Acc: 78.42% | Loss: 0.6322\n",
            "Validation Epoch 25: Acc 75.45% | Loss 0.7389\n",
            "Epoch 25 complete | Best Acc: 75.45%\n",
            "\n",
            "Epoch [26] [0/196] LR 9.61940e-02  Loss 0.6361 (0.6361)  Acc 80.08% (80.08%)\n",
            "Epoch [26] [100/196] LR 9.61940e-02  Loss 0.7138 (0.6040)  Acc 75.39% (79.57%)\n",
            " Epoch 26 done in 19.3s | LR: 9.61940e-02 | Train Acc: 79.51% | Loss: 0.6068\n",
            "Validation Epoch 26: Acc 75.21% | Loss 0.7359\n",
            "Epoch 26 complete | Best Acc: 75.45%\n",
            "\n",
            "Epoch [27] [0/196] LR 9.58877e-02  Loss 0.5766 (0.5766)  Acc 77.34% (77.34%)\n",
            "Epoch [27] [100/196] LR 9.58877e-02  Loss 0.5138 (0.5902)  Acc 80.86% (80.02%)\n",
            " Epoch 27 done in 19.7s | LR: 9.58877e-02 | Train Acc: 80.16% | Loss: 0.5905\n",
            "Validation Epoch 27: Acc 75.07% | Loss 0.7487\n",
            "Epoch 27 complete | Best Acc: 75.45%\n",
            "\n",
            "Epoch [28] [0/196] LR 9.55702e-02  Loss 0.5992 (0.5992)  Acc 77.73% (77.73%)\n",
            "Epoch [28] [100/196] LR 9.55702e-02  Loss 0.5778 (0.5635)  Acc 80.86% (80.89%)\n",
            " Epoch 28 done in 20.2s | LR: 9.55702e-02 | Train Acc: 80.83% | Loss: 0.5666\n",
            "Validation Epoch 28: Acc 77.08% | Loss 0.6711\n",
            "Epoch 28 complete | Best Acc: 77.08%\n",
            "\n",
            "Epoch [29] [0/196] LR 9.52414e-02  Loss 0.6734 (0.6734)  Acc 78.12% (78.12%)\n",
            "Epoch [29] [100/196] LR 9.52414e-02  Loss 0.5709 (0.5583)  Acc 78.91% (80.98%)\n",
            " Epoch 29 done in 20.3s | LR: 9.52414e-02 | Train Acc: 81.13% | Loss: 0.5557\n",
            "Validation Epoch 29: Acc 72.93% | Loss 0.8054\n",
            "Epoch 29 complete | Best Acc: 77.08%\n",
            "\n",
            "Epoch [30] [0/196] LR 9.49014e-02  Loss 0.5024 (0.5024)  Acc 83.98% (83.98%)\n",
            "Epoch [30] [100/196] LR 9.49014e-02  Loss 0.4760 (0.5366)  Acc 83.98% (81.87%)\n",
            " Epoch 30 done in 19.3s | LR: 9.49014e-02 | Train Acc: 81.89% | Loss: 0.5359\n",
            "Validation Epoch 30: Acc 65.56% | Loss 1.1354\n",
            "Epoch 30 complete | Best Acc: 77.08%\n",
            "\n",
            "Epoch [31] [0/196] LR 9.45503e-02  Loss 0.4493 (0.4493)  Acc 85.94% (85.94%)\n",
            "Epoch [31] [100/196] LR 9.45503e-02  Loss 0.4149 (0.5095)  Acc 85.94% (82.90%)\n",
            " Epoch 31 done in 20.0s | LR: 9.45503e-02 | Train Acc: 82.44% | Loss: 0.5184\n",
            "Validation Epoch 31: Acc 74.32% | Loss 0.8176\n",
            "Epoch 31 complete | Best Acc: 77.08%\n",
            "\n",
            "Epoch [32] [0/196] LR 9.41883e-02  Loss 0.6284 (0.6284)  Acc 78.91% (78.91%)\n",
            "Epoch [32] [100/196] LR 9.41883e-02  Loss 0.5012 (0.5104)  Acc 83.59% (82.88%)\n",
            " Epoch 32 done in 20.3s | LR: 9.41883e-02 | Train Acc: 82.85% | Loss: 0.5059\n",
            "Validation Epoch 32: Acc 79.83% | Loss 0.6049\n",
            "Epoch 32 complete | Best Acc: 79.83%\n",
            "\n",
            "Epoch [33] [0/196] LR 9.38153e-02  Loss 0.4355 (0.4355)  Acc 81.64% (81.64%)\n",
            "Epoch [33] [100/196] LR 9.38153e-02  Loss 0.4521 (0.4869)  Acc 83.98% (83.48%)\n",
            " Epoch 33 done in 20.5s | LR: 9.38153e-02 | Train Acc: 83.51% | Loss: 0.4862\n",
            "Validation Epoch 33: Acc 78.66% | Loss 0.6536\n",
            "Epoch 33 complete | Best Acc: 79.83%\n",
            "\n",
            "Epoch [34] [0/196] LR 9.34316e-02  Loss 0.4495 (0.4495)  Acc 86.72% (86.72%)\n",
            "Epoch [34] [100/196] LR 9.34316e-02  Loss 0.3664 (0.4693)  Acc 86.72% (84.26%)\n",
            " Epoch 34 done in 20.4s | LR: 9.34316e-02 | Train Acc: 84.06% | Loss: 0.4696\n",
            "Validation Epoch 34: Acc 73.02% | Loss 0.8490\n",
            "Epoch 34 complete | Best Acc: 79.83%\n",
            "\n",
            "Epoch [35] [0/196] LR 9.30371e-02  Loss 0.4170 (0.4170)  Acc 86.72% (86.72%)\n",
            "Epoch [35] [100/196] LR 9.30371e-02  Loss 0.3983 (0.4502)  Acc 87.50% (84.70%)\n",
            " Epoch 35 done in 20.4s | LR: 9.30371e-02 | Train Acc: 84.53% | Loss: 0.4599\n",
            "Validation Epoch 35: Acc 79.88% | Loss 0.6108\n",
            "Epoch 35 complete | Best Acc: 79.88%\n",
            "\n",
            "Epoch [36] [0/196] LR 9.26320e-02  Loss 0.4259 (0.4259)  Acc 87.50% (87.50%)\n",
            "Epoch [36] [100/196] LR 9.26320e-02  Loss 0.4373 (0.4407)  Acc 85.55% (85.12%)\n",
            " Epoch 36 done in 19.6s | LR: 9.26320e-02 | Train Acc: 85.01% | Loss: 0.4447\n",
            "Validation Epoch 36: Acc 79.86% | Loss 0.5990\n",
            "Epoch 36 complete | Best Acc: 79.88%\n",
            "\n",
            "Epoch [37] [0/196] LR 9.22164e-02  Loss 0.4600 (0.4600)  Acc 83.98% (83.98%)\n",
            "Epoch [37] [100/196] LR 9.22164e-02  Loss 0.3494 (0.4342)  Acc 87.89% (85.35%)\n",
            " Epoch 37 done in 19.4s | LR: 9.22164e-02 | Train Acc: 85.25% | Loss: 0.4403\n",
            "Validation Epoch 37: Acc 81.49% | Loss 0.5584\n",
            "Epoch 37 complete | Best Acc: 81.49%\n",
            "\n",
            "Epoch [38] [0/196] LR 9.17904e-02  Loss 0.4339 (0.4339)  Acc 85.55% (85.55%)\n",
            "Epoch [38] [100/196] LR 9.17904e-02  Loss 0.3609 (0.4208)  Acc 87.50% (85.73%)\n",
            " Epoch 38 done in 20.3s | LR: 9.17904e-02 | Train Acc: 85.68% | Loss: 0.4236\n",
            "Validation Epoch 38: Acc 81.57% | Loss 0.5692\n",
            "Epoch 38 complete | Best Acc: 81.57%\n",
            "\n",
            "Epoch [39] [0/196] LR 9.13540e-02  Loss 0.3170 (0.3170)  Acc 91.02% (91.02%)\n",
            "Epoch [39] [100/196] LR 9.13540e-02  Loss 0.3732 (0.4071)  Acc 87.11% (86.32%)\n",
            " Epoch 39 done in 20.4s | LR: 9.13540e-02 | Train Acc: 85.97% | Loss: 0.4168\n",
            "Validation Epoch 39: Acc 76.71% | Loss 0.7458\n",
            "Epoch 39 complete | Best Acc: 81.57%\n",
            "\n",
            "Epoch [40] [0/196] LR 9.09075e-02  Loss 0.3311 (0.3311)  Acc 89.45% (89.45%)\n",
            "Epoch [40] [100/196] LR 9.09075e-02  Loss 0.3743 (0.4072)  Acc 86.72% (86.36%)\n",
            " Epoch 40 done in 20.5s | LR: 9.09075e-02 | Train Acc: 86.41% | Loss: 0.4053\n",
            "Validation Epoch 40: Acc 77.90% | Loss 0.6913\n",
            "Epoch 40 complete | Best Acc: 81.57%\n",
            "\n",
            "Epoch [41] [0/196] LR 9.04508e-02  Loss 0.3872 (0.3872)  Acc 85.94% (85.94%)\n",
            "Epoch [41] [100/196] LR 9.04508e-02  Loss 0.4377 (0.3920)  Acc 88.28% (86.86%)\n",
            " Epoch 41 done in 20.7s | LR: 9.04508e-02 | Train Acc: 86.75% | Loss: 0.3949\n",
            "Validation Epoch 41: Acc 81.50% | Loss 0.5561\n",
            "Epoch 41 complete | Best Acc: 81.57%\n",
            "\n",
            "Epoch [42] [0/196] LR 8.99842e-02  Loss 0.3118 (0.3118)  Acc 87.89% (87.89%)\n",
            "Epoch [42] [100/196] LR 8.99842e-02  Loss 0.4133 (0.3775)  Acc 85.16% (87.30%)\n",
            " Epoch 42 done in 20.0s | LR: 8.99842e-02 | Train Acc: 87.28% | Loss: 0.3788\n",
            "Validation Epoch 42: Acc 81.01% | Loss 0.6096\n",
            "Epoch 42 complete | Best Acc: 81.57%\n",
            "\n",
            "Epoch [43] [0/196] LR 8.95078e-02  Loss 0.3931 (0.3931)  Acc 87.11% (87.11%)\n",
            "Epoch [43] [100/196] LR 8.95078e-02  Loss 0.3496 (0.3662)  Acc 86.72% (87.59%)\n",
            " Epoch 43 done in 19.5s | LR: 8.95078e-02 | Train Acc: 87.58% | Loss: 0.3702\n",
            "Validation Epoch 43: Acc 81.19% | Loss 0.6127\n",
            "Epoch 43 complete | Best Acc: 81.57%\n",
            "\n",
            "Epoch [44] [0/196] LR 8.90215e-02  Loss 0.3644 (0.3644)  Acc 87.50% (87.50%)\n",
            "Epoch [44] [100/196] LR 8.90215e-02  Loss 0.3636 (0.3627)  Acc 86.33% (87.62%)\n",
            " Epoch 44 done in 20.5s | LR: 8.90215e-02 | Train Acc: 87.76% | Loss: 0.3637\n",
            "Validation Epoch 44: Acc 81.54% | Loss 0.5534\n",
            "Epoch 44 complete | Best Acc: 81.57%\n",
            "\n",
            "Epoch [45] [0/196] LR 8.85257e-02  Loss 0.3666 (0.3666)  Acc 87.89% (87.89%)\n",
            "Epoch [45] [100/196] LR 8.85257e-02  Loss 0.2825 (0.3498)  Acc 92.19% (88.31%)\n",
            " Epoch 45 done in 20.6s | LR: 8.85257e-02 | Train Acc: 88.25% | Loss: 0.3509\n",
            "Validation Epoch 45: Acc 79.24% | Loss 0.6546\n",
            "Epoch 45 complete | Best Acc: 81.57%\n",
            "\n",
            "Epoch [46] [0/196] LR 8.80203e-02  Loss 0.2921 (0.2921)  Acc 90.23% (90.23%)\n",
            "Epoch [46] [100/196] LR 8.80203e-02  Loss 0.3018 (0.3393)  Acc 89.06% (88.43%)\n",
            " Epoch 46 done in 20.6s | LR: 8.80203e-02 | Train Acc: 88.40% | Loss: 0.3435\n",
            "Validation Epoch 46: Acc 81.30% | Loss 0.5831\n",
            "Epoch 46 complete | Best Acc: 81.57%\n",
            "\n",
            "Epoch [47] [0/196] LR 8.75056e-02  Loss 0.2827 (0.2827)  Acc 91.41% (91.41%)\n",
            "Epoch [47] [100/196] LR 8.75056e-02  Loss 0.3136 (0.3215)  Acc 89.45% (89.04%)\n",
            " Epoch 47 done in 20.4s | LR: 8.75056e-02 | Train Acc: 88.72% | Loss: 0.3340\n",
            "Validation Epoch 47: Acc 79.07% | Loss 0.6705\n",
            "Epoch 47 complete | Best Acc: 81.57%\n",
            "\n",
            "Epoch [48] [0/196] LR 8.69816e-02  Loss 0.2977 (0.2977)  Acc 89.84% (89.84%)\n",
            "Epoch [48] [100/196] LR 8.69816e-02  Loss 0.2972 (0.3187)  Acc 90.23% (89.43%)\n",
            " Epoch 48 done in 20.0s | LR: 8.69816e-02 | Train Acc: 89.20% | Loss: 0.3227\n",
            "Validation Epoch 48: Acc 80.65% | Loss 0.6245\n",
            "Epoch 48 complete | Best Acc: 81.57%\n",
            "\n",
            "Epoch [49] [0/196] LR 8.64484e-02  Loss 0.3174 (0.3174)  Acc 88.67% (88.67%)\n",
            "Epoch [49] [100/196] LR 8.64484e-02  Loss 0.3368 (0.3193)  Acc 88.28% (89.28%)\n",
            " Epoch 49 done in 19.7s | LR: 8.64484e-02 | Train Acc: 89.02% | Loss: 0.3280\n",
            "Validation Epoch 49: Acc 79.94% | Loss 0.6550\n",
            "Epoch 49 complete | Best Acc: 81.57%\n",
            "\n",
            "Epoch [50] [0/196] LR 8.59063e-02  Loss 0.2397 (0.2397)  Acc 90.23% (90.23%)\n",
            "Epoch [50] [100/196] LR 8.59063e-02  Loss 0.2816 (0.3065)  Acc 89.84% (89.60%)\n",
            " Epoch 50 done in 20.2s | LR: 8.59063e-02 | Train Acc: 89.42% | Loss: 0.3149\n",
            "Validation Epoch 50: Acc 84.20% | Loss 0.4844\n",
            "Epoch 50 complete | Best Acc: 84.20%\n",
            "\n",
            "Epoch [51] [0/196] LR 8.53553e-02  Loss 0.3055 (0.3055)  Acc 91.02% (91.02%)\n",
            "Epoch [51] [100/196] LR 8.53553e-02  Loss 0.2962 (0.3074)  Acc 88.67% (89.65%)\n",
            " Epoch 51 done in 21.0s | LR: 8.53553e-02 | Train Acc: 89.45% | Loss: 0.3143\n",
            "Validation Epoch 51: Acc 83.47% | Loss 0.5046\n",
            "Epoch 51 complete | Best Acc: 84.20%\n",
            "\n",
            "Epoch [52] [0/196] LR 8.47956e-02  Loss 0.2668 (0.2668)  Acc 91.02% (91.02%)\n",
            "Epoch [52] [100/196] LR 8.47956e-02  Loss 0.2887 (0.2864)  Acc 89.84% (90.51%)\n",
            " Epoch 52 done in 20.7s | LR: 8.47956e-02 | Train Acc: 90.10% | Loss: 0.2970\n",
            "Validation Epoch 52: Acc 84.57% | Loss 0.4882\n",
            "Epoch 52 complete | Best Acc: 84.57%\n",
            "\n",
            "Epoch [53] [0/196] LR 8.42274e-02  Loss 0.2671 (0.2671)  Acc 90.62% (90.62%)\n",
            "Epoch [53] [100/196] LR 8.42274e-02  Loss 0.3627 (0.2902)  Acc 89.06% (90.14%)\n",
            " Epoch 53 done in 20.1s | LR: 8.42274e-02 | Train Acc: 89.95% | Loss: 0.2976\n",
            "Validation Epoch 53: Acc 82.81% | Loss 0.5335\n",
            "Epoch 53 complete | Best Acc: 84.57%\n",
            "\n",
            "Epoch [54] [0/196] LR 8.36506e-02  Loss 0.2364 (0.2364)  Acc 91.80% (91.80%)\n",
            "Epoch [54] [100/196] LR 8.36506e-02  Loss 0.3282 (0.2846)  Acc 89.06% (90.28%)\n",
            " Epoch 54 done in 19.8s | LR: 8.36506e-02 | Train Acc: 90.26% | Loss: 0.2850\n",
            "Validation Epoch 54: Acc 76.53% | Loss 0.8188\n",
            "Epoch 54 complete | Best Acc: 84.57%\n",
            "\n",
            "Epoch [55] [0/196] LR 8.30656e-02  Loss 0.3189 (0.3189)  Acc 87.50% (87.50%)\n",
            "Epoch [55] [100/196] LR 8.30656e-02  Loss 0.2673 (0.2831)  Acc 90.62% (90.49%)\n",
            " Epoch 55 done in 19.7s | LR: 8.30656e-02 | Train Acc: 90.35% | Loss: 0.2870\n",
            "Validation Epoch 55: Acc 81.30% | Loss 0.6168\n",
            "Epoch 55 complete | Best Acc: 84.57%\n",
            "\n",
            "Epoch [56] [0/196] LR 8.24724e-02  Loss 0.2852 (0.2852)  Acc 90.62% (90.62%)\n",
            "Epoch [56] [100/196] LR 8.24724e-02  Loss 0.2683 (0.2732)  Acc 91.02% (90.69%)\n",
            " Epoch 56 done in 20.5s | LR: 8.24724e-02 | Train Acc: 90.60% | Loss: 0.2768\n",
            "Validation Epoch 56: Acc 79.95% | Loss 0.6875\n",
            "Epoch 56 complete | Best Acc: 84.57%\n",
            "\n",
            "Epoch [57] [0/196] LR 8.18712e-02  Loss 0.2946 (0.2946)  Acc 91.41% (91.41%)\n",
            "Epoch [57] [100/196] LR 8.18712e-02  Loss 0.2899 (0.2644)  Acc 89.06% (91.14%)\n",
            " Epoch 57 done in 20.5s | LR: 8.18712e-02 | Train Acc: 90.66% | Loss: 0.2719\n",
            "Validation Epoch 57: Acc 83.45% | Loss 0.5623\n",
            "Epoch 57 complete | Best Acc: 84.57%\n",
            "\n",
            "Epoch [58] [0/196] LR 8.12621e-02  Loss 0.2762 (0.2762)  Acc 91.02% (91.02%)\n",
            "Epoch [58] [100/196] LR 8.12621e-02  Loss 0.2543 (0.2557)  Acc 91.80% (91.30%)\n",
            " Epoch 58 done in 20.8s | LR: 8.12621e-02 | Train Acc: 91.02% | Loss: 0.2634\n",
            "Validation Epoch 58: Acc 82.52% | Loss 0.5733\n",
            "Epoch 58 complete | Best Acc: 84.57%\n",
            "\n",
            "Epoch [59] [0/196] LR 8.06454e-02  Loss 0.2322 (0.2322)  Acc 92.97% (92.97%)\n",
            "Epoch [59] [100/196] LR 8.06454e-02  Loss 0.2694 (0.2506)  Acc 90.23% (91.51%)\n",
            " Epoch 59 done in 19.7s | LR: 8.06454e-02 | Train Acc: 91.30% | Loss: 0.2578\n",
            "Validation Epoch 59: Acc 79.85% | Loss 0.6663\n",
            "Epoch 59 complete | Best Acc: 84.57%\n",
            "\n",
            "Epoch [60] [0/196] LR 8.00210e-02  Loss 0.2059 (0.2059)  Acc 93.36% (93.36%)\n",
            "Epoch [60] [100/196] LR 8.00210e-02  Loss 0.2499 (0.2524)  Acc 91.80% (91.38%)\n",
            " Epoch 60 done in 19.9s | LR: 8.00210e-02 | Train Acc: 91.33% | Loss: 0.2551\n",
            "Validation Epoch 60: Acc 83.12% | Loss 0.5429\n",
            "Epoch 60 complete | Best Acc: 84.57%\n",
            "\n",
            "Epoch [61] [0/196] LR 7.93893e-02  Loss 0.1896 (0.1896)  Acc 94.53% (94.53%)\n",
            "Epoch [61] [100/196] LR 7.93893e-02  Loss 0.2093 (0.2474)  Acc 91.41% (91.73%)\n",
            " Epoch 61 done in 19.7s | LR: 7.93893e-02 | Train Acc: 91.43% | Loss: 0.2541\n",
            "Validation Epoch 61: Acc 79.42% | Loss 0.6944\n",
            "Epoch 61 complete | Best Acc: 84.57%\n",
            "\n",
            "Epoch [62] [0/196] LR 7.87503e-02  Loss 0.2567 (0.2567)  Acc 91.80% (91.80%)\n",
            "Epoch [62] [100/196] LR 7.87503e-02  Loss 0.2598 (0.2454)  Acc 92.58% (91.69%)\n",
            " Epoch 62 done in 20.0s | LR: 7.87503e-02 | Train Acc: 91.59% | Loss: 0.2493\n",
            "Validation Epoch 62: Acc 84.05% | Loss 0.5134\n",
            "Epoch 62 complete | Best Acc: 84.57%\n",
            "\n",
            "Epoch [63] [0/196] LR 7.81042e-02  Loss 0.2610 (0.2610)  Acc 90.62% (90.62%)\n",
            "Epoch [63] [100/196] LR 7.81042e-02  Loss 0.2022 (0.2411)  Acc 93.36% (91.82%)\n",
            " Epoch 63 done in 20.2s | LR: 7.81042e-02 | Train Acc: 91.70% | Loss: 0.2434\n",
            "Validation Epoch 63: Acc 84.22% | Loss 0.5147\n",
            "Epoch 63 complete | Best Acc: 84.57%\n",
            "\n",
            "Epoch [64] [0/196] LR 7.74511e-02  Loss 0.2294 (0.2294)  Acc 92.97% (92.97%)\n",
            "Epoch [64] [100/196] LR 7.74511e-02  Loss 0.1850 (0.2302)  Acc 94.53% (92.20%)\n",
            " Epoch 64 done in 19.5s | LR: 7.74511e-02 | Train Acc: 91.99% | Loss: 0.2393\n",
            "Validation Epoch 64: Acc 78.06% | Loss 0.7430\n",
            "Epoch 64 complete | Best Acc: 84.57%\n",
            "\n",
            "Epoch [65] [0/196] LR 7.67913e-02  Loss 0.2519 (0.2519)  Acc 90.62% (90.62%)\n",
            "Epoch [65] [100/196] LR 7.67913e-02  Loss 0.3178 (0.2256)  Acc 89.84% (92.34%)\n",
            " Epoch 65 done in 18.7s | LR: 7.67913e-02 | Train Acc: 92.12% | Loss: 0.2333\n",
            "Validation Epoch 65: Acc 82.97% | Loss 0.5638\n",
            "Epoch 65 complete | Best Acc: 84.57%\n",
            "\n",
            "Epoch [66] [0/196] LR 7.61249e-02  Loss 0.2621 (0.2621)  Acc 92.19% (92.19%)\n",
            "Epoch [66] [100/196] LR 7.61249e-02  Loss 0.1821 (0.2217)  Acc 93.75% (92.52%)\n",
            " Epoch 66 done in 19.9s | LR: 7.61249e-02 | Train Acc: 92.38% | Loss: 0.2226\n",
            "Validation Epoch 66: Acc 84.27% | Loss 0.5331\n",
            "Epoch 66 complete | Best Acc: 84.57%\n",
            "\n",
            "Epoch [67] [0/196] LR 7.54521e-02  Loss 0.2011 (0.2011)  Acc 93.36% (93.36%)\n",
            "Epoch [67] [100/196] LR 7.54521e-02  Loss 0.2217 (0.2293)  Acc 92.58% (92.14%)\n",
            " Epoch 67 done in 20.8s | LR: 7.54521e-02 | Train Acc: 92.27% | Loss: 0.2268\n",
            "Validation Epoch 67: Acc 83.80% | Loss 0.5412\n",
            "Epoch 67 complete | Best Acc: 84.57%\n",
            "\n",
            "Epoch [68] [0/196] LR 7.47729e-02  Loss 0.1606 (0.1606)  Acc 95.31% (95.31%)\n",
            "Epoch [68] [100/196] LR 7.47729e-02  Loss 0.1521 (0.2226)  Acc 94.14% (92.33%)\n",
            " Epoch 68 done in 20.6s | LR: 7.47729e-02 | Train Acc: 92.49% | Loss: 0.2179\n",
            "Validation Epoch 68: Acc 80.84% | Loss 0.6846\n",
            "Epoch 68 complete | Best Acc: 84.57%\n",
            "\n",
            "Epoch [69] [0/196] LR 7.40877e-02  Loss 0.2503 (0.2503)  Acc 89.45% (89.45%)\n",
            "Epoch [69] [100/196] LR 7.40877e-02  Loss 0.2155 (0.2198)  Acc 93.75% (92.61%)\n",
            " Epoch 69 done in 20.4s | LR: 7.40877e-02 | Train Acc: 92.65% | Loss: 0.2208\n",
            "Validation Epoch 69: Acc 84.08% | Loss 0.5233\n",
            "Epoch 69 complete | Best Acc: 84.57%\n",
            "\n",
            "Epoch [70] [0/196] LR 7.33965e-02  Loss 0.1724 (0.1724)  Acc 94.14% (94.14%)\n",
            "Epoch [70] [100/196] LR 7.33965e-02  Loss 0.1726 (0.2093)  Acc 94.92% (93.00%)\n",
            " Epoch 70 done in 20.0s | LR: 7.33965e-02 | Train Acc: 92.75% | Loss: 0.2167\n",
            "Validation Epoch 70: Acc 84.00% | Loss 0.5543\n",
            "Epoch 70 complete | Best Acc: 84.57%\n",
            "\n",
            "Epoch [71] [0/196] LR 7.26995e-02  Loss 0.2272 (0.2272)  Acc 91.80% (91.80%)\n",
            "Epoch [71] [100/196] LR 7.26995e-02  Loss 0.2066 (0.2047)  Acc 92.97% (93.17%)\n",
            " Epoch 71 done in 20.0s | LR: 7.26995e-02 | Train Acc: 92.97% | Loss: 0.2099\n",
            "Validation Epoch 71: Acc 83.96% | Loss 0.5328\n",
            "Epoch 71 complete | Best Acc: 84.57%\n",
            "\n",
            "Epoch [72] [0/196] LR 7.19970e-02  Loss 0.2546 (0.2546)  Acc 90.62% (90.62%)\n",
            "Epoch [72] [100/196] LR 7.19970e-02  Loss 0.2302 (0.2008)  Acc 90.23% (93.31%)\n",
            " Epoch 72 done in 20.1s | LR: 7.19970e-02 | Train Acc: 93.15% | Loss: 0.2039\n",
            "Validation Epoch 72: Acc 85.13% | Loss 0.5236\n",
            "Epoch 72 complete | Best Acc: 85.13%\n",
            "\n",
            "Epoch [73] [0/196] LR 7.12890e-02  Loss 0.1100 (0.1100)  Acc 95.70% (95.70%)\n",
            "Epoch [73] [100/196] LR 7.12890e-02  Loss 0.1946 (0.1961)  Acc 93.75% (93.44%)\n",
            " Epoch 73 done in 20.9s | LR: 7.12890e-02 | Train Acc: 93.35% | Loss: 0.2004\n",
            "Validation Epoch 73: Acc 84.66% | Loss 0.5123\n",
            "Epoch 73 complete | Best Acc: 85.13%\n",
            "\n",
            "Epoch [74] [0/196] LR 7.05757e-02  Loss 0.1969 (0.1969)  Acc 93.36% (93.36%)\n",
            "Epoch [74] [100/196] LR 7.05757e-02  Loss 0.1485 (0.1945)  Acc 96.48% (93.46%)\n",
            " Epoch 74 done in 20.5s | LR: 7.05757e-02 | Train Acc: 93.18% | Loss: 0.2009\n",
            "Validation Epoch 74: Acc 86.37% | Loss 0.4525\n",
            "Epoch 74 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [75] [0/196] LR 6.98574e-02  Loss 0.2118 (0.2118)  Acc 93.36% (93.36%)\n",
            "Epoch [75] [100/196] LR 6.98574e-02  Loss 0.1946 (0.1899)  Acc 92.97% (93.42%)\n",
            " Epoch 75 done in 20.5s | LR: 6.98574e-02 | Train Acc: 93.27% | Loss: 0.1965\n",
            "Validation Epoch 75: Acc 82.00% | Loss 0.6149\n",
            "Epoch 75 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [76] [0/196] LR 6.91342e-02  Loss 0.1710 (0.1710)  Acc 94.14% (94.14%)\n",
            "Epoch [76] [100/196] LR 6.91342e-02  Loss 0.1726 (0.1890)  Acc 93.75% (93.65%)\n",
            " Epoch 76 done in 19.3s | LR: 6.91342e-02 | Train Acc: 93.44% | Loss: 0.1939\n",
            "Validation Epoch 76: Acc 85.07% | Loss 0.4955\n",
            "Epoch 76 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [77] [0/196] LR 6.84062e-02  Loss 0.1768 (0.1768)  Acc 93.75% (93.75%)\n",
            "Epoch [77] [100/196] LR 6.84062e-02  Loss 0.1887 (0.1832)  Acc 95.31% (93.64%)\n",
            " Epoch 77 done in 19.8s | LR: 6.84062e-02 | Train Acc: 93.53% | Loss: 0.1881\n",
            "Validation Epoch 77: Acc 84.22% | Loss 0.5428\n",
            "Epoch 77 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [78] [0/196] LR 6.76737e-02  Loss 0.1600 (0.1600)  Acc 94.14% (94.14%)\n",
            "Epoch [78] [100/196] LR 6.76737e-02  Loss 0.1765 (0.1827)  Acc 95.31% (93.87%)\n",
            " Epoch 78 done in 20.5s | LR: 6.76737e-02 | Train Acc: 93.72% | Loss: 0.1868\n",
            "Validation Epoch 78: Acc 84.03% | Loss 0.5451\n",
            "Epoch 78 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [79] [0/196] LR 6.69369e-02  Loss 0.1701 (0.1701)  Acc 93.36% (93.36%)\n",
            "Epoch [79] [100/196] LR 6.69369e-02  Loss 0.1779 (0.1735)  Acc 93.36% (94.21%)\n",
            " Epoch 79 done in 20.4s | LR: 6.69369e-02 | Train Acc: 94.01% | Loss: 0.1795\n",
            "Validation Epoch 79: Acc 84.82% | Loss 0.5088\n",
            "Epoch 79 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [80] [0/196] LR 6.61959e-02  Loss 0.1461 (0.1461)  Acc 95.31% (95.31%)\n",
            "Epoch [80] [100/196] LR 6.61959e-02  Loss 0.1209 (0.1775)  Acc 95.31% (93.97%)\n",
            " Epoch 80 done in 20.5s | LR: 6.61959e-02 | Train Acc: 93.74% | Loss: 0.1835\n",
            "Validation Epoch 80: Acc 84.44% | Loss 0.5330\n",
            "Epoch 80 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [81] [0/196] LR 6.54508e-02  Loss 0.1676 (0.1676)  Acc 93.75% (93.75%)\n",
            "Epoch [81] [100/196] LR 6.54508e-02  Loss 0.1648 (0.1720)  Acc 94.92% (94.18%)\n",
            " Epoch 81 done in 19.3s | LR: 6.54508e-02 | Train Acc: 94.00% | Loss: 0.1764\n",
            "Validation Epoch 81: Acc 83.01% | Loss 0.6068\n",
            "Epoch 81 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [82] [0/196] LR 6.47020e-02  Loss 0.1844 (0.1844)  Acc 93.75% (93.75%)\n",
            "Epoch [82] [100/196] LR 6.47020e-02  Loss 0.2096 (0.1731)  Acc 92.97% (94.22%)\n",
            " Epoch 82 done in 19.4s | LR: 6.47020e-02 | Train Acc: 94.10% | Loss: 0.1761\n",
            "Validation Epoch 82: Acc 85.37% | Loss 0.5091\n",
            "Epoch 82 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [83] [0/196] LR 6.39496e-02  Loss 0.1340 (0.1340)  Acc 95.70% (95.70%)\n",
            "Epoch [83] [100/196] LR 6.39496e-02  Loss 0.2006 (0.1687)  Acc 94.92% (94.06%)\n",
            " Epoch 83 done in 20.3s | LR: 6.39496e-02 | Train Acc: 94.11% | Loss: 0.1711\n",
            "Validation Epoch 83: Acc 85.24% | Loss 0.5272\n",
            "Epoch 83 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [84] [0/196] LR 6.31937e-02  Loss 0.0975 (0.0975)  Acc 96.88% (96.88%)\n",
            "Epoch [84] [100/196] LR 6.31937e-02  Loss 0.1156 (0.1660)  Acc 96.48% (94.39%)\n",
            " Epoch 84 done in 20.7s | LR: 6.31937e-02 | Train Acc: 94.38% | Loss: 0.1653\n",
            "Validation Epoch 84: Acc 83.06% | Loss 0.6128\n",
            "Epoch 84 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [85] [0/196] LR 6.24345e-02  Loss 0.1609 (0.1609)  Acc 94.53% (94.53%)\n",
            "Epoch [85] [100/196] LR 6.24345e-02  Loss 0.1413 (0.1637)  Acc 95.31% (94.47%)\n",
            " Epoch 85 done in 20.7s | LR: 6.24345e-02 | Train Acc: 94.41% | Loss: 0.1654\n",
            "Validation Epoch 85: Acc 83.45% | Loss 0.5930\n",
            "Epoch 85 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [86] [0/196] LR 6.16723e-02  Loss 0.2115 (0.2115)  Acc 93.75% (93.75%)\n",
            "Epoch [86] [100/196] LR 6.16723e-02  Loss 0.1501 (0.1593)  Acc 94.53% (94.54%)\n",
            " Epoch 86 done in 20.7s | LR: 6.16723e-02 | Train Acc: 94.52% | Loss: 0.1600\n",
            "Validation Epoch 86: Acc 81.77% | Loss 0.6586\n",
            "Epoch 86 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [87] [0/196] LR 6.09072e-02  Loss 0.1707 (0.1707)  Acc 93.36% (93.36%)\n",
            "Epoch [87] [100/196] LR 6.09072e-02  Loss 0.1433 (0.1556)  Acc 94.53% (94.71%)\n",
            " Epoch 87 done in 20.0s | LR: 6.09072e-02 | Train Acc: 94.49% | Loss: 0.1610\n",
            "Validation Epoch 87: Acc 85.60% | Loss 0.4908\n",
            "Epoch 87 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [88] [0/196] LR 6.01394e-02  Loss 0.1852 (0.1852)  Acc 95.31% (95.31%)\n",
            "Epoch [88] [100/196] LR 6.01394e-02  Loss 0.1696 (0.1532)  Acc 93.75% (94.83%)\n",
            " Epoch 88 done in 19.4s | LR: 6.01394e-02 | Train Acc: 94.63% | Loss: 0.1576\n",
            "Validation Epoch 88: Acc 82.12% | Loss 0.6674\n",
            "Epoch 88 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [89] [0/196] LR 5.93691e-02  Loss 0.1432 (0.1432)  Acc 95.31% (95.31%)\n",
            "Epoch [89] [100/196] LR 5.93691e-02  Loss 0.1072 (0.1514)  Acc 97.66% (94.81%)\n",
            " Epoch 89 done in 19.9s | LR: 5.93691e-02 | Train Acc: 94.72% | Loss: 0.1557\n",
            "Validation Epoch 89: Acc 85.09% | Loss 0.5176\n",
            "Epoch 89 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [90] [0/196] LR 5.85965e-02  Loss 0.1591 (0.1591)  Acc 94.53% (94.53%)\n",
            "Epoch [90] [100/196] LR 5.85965e-02  Loss 0.0847 (0.1494)  Acc 96.88% (94.90%)\n",
            " Epoch 90 done in 20.5s | LR: 5.85965e-02 | Train Acc: 94.74% | Loss: 0.1537\n",
            "Validation Epoch 90: Acc 84.57% | Loss 0.5395\n",
            "Epoch 90 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [91] [0/196] LR 5.78217e-02  Loss 0.1624 (0.1624)  Acc 93.36% (93.36%)\n",
            "Epoch [91] [100/196] LR 5.78217e-02  Loss 0.1193 (0.1456)  Acc 96.48% (94.96%)\n",
            " Epoch 91 done in 20.3s | LR: 5.78217e-02 | Train Acc: 94.79% | Loss: 0.1524\n",
            "Validation Epoch 91: Acc 84.65% | Loss 0.5391\n",
            "Epoch 91 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [92] [0/196] LR 5.70451e-02  Loss 0.1310 (0.1310)  Acc 94.92% (94.92%)\n",
            "Epoch [92] [100/196] LR 5.70451e-02  Loss 0.1129 (0.1398)  Acc 95.70% (95.24%)\n",
            " Epoch 92 done in 20.6s | LR: 5.70451e-02 | Train Acc: 95.10% | Loss: 0.1440\n",
            "Validation Epoch 92: Acc 85.77% | Loss 0.5081\n",
            "Epoch 92 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [93] [0/196] LR 5.62667e-02  Loss 0.1423 (0.1423)  Acc 95.31% (95.31%)\n",
            "Epoch [93] [100/196] LR 5.62667e-02  Loss 0.1520 (0.1424)  Acc 94.92% (95.13%)\n",
            " Epoch 93 done in 19.4s | LR: 5.62667e-02 | Train Acc: 94.99% | Loss: 0.1469\n",
            "Validation Epoch 93: Acc 84.58% | Loss 0.5334\n",
            "Epoch 93 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [94] [0/196] LR 5.54867e-02  Loss 0.1221 (0.1221)  Acc 95.70% (95.70%)\n",
            "Epoch [94] [100/196] LR 5.54867e-02  Loss 0.1192 (0.1381)  Acc 96.09% (95.14%)\n",
            " Epoch 94 done in 19.3s | LR: 5.54867e-02 | Train Acc: 95.06% | Loss: 0.1425\n",
            "Validation Epoch 94: Acc 86.14% | Loss 0.4740\n",
            "Epoch 94 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [95] [0/196] LR 5.47054e-02  Loss 0.1516 (0.1516)  Acc 93.75% (93.75%)\n",
            "Epoch [95] [100/196] LR 5.47054e-02  Loss 0.1216 (0.1412)  Acc 96.09% (95.13%)\n",
            " Epoch 95 done in 20.3s | LR: 5.47054e-02 | Train Acc: 95.09% | Loss: 0.1436\n",
            "Validation Epoch 95: Acc 86.08% | Loss 0.4997\n",
            "Epoch 95 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [96] [0/196] LR 5.39230e-02  Loss 0.2049 (0.2049)  Acc 92.97% (92.97%)\n",
            "Epoch [96] [100/196] LR 5.39230e-02  Loss 0.1093 (0.1381)  Acc 96.88% (95.20%)\n",
            " Epoch 96 done in 19.9s | LR: 5.39230e-02 | Train Acc: 95.20% | Loss: 0.1390\n",
            "Validation Epoch 96: Acc 83.93% | Loss 0.5983\n",
            "Epoch 96 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [97] [0/196] LR 5.31395e-02  Loss 0.1592 (0.1592)  Acc 94.53% (94.53%)\n",
            "Epoch [97] [100/196] LR 5.31395e-02  Loss 0.1771 (0.1373)  Acc 94.14% (95.37%)\n",
            " Epoch 97 done in 20.1s | LR: 5.31395e-02 | Train Acc: 95.25% | Loss: 0.1403\n",
            "Validation Epoch 97: Acc 85.81% | Loss 0.4850\n",
            "Epoch 97 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [98] [0/196] LR 5.23553e-02  Loss 0.1448 (0.1448)  Acc 95.31% (95.31%)\n",
            "Epoch [98] [100/196] LR 5.23553e-02  Loss 0.1954 (0.1356)  Acc 94.14% (95.41%)\n",
            " Epoch 98 done in 19.5s | LR: 5.23553e-02 | Train Acc: 95.47% | Loss: 0.1350\n",
            "Validation Epoch 98: Acc 84.38% | Loss 0.5762\n",
            "Epoch 98 complete | Best Acc: 86.37%\n",
            "\n",
            "Epoch [99] [0/196] LR 5.15705e-02  Loss 0.1282 (0.1282)  Acc 95.70% (95.70%)\n",
            "Epoch [99] [100/196] LR 5.15705e-02  Loss 0.1395 (0.1306)  Acc 95.70% (95.54%)\n",
            " Epoch 99 done in 19.7s | LR: 5.15705e-02 | Train Acc: 95.30% | Loss: 0.1357\n",
            "Validation Epoch 99: Acc 86.47% | Loss 0.4875\n",
            "Epoch 99 complete | Best Acc: 86.47%\n",
            "\n",
            "Epoch [100] [0/196] LR 5.07854e-02  Loss 0.1007 (0.1007)  Acc 96.48% (96.48%)\n",
            "Epoch [100] [100/196] LR 5.07854e-02  Loss 0.1063 (0.1331)  Acc 96.48% (95.35%)\n",
            " Epoch 100 done in 20.2s | LR: 5.07854e-02 | Train Acc: 95.16% | Loss: 0.1377\n",
            "Validation Epoch 100: Acc 83.80% | Loss 0.5702\n",
            "Epoch 100 complete | Best Acc: 86.47%\n",
            "\n",
            "Epoch [101] [0/196] LR 5.00000e-02  Loss 0.1603 (0.1603)  Acc 93.75% (93.75%)\n",
            "Epoch [101] [100/196] LR 5.00000e-02  Loss 0.1303 (0.1266)  Acc 96.09% (95.71%)\n",
            " Epoch 101 done in 20.0s | LR: 5.00000e-02 | Train Acc: 95.57% | Loss: 0.1320\n",
            "Validation Epoch 101: Acc 85.08% | Loss 0.5294\n",
            "Epoch 101 complete | Best Acc: 86.47%\n",
            "\n",
            "Epoch [102] [0/196] LR 4.92146e-02  Loss 0.1444 (0.1444)  Acc 95.31% (95.31%)\n",
            "Epoch [102] [100/196] LR 4.92146e-02  Loss 0.1509 (0.1303)  Acc 95.70% (95.57%)\n",
            " Epoch 102 done in 20.2s | LR: 4.92146e-02 | Train Acc: 95.31% | Loss: 0.1379\n",
            "Validation Epoch 102: Acc 84.34% | Loss 0.5596\n",
            "Epoch 102 complete | Best Acc: 86.47%\n",
            "\n",
            "Epoch [103] [0/196] LR 4.84295e-02  Loss 0.1075 (0.1075)  Acc 96.48% (96.48%)\n",
            "Epoch [103] [100/196] LR 4.84295e-02  Loss 0.1187 (0.1316)  Acc 96.09% (95.58%)\n",
            " Epoch 103 done in 19.4s | LR: 4.84295e-02 | Train Acc: 95.53% | Loss: 0.1315\n",
            "Validation Epoch 103: Acc 87.21% | Loss 0.4528\n",
            "Epoch 103 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [104] [0/196] LR 4.76447e-02  Loss 0.0989 (0.0989)  Acc 96.09% (96.09%)\n",
            "Epoch [104] [100/196] LR 4.76447e-02  Loss 0.2006 (0.1306)  Acc 92.58% (95.45%)\n",
            " Epoch 104 done in 19.5s | LR: 4.76447e-02 | Train Acc: 95.43% | Loss: 0.1323\n",
            "Validation Epoch 104: Acc 86.48% | Loss 0.4760\n",
            "Epoch 104 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [105] [0/196] LR 4.68605e-02  Loss 0.1208 (0.1208)  Acc 95.70% (95.70%)\n",
            "Epoch [105] [100/196] LR 4.68605e-02  Loss 0.1777 (0.1291)  Acc 93.75% (95.56%)\n",
            " Epoch 105 done in 20.0s | LR: 4.68605e-02 | Train Acc: 95.54% | Loss: 0.1317\n",
            "Validation Epoch 105: Acc 85.74% | Loss 0.5146\n",
            "Epoch 105 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [106] [0/196] LR 4.60770e-02  Loss 0.1142 (0.1142)  Acc 95.70% (95.70%)\n",
            "Epoch [106] [100/196] LR 4.60770e-02  Loss 0.1171 (0.1288)  Acc 95.70% (95.55%)\n",
            " Epoch 106 done in 20.3s | LR: 4.60770e-02 | Train Acc: 95.63% | Loss: 0.1267\n",
            "Validation Epoch 106: Acc 85.84% | Loss 0.4919\n",
            "Epoch 106 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [107] [0/196] LR 4.52946e-02  Loss 0.1189 (0.1189)  Acc 96.48% (96.48%)\n",
            "Epoch [107] [100/196] LR 4.52946e-02  Loss 0.1258 (0.1210)  Acc 94.92% (95.85%)\n",
            " Epoch 107 done in 20.2s | LR: 4.52946e-02 | Train Acc: 95.75% | Loss: 0.1251\n",
            "Validation Epoch 107: Acc 86.40% | Loss 0.5085\n",
            "Epoch 107 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [108] [0/196] LR 4.45133e-02  Loss 0.1782 (0.1782)  Acc 94.53% (94.53%)\n",
            "Epoch [108] [100/196] LR 4.45133e-02  Loss 0.1043 (0.1192)  Acc 96.48% (95.95%)\n",
            " Epoch 108 done in 19.5s | LR: 4.45133e-02 | Train Acc: 95.82% | Loss: 0.1214\n",
            "Validation Epoch 108: Acc 85.95% | Loss 0.5293\n",
            "Epoch 108 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [109] [0/196] LR 4.37333e-02  Loss 0.1038 (0.1038)  Acc 96.09% (96.09%)\n",
            "Epoch [109] [100/196] LR 4.37333e-02  Loss 0.0601 (0.1227)  Acc 97.66% (95.89%)\n",
            " Epoch 109 done in 19.6s | LR: 4.37333e-02 | Train Acc: 95.74% | Loss: 0.1257\n",
            "Validation Epoch 109: Acc 85.85% | Loss 0.4990\n",
            "Epoch 109 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [110] [0/196] LR 4.29549e-02  Loss 0.1103 (0.1103)  Acc 96.09% (96.09%)\n",
            "Epoch [110] [100/196] LR 4.29549e-02  Loss 0.0574 (0.1123)  Acc 97.66% (96.20%)\n",
            " Epoch 110 done in 19.9s | LR: 4.29549e-02 | Train Acc: 96.00% | Loss: 0.1184\n",
            "Validation Epoch 110: Acc 85.40% | Loss 0.5462\n",
            "Epoch 110 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [111] [0/196] LR 4.21783e-02  Loss 0.0834 (0.0834)  Acc 96.88% (96.88%)\n",
            "Epoch [111] [100/196] LR 4.21783e-02  Loss 0.0897 (0.1113)  Acc 95.31% (96.18%)\n",
            " Epoch 111 done in 20.7s | LR: 4.21783e-02 | Train Acc: 96.14% | Loss: 0.1121\n",
            "Validation Epoch 111: Acc 85.49% | Loss 0.5498\n",
            "Epoch 111 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [112] [0/196] LR 4.14035e-02  Loss 0.0756 (0.0756)  Acc 96.88% (96.88%)\n",
            "Epoch [112] [100/196] LR 4.14035e-02  Loss 0.1704 (0.1128)  Acc 93.75% (96.18%)\n",
            " Epoch 112 done in 20.1s | LR: 4.14035e-02 | Train Acc: 96.21% | Loss: 0.1120\n",
            "Validation Epoch 112: Acc 86.50% | Loss 0.4966\n",
            "Epoch 112 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [113] [0/196] LR 4.06309e-02  Loss 0.0994 (0.0994)  Acc 96.09% (96.09%)\n",
            "Epoch [113] [100/196] LR 4.06309e-02  Loss 0.1058 (0.1063)  Acc 96.88% (96.37%)\n",
            " Epoch 113 done in 20.3s | LR: 4.06309e-02 | Train Acc: 96.37% | Loss: 0.1084\n",
            "Validation Epoch 113: Acc 84.90% | Loss 0.5600\n",
            "Epoch 113 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [114] [0/196] LR 3.98606e-02  Loss 0.0881 (0.0881)  Acc 97.27% (97.27%)\n",
            "Epoch [114] [100/196] LR 3.98606e-02  Loss 0.0769 (0.1008)  Acc 97.27% (96.58%)\n",
            " Epoch 114 done in 19.6s | LR: 3.98606e-02 | Train Acc: 96.37% | Loss: 0.1069\n",
            "Validation Epoch 114: Acc 85.95% | Loss 0.5202\n",
            "Epoch 114 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [115] [0/196] LR 3.90928e-02  Loss 0.1386 (0.1386)  Acc 94.92% (94.92%)\n",
            "Epoch [115] [100/196] LR 3.90928e-02  Loss 0.1507 (0.1012)  Acc 93.75% (96.62%)\n",
            " Epoch 115 done in 19.9s | LR: 3.90928e-02 | Train Acc: 96.47% | Loss: 0.1037\n",
            "Validation Epoch 115: Acc 86.11% | Loss 0.5271\n",
            "Epoch 115 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [116] [0/196] LR 3.83277e-02  Loss 0.0999 (0.0999)  Acc 96.48% (96.48%)\n",
            "Epoch [116] [100/196] LR 3.83277e-02  Loss 0.0689 (0.0989)  Acc 97.66% (96.62%)\n",
            " Epoch 116 done in 20.9s | LR: 3.83277e-02 | Train Acc: 96.51% | Loss: 0.1021\n",
            "Validation Epoch 116: Acc 85.43% | Loss 0.5664\n",
            "Epoch 116 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [117] [0/196] LR 3.75655e-02  Loss 0.1316 (0.1316)  Acc 96.09% (96.09%)\n",
            "Epoch [117] [100/196] LR 3.75655e-02  Loss 0.0691 (0.0962)  Acc 97.66% (96.73%)\n",
            " Epoch 117 done in 20.4s | LR: 3.75655e-02 | Train Acc: 96.67% | Loss: 0.0972\n",
            "Validation Epoch 117: Acc 85.94% | Loss 0.5443\n",
            "Epoch 117 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [118] [0/196] LR 3.68063e-02  Loss 0.0990 (0.0990)  Acc 96.88% (96.88%)\n",
            "Epoch [118] [100/196] LR 3.68063e-02  Loss 0.0771 (0.0836)  Acc 97.27% (97.13%)\n",
            " Epoch 118 done in 20.3s | LR: 3.68063e-02 | Train Acc: 96.81% | Loss: 0.0920\n",
            "Validation Epoch 118: Acc 86.40% | Loss 0.5279\n",
            "Epoch 118 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [119] [0/196] LR 3.60504e-02  Loss 0.0934 (0.0934)  Acc 96.09% (96.09%)\n",
            "Epoch [119] [100/196] LR 3.60504e-02  Loss 0.1062 (0.0971)  Acc 97.27% (96.68%)\n",
            " Epoch 119 done in 19.8s | LR: 3.60504e-02 | Train Acc: 96.64% | Loss: 0.0989\n",
            "Validation Epoch 119: Acc 87.19% | Loss 0.4865\n",
            "Epoch 119 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [120] [0/196] LR 3.52980e-02  Loss 0.0911 (0.0911)  Acc 97.27% (97.27%)\n",
            "Epoch [120] [100/196] LR 3.52980e-02  Loss 0.1012 (0.0877)  Acc 97.27% (97.08%)\n",
            " Epoch 120 done in 19.5s | LR: 3.52980e-02 | Train Acc: 97.08% | Loss: 0.0884\n",
            "Validation Epoch 120: Acc 85.62% | Loss 0.5610\n",
            "Epoch 120 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [121] [0/196] LR 3.45492e-02  Loss 0.0584 (0.0584)  Acc 97.66% (97.66%)\n",
            "Epoch [121] [100/196] LR 3.45492e-02  Loss 0.1120 (0.0869)  Acc 96.48% (97.06%)\n",
            " Epoch 121 done in 20.2s | LR: 3.45492e-02 | Train Acc: 96.89% | Loss: 0.0902\n",
            "Validation Epoch 121: Acc 86.32% | Loss 0.5226\n",
            "Epoch 121 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [122] [0/196] LR 3.38041e-02  Loss 0.0781 (0.0781)  Acc 96.88% (96.88%)\n",
            "Epoch [122] [100/196] LR 3.38041e-02  Loss 0.0736 (0.0875)  Acc 97.27% (96.92%)\n",
            " Epoch 122 done in 20.2s | LR: 3.38041e-02 | Train Acc: 97.04% | Loss: 0.0865\n",
            "Validation Epoch 122: Acc 85.04% | Loss 0.5901\n",
            "Epoch 122 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [123] [0/196] LR 3.30631e-02  Loss 0.0862 (0.0862)  Acc 96.88% (96.88%)\n",
            "Epoch [123] [100/196] LR 3.30631e-02  Loss 0.0876 (0.0801)  Acc 96.88% (97.20%)\n",
            " Epoch 123 done in 20.1s | LR: 3.30631e-02 | Train Acc: 97.19% | Loss: 0.0834\n",
            "Validation Epoch 123: Acc 85.57% | Loss 0.5516\n",
            "Epoch 123 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [124] [0/196] LR 3.23263e-02  Loss 0.0676 (0.0676)  Acc 97.66% (97.66%)\n",
            "Epoch [124] [100/196] LR 3.23263e-02  Loss 0.0605 (0.0806)  Acc 97.66% (97.34%)\n",
            " Epoch 124 done in 19.8s | LR: 3.23263e-02 | Train Acc: 97.12% | Loss: 0.0850\n",
            "Validation Epoch 124: Acc 85.17% | Loss 0.6084\n",
            "Epoch 124 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [125] [0/196] LR 3.15938e-02  Loss 0.0558 (0.0558)  Acc 97.27% (97.27%)\n",
            "Epoch [125] [100/196] LR 3.15938e-02  Loss 0.0572 (0.0743)  Acc 98.44% (97.48%)\n",
            " Epoch 125 done in 19.2s | LR: 3.15938e-02 | Train Acc: 97.35% | Loss: 0.0787\n",
            "Validation Epoch 125: Acc 87.12% | Loss 0.5048\n",
            "Epoch 125 complete | Best Acc: 87.21%\n",
            "\n",
            "Epoch [126] [0/196] LR 3.08658e-02  Loss 0.0525 (0.0525)  Acc 98.05% (98.05%)\n",
            "Epoch [126] [100/196] LR 3.08658e-02  Loss 0.0862 (0.0724)  Acc 96.48% (97.63%)\n",
            " Epoch 126 done in 19.4s | LR: 3.08658e-02 | Train Acc: 97.47% | Loss: 0.0781\n",
            "Validation Epoch 126: Acc 87.33% | Loss 0.5151\n",
            "Epoch 126 complete | Best Acc: 87.33%\n",
            "\n",
            "Epoch [127] [0/196] LR 3.01426e-02  Loss 0.0779 (0.0779)  Acc 96.88% (96.88%)\n",
            "Epoch [127] [100/196] LR 3.01426e-02  Loss 0.0540 (0.0760)  Acc 98.05% (97.38%)\n",
            " Epoch 127 done in 20.1s | LR: 3.01426e-02 | Train Acc: 97.42% | Loss: 0.0759\n",
            "Validation Epoch 127: Acc 87.63% | Loss 0.4591\n",
            "Epoch 127 complete | Best Acc: 87.63%\n",
            "\n",
            "Epoch [128] [0/196] LR 2.94243e-02  Loss 0.0384 (0.0384)  Acc 98.83% (98.83%)\n",
            "Epoch [128] [100/196] LR 2.94243e-02  Loss 0.1067 (0.0744)  Acc 96.48% (97.46%)\n",
            " Epoch 128 done in 20.2s | LR: 2.94243e-02 | Train Acc: 97.47% | Loss: 0.0740\n",
            "Validation Epoch 128: Acc 85.64% | Loss 0.5635\n",
            "Epoch 128 complete | Best Acc: 87.63%\n",
            "\n",
            "Epoch [129] [0/196] LR 2.87110e-02  Loss 0.1219 (0.1219)  Acc 96.48% (96.48%)\n",
            "Epoch [129] [100/196] LR 2.87110e-02  Loss 0.0767 (0.0754)  Acc 98.83% (97.44%)\n",
            " Epoch 129 done in 19.7s | LR: 2.87110e-02 | Train Acc: 97.57% | Loss: 0.0733\n",
            "Validation Epoch 129: Acc 87.45% | Loss 0.4999\n",
            "Epoch 129 complete | Best Acc: 87.63%\n",
            "\n",
            "Epoch [130] [0/196] LR 2.80030e-02  Loss 0.0453 (0.0453)  Acc 98.83% (98.83%)\n",
            "Epoch [130] [100/196] LR 2.80030e-02  Loss 0.0823 (0.0662)  Acc 96.48% (97.64%)\n",
            " Epoch 130 done in 19.5s | LR: 2.80030e-02 | Train Acc: 97.72% | Loss: 0.0658\n",
            "Validation Epoch 130: Acc 87.07% | Loss 0.5315\n",
            "Epoch 130 complete | Best Acc: 87.63%\n",
            "\n",
            "Epoch [131] [0/196] LR 2.73005e-02  Loss 0.0497 (0.0497)  Acc 98.05% (98.05%)\n",
            "Epoch [131] [100/196] LR 2.73005e-02  Loss 0.0313 (0.0666)  Acc 99.22% (97.75%)\n",
            " Epoch 131 done in 19.5s | LR: 2.73005e-02 | Train Acc: 97.76% | Loss: 0.0660\n",
            "Validation Epoch 131: Acc 87.76% | Loss 0.5027\n",
            "Epoch 131 complete | Best Acc: 87.76%\n",
            "\n",
            "Epoch [132] [0/196] LR 2.66035e-02  Loss 0.0528 (0.0528)  Acc 98.44% (98.44%)\n",
            "Epoch [132] [100/196] LR 2.66035e-02  Loss 0.1552 (0.0643)  Acc 96.09% (97.76%)\n",
            " Epoch 132 done in 20.1s | LR: 2.66035e-02 | Train Acc: 97.69% | Loss: 0.0682\n",
            "Validation Epoch 132: Acc 87.59% | Loss 0.4842\n",
            "Epoch 132 complete | Best Acc: 87.76%\n",
            "\n",
            "Epoch [133] [0/196] LR 2.59123e-02  Loss 0.0401 (0.0401)  Acc 98.83% (98.83%)\n",
            "Epoch [133] [100/196] LR 2.59123e-02  Loss 0.0653 (0.0612)  Acc 97.66% (97.95%)\n",
            " Epoch 133 done in 20.3s | LR: 2.59123e-02 | Train Acc: 97.89% | Loss: 0.0618\n",
            "Validation Epoch 133: Acc 87.17% | Loss 0.4925\n",
            "Epoch 133 complete | Best Acc: 87.76%\n",
            "\n",
            "Epoch [134] [0/196] LR 2.52271e-02  Loss 0.0846 (0.0846)  Acc 96.48% (96.48%)\n",
            "Epoch [134] [100/196] LR 2.52271e-02  Loss 0.0239 (0.0634)  Acc 99.61% (97.82%)\n",
            " Epoch 134 done in 19.7s | LR: 2.52271e-02 | Train Acc: 97.79% | Loss: 0.0646\n",
            "Validation Epoch 134: Acc 87.72% | Loss 0.5010\n",
            "Epoch 134 complete | Best Acc: 87.76%\n",
            "\n",
            "Epoch [135] [0/196] LR 2.45479e-02  Loss 0.0315 (0.0315)  Acc 99.22% (99.22%)\n",
            "Epoch [135] [100/196] LR 2.45479e-02  Loss 0.0493 (0.0659)  Acc 97.66% (97.78%)\n",
            " Epoch 135 done in 19.4s | LR: 2.45479e-02 | Train Acc: 97.85% | Loss: 0.0634\n",
            "Validation Epoch 135: Acc 87.64% | Loss 0.4933\n",
            "Epoch 135 complete | Best Acc: 87.76%\n",
            "\n",
            "Epoch [136] [0/196] LR 2.38751e-02  Loss 0.0800 (0.0800)  Acc 95.70% (95.70%)\n",
            "Epoch [136] [100/196] LR 2.38751e-02  Loss 0.0733 (0.0515)  Acc 97.27% (98.34%)\n",
            " Epoch 136 done in 19.7s | LR: 2.38751e-02 | Train Acc: 98.18% | Loss: 0.0544\n",
            "Validation Epoch 136: Acc 87.91% | Loss 0.4877\n",
            "Epoch 136 complete | Best Acc: 87.91%\n",
            "\n",
            "Epoch [137] [0/196] LR 2.32087e-02  Loss 0.0901 (0.0901)  Acc 96.48% (96.48%)\n",
            "Epoch [137] [100/196] LR 2.32087e-02  Loss 0.0384 (0.0542)  Acc 98.44% (98.15%)\n",
            " Epoch 137 done in 19.9s | LR: 2.32087e-02 | Train Acc: 97.99% | Loss: 0.0580\n",
            "Validation Epoch 137: Acc 87.18% | Loss 0.5141\n",
            "Epoch 137 complete | Best Acc: 87.91%\n",
            "\n",
            "Epoch [138] [0/196] LR 2.25489e-02  Loss 0.0715 (0.0715)  Acc 97.27% (97.27%)\n",
            "Epoch [138] [100/196] LR 2.25489e-02  Loss 0.0595 (0.0564)  Acc 97.66% (98.12%)\n",
            " Epoch 138 done in 20.2s | LR: 2.25489e-02 | Train Acc: 98.12% | Loss: 0.0556\n",
            "Validation Epoch 138: Acc 87.93% | Loss 0.4850\n",
            "Epoch 138 complete | Best Acc: 87.93%\n",
            "\n",
            "Epoch [139] [0/196] LR 2.18958e-02  Loss 0.0278 (0.0278)  Acc 98.83% (98.83%)\n",
            "Epoch [139] [100/196] LR 2.18958e-02  Loss 0.0727 (0.0520)  Acc 98.05% (98.33%)\n",
            " Epoch 139 done in 20.0s | LR: 2.18958e-02 | Train Acc: 98.21% | Loss: 0.0548\n",
            "Validation Epoch 139: Acc 87.62% | Loss 0.4945\n",
            "Epoch 139 complete | Best Acc: 87.93%\n",
            "\n",
            "Epoch [140] [0/196] LR 2.12497e-02  Loss 0.0518 (0.0518)  Acc 98.44% (98.44%)\n",
            "Epoch [140] [100/196] LR 2.12497e-02  Loss 0.0440 (0.0498)  Acc 98.05% (98.37%)\n",
            " Epoch 140 done in 19.4s | LR: 2.12497e-02 | Train Acc: 98.39% | Loss: 0.0493\n",
            "Validation Epoch 140: Acc 88.47% | Loss 0.4822\n",
            "Epoch 140 complete | Best Acc: 88.47%\n",
            "\n",
            "Epoch [141] [0/196] LR 2.06107e-02  Loss 0.0390 (0.0390)  Acc 98.83% (98.83%)\n",
            "Epoch [141] [100/196] LR 2.06107e-02  Loss 0.0483 (0.0457)  Acc 97.66% (98.46%)\n",
            " Epoch 141 done in 19.1s | LR: 2.06107e-02 | Train Acc: 98.37% | Loss: 0.0488\n",
            "Validation Epoch 141: Acc 87.65% | Loss 0.5171\n",
            "Epoch 141 complete | Best Acc: 88.47%\n",
            "\n",
            "Epoch [142] [0/196] LR 1.99790e-02  Loss 0.0347 (0.0347)  Acc 98.83% (98.83%)\n",
            "Epoch [142] [100/196] LR 1.99790e-02  Loss 0.0438 (0.0437)  Acc 98.44% (98.51%)\n",
            " Epoch 142 done in 19.8s | LR: 1.99790e-02 | Train Acc: 98.35% | Loss: 0.0475\n",
            "Validation Epoch 142: Acc 88.04% | Loss 0.4878\n",
            "Epoch 142 complete | Best Acc: 88.47%\n",
            "\n",
            "Epoch [143] [0/196] LR 1.93546e-02  Loss 0.0658 (0.0658)  Acc 98.05% (98.05%)\n",
            "Epoch [143] [100/196] LR 1.93546e-02  Loss 0.0321 (0.0435)  Acc 98.44% (98.54%)\n",
            " Epoch 143 done in 20.0s | LR: 1.93546e-02 | Train Acc: 98.53% | Loss: 0.0454\n",
            "Validation Epoch 143: Acc 88.62% | Loss 0.4850\n",
            "Epoch 143 complete | Best Acc: 88.62%\n",
            "\n",
            "Epoch [144] [0/196] LR 1.87379e-02  Loss 0.0459 (0.0459)  Acc 98.44% (98.44%)\n",
            "Epoch [144] [100/196] LR 1.87379e-02  Loss 0.0497 (0.0405)  Acc 98.83% (98.64%)\n",
            " Epoch 144 done in 20.0s | LR: 1.87379e-02 | Train Acc: 98.56% | Loss: 0.0434\n",
            "Validation Epoch 144: Acc 88.80% | Loss 0.4644\n",
            "Epoch 144 complete | Best Acc: 88.80%\n",
            "\n",
            "Epoch [145] [0/196] LR 1.81288e-02  Loss 0.0354 (0.0354)  Acc 98.83% (98.83%)\n",
            "Epoch [145] [100/196] LR 1.81288e-02  Loss 0.0343 (0.0398)  Acc 99.22% (98.69%)\n",
            " Epoch 145 done in 19.3s | LR: 1.81288e-02 | Train Acc: 98.72% | Loss: 0.0387\n",
            "Validation Epoch 145: Acc 88.25% | Loss 0.4913\n",
            "Epoch 145 complete | Best Acc: 88.80%\n",
            "\n",
            "Epoch [146] [0/196] LR 1.75276e-02  Loss 0.0498 (0.0498)  Acc 98.05% (98.05%)\n",
            "Epoch [146] [100/196] LR 1.75276e-02  Loss 0.0555 (0.0407)  Acc 98.05% (98.59%)\n",
            " Epoch 146 done in 19.3s | LR: 1.75276e-02 | Train Acc: 98.55% | Loss: 0.0417\n",
            "Validation Epoch 146: Acc 88.81% | Loss 0.4796\n",
            "Epoch 146 complete | Best Acc: 88.81%\n",
            "\n",
            "Epoch [147] [0/196] LR 1.69344e-02  Loss 0.0298 (0.0298)  Acc 98.83% (98.83%)\n",
            "Epoch [147] [100/196] LR 1.69344e-02  Loss 0.0694 (0.0396)  Acc 97.66% (98.74%)\n",
            " Epoch 147 done in 19.7s | LR: 1.69344e-02 | Train Acc: 98.63% | Loss: 0.0419\n",
            "Validation Epoch 147: Acc 88.59% | Loss 0.4951\n",
            "Epoch 147 complete | Best Acc: 88.81%\n",
            "\n",
            "Epoch [148] [0/196] LR 1.63494e-02  Loss 0.0711 (0.0711)  Acc 97.66% (97.66%)\n",
            "Epoch [148] [100/196] LR 1.63494e-02  Loss 0.0288 (0.0410)  Acc 99.22% (98.65%)\n",
            " Epoch 148 done in 19.9s | LR: 1.63494e-02 | Train Acc: 98.62% | Loss: 0.0414\n",
            "Validation Epoch 148: Acc 88.76% | Loss 0.5070\n",
            "Epoch 148 complete | Best Acc: 88.81%\n",
            "\n",
            "Epoch [149] [0/196] LR 1.57726e-02  Loss 0.0522 (0.0522)  Acc 97.66% (97.66%)\n",
            "Epoch [149] [100/196] LR 1.57726e-02  Loss 0.0348 (0.0411)  Acc 99.22% (98.66%)\n",
            " Epoch 149 done in 19.1s | LR: 1.57726e-02 | Train Acc: 98.64% | Loss: 0.0409\n",
            "Validation Epoch 149: Acc 88.35% | Loss 0.4889\n",
            "Epoch 149 complete | Best Acc: 88.81%\n",
            "\n",
            "Epoch [150] [0/196] LR 1.52044e-02  Loss 0.0260 (0.0260)  Acc 98.83% (98.83%)\n",
            "Epoch [150] [100/196] LR 1.52044e-02  Loss 0.0204 (0.0358)  Acc 99.61% (98.79%)\n",
            " Epoch 150 done in 18.9s | LR: 1.52044e-02 | Train Acc: 98.79% | Loss: 0.0356\n",
            "Validation Epoch 150: Acc 88.79% | Loss 0.4806\n",
            "Epoch 150 complete | Best Acc: 88.81%\n",
            "\n",
            "Epoch [151] [0/196] LR 1.46447e-02  Loss 0.0132 (0.0132)  Acc 100.00% (100.00%)\n",
            "Epoch [151] [100/196] LR 1.46447e-02  Loss 0.0545 (0.0338)  Acc 98.44% (98.86%)\n",
            " Epoch 151 done in 20.3s | LR: 1.46447e-02 | Train Acc: 98.78% | Loss: 0.0357\n",
            "Validation Epoch 151: Acc 89.01% | Loss 0.4680\n",
            "Epoch 151 complete | Best Acc: 89.01%\n",
            "\n",
            "Epoch [152] [0/196] LR 1.40937e-02  Loss 0.0354 (0.0354)  Acc 98.44% (98.44%)\n",
            "Epoch [152] [100/196] LR 1.40937e-02  Loss 0.0129 (0.0348)  Acc 99.61% (98.86%)\n",
            " Epoch 152 done in 20.2s | LR: 1.40937e-02 | Train Acc: 98.88% | Loss: 0.0347\n",
            "Validation Epoch 152: Acc 88.41% | Loss 0.5078\n",
            "Epoch 152 complete | Best Acc: 89.01%\n",
            "\n",
            "Epoch [153] [0/196] LR 1.35516e-02  Loss 0.0361 (0.0361)  Acc 99.22% (99.22%)\n",
            "Epoch [153] [100/196] LR 1.35516e-02  Loss 0.0367 (0.0317)  Acc 98.83% (99.02%)\n",
            " Epoch 153 done in 20.5s | LR: 1.35516e-02 | Train Acc: 98.92% | Loss: 0.0336\n",
            "Validation Epoch 153: Acc 88.87% | Loss 0.4710\n",
            "Epoch 153 complete | Best Acc: 89.01%\n",
            "\n",
            "Epoch [154] [0/196] LR 1.30184e-02  Loss 0.0462 (0.0462)  Acc 98.05% (98.05%)\n",
            "Epoch [154] [100/196] LR 1.30184e-02  Loss 0.0184 (0.0322)  Acc 99.22% (98.89%)\n",
            " Epoch 154 done in 19.5s | LR: 1.30184e-02 | Train Acc: 98.95% | Loss: 0.0313\n",
            "Validation Epoch 154: Acc 88.71% | Loss 0.4760\n",
            "Epoch 154 complete | Best Acc: 89.01%\n",
            "\n",
            "Epoch [155] [0/196] LR 1.24944e-02  Loss 0.0172 (0.0172)  Acc 99.61% (99.61%)\n",
            "Epoch [155] [100/196] LR 1.24944e-02  Loss 0.0473 (0.0328)  Acc 98.05% (98.90%)\n",
            " Epoch 155 done in 20.1s | LR: 1.24944e-02 | Train Acc: 98.91% | Loss: 0.0326\n",
            "Validation Epoch 155: Acc 89.23% | Loss 0.4541\n",
            "Epoch 155 complete | Best Acc: 89.23%\n",
            "\n",
            "Epoch [156] [0/196] LR 1.19797e-02  Loss 0.0648 (0.0648)  Acc 98.44% (98.44%)\n",
            "Epoch [156] [100/196] LR 1.19797e-02  Loss 0.0379 (0.0308)  Acc 99.22% (99.01%)\n",
            " Epoch 156 done in 19.6s | LR: 1.19797e-02 | Train Acc: 99.06% | Loss: 0.0290\n",
            "Validation Epoch 156: Acc 89.07% | Loss 0.4496\n",
            "Epoch 156 complete | Best Acc: 89.23%\n",
            "\n",
            "Epoch [157] [0/196] LR 1.14743e-02  Loss 0.0231 (0.0231)  Acc 99.22% (99.22%)\n",
            "Epoch [157] [100/196] LR 1.14743e-02  Loss 0.0225 (0.0285)  Acc 99.61% (99.08%)\n",
            " Epoch 157 done in 20.8s | LR: 1.14743e-02 | Train Acc: 99.05% | Loss: 0.0288\n",
            "Validation Epoch 157: Acc 88.90% | Loss 0.4924\n",
            "Epoch 157 complete | Best Acc: 89.23%\n",
            "\n",
            "Epoch [158] [0/196] LR 1.09785e-02  Loss 0.0059 (0.0059)  Acc 100.00% (100.00%)\n",
            "Epoch [158] [100/196] LR 1.09785e-02  Loss 0.0147 (0.0260)  Acc 99.22% (99.14%)\n",
            " Epoch 158 done in 20.6s | LR: 1.09785e-02 | Train Acc: 99.19% | Loss: 0.0248\n",
            "Validation Epoch 158: Acc 88.99% | Loss 0.4801\n",
            "Epoch 158 complete | Best Acc: 89.23%\n",
            "\n",
            "Epoch [159] [0/196] LR 1.04922e-02  Loss 0.0099 (0.0099)  Acc 99.61% (99.61%)\n",
            "Epoch [159] [100/196] LR 1.04922e-02  Loss 0.0200 (0.0237)  Acc 99.61% (99.27%)\n",
            " Epoch 159 done in 20.2s | LR: 1.04922e-02 | Train Acc: 99.18% | Loss: 0.0252\n",
            "Validation Epoch 159: Acc 89.02% | Loss 0.4887\n",
            "Epoch 159 complete | Best Acc: 89.23%\n",
            "\n",
            "Epoch [160] [0/196] LR 1.00158e-02  Loss 0.0132 (0.0132)  Acc 99.61% (99.61%)\n",
            "Epoch [160] [100/196] LR 1.00158e-02  Loss 0.0244 (0.0249)  Acc 99.22% (99.27%)\n",
            " Epoch 160 done in 19.3s | LR: 1.00158e-02 | Train Acc: 99.26% | Loss: 0.0246\n",
            "Validation Epoch 160: Acc 88.43% | Loss 0.5106\n",
            "Epoch 160 complete | Best Acc: 89.23%\n",
            "\n",
            "Epoch [161] [0/196] LR 9.54915e-03  Loss 0.0397 (0.0397)  Acc 98.44% (98.44%)\n",
            "Epoch [161] [100/196] LR 9.54915e-03  Loss 0.0233 (0.0260)  Acc 99.22% (99.16%)\n",
            " Epoch 161 done in 18.9s | LR: 9.54915e-03 | Train Acc: 99.14% | Loss: 0.0256\n",
            "Validation Epoch 161: Acc 89.29% | Loss 0.4747\n",
            "Epoch 161 complete | Best Acc: 89.29%\n",
            "\n",
            "Epoch [162] [0/196] LR 9.09251e-03  Loss 0.0178 (0.0178)  Acc 99.22% (99.22%)\n",
            "Epoch [162] [100/196] LR 9.09251e-03  Loss 0.0261 (0.0251)  Acc 98.83% (99.18%)\n",
            " Epoch 162 done in 20.2s | LR: 9.09251e-03 | Train Acc: 99.21% | Loss: 0.0238\n",
            "Validation Epoch 162: Acc 89.17% | Loss 0.4865\n",
            "Epoch 162 complete | Best Acc: 89.29%\n",
            "\n",
            "Epoch [163] [0/196] LR 8.64597e-03  Loss 0.0285 (0.0285)  Acc 99.22% (99.22%)\n",
            "Epoch [163] [100/196] LR 8.64597e-03  Loss 0.0091 (0.0237)  Acc 100.00% (99.26%)\n",
            " Epoch 163 done in 20.3s | LR: 8.64597e-03 | Train Acc: 99.25% | Loss: 0.0234\n",
            "Validation Epoch 163: Acc 89.35% | Loss 0.4899\n",
            "Epoch 163 complete | Best Acc: 89.35%\n",
            "\n",
            "Epoch [164] [0/196] LR 8.20963e-03  Loss 0.0202 (0.0202)  Acc 99.22% (99.22%)\n",
            "Epoch [164] [100/196] LR 8.20963e-03  Loss 0.0053 (0.0200)  Acc 100.00% (99.36%)\n",
            " Epoch 164 done in 20.3s | LR: 8.20963e-03 | Train Acc: 99.35% | Loss: 0.0211\n",
            "Validation Epoch 164: Acc 89.60% | Loss 0.4683\n",
            "Epoch 164 complete | Best Acc: 89.60%\n",
            "\n",
            "Epoch [165] [0/196] LR 7.78360e-03  Loss 0.0250 (0.0250)  Acc 99.61% (99.61%)\n",
            "Epoch [165] [100/196] LR 7.78360e-03  Loss 0.0140 (0.0219)  Acc 99.61% (99.27%)\n",
            " Epoch 165 done in 19.3s | LR: 7.78360e-03 | Train Acc: 99.32% | Loss: 0.0213\n",
            "Validation Epoch 165: Acc 89.57% | Loss 0.4592\n",
            "Epoch 165 complete | Best Acc: 89.60%\n",
            "\n",
            "Epoch [166] [0/196] LR 7.36799e-03  Loss 0.0035 (0.0035)  Acc 100.00% (100.00%)\n",
            "Epoch [166] [100/196] LR 7.36799e-03  Loss 0.0185 (0.0163)  Acc 99.61% (99.45%)\n",
            " Epoch 166 done in 19.1s | LR: 7.36799e-03 | Train Acc: 99.45% | Loss: 0.0169\n",
            "Validation Epoch 166: Acc 89.48% | Loss 0.4765\n",
            "Epoch 166 complete | Best Acc: 89.60%\n",
            "\n",
            "Epoch [167] [0/196] LR 6.96290e-03  Loss 0.0370 (0.0370)  Acc 99.22% (99.22%)\n",
            "Epoch [167] [100/196] LR 6.96290e-03  Loss 0.0080 (0.0203)  Acc 100.00% (99.39%)\n",
            " Epoch 167 done in 20.1s | LR: 6.96290e-03 | Train Acc: 99.43% | Loss: 0.0190\n",
            "Validation Epoch 167: Acc 89.50% | Loss 0.4664\n",
            "Epoch 167 complete | Best Acc: 89.60%\n",
            "\n",
            "Epoch [168] [0/196] LR 6.56842e-03  Loss 0.0183 (0.0183)  Acc 99.22% (99.22%)\n",
            "Epoch [168] [100/196] LR 6.56842e-03  Loss 0.0041 (0.0154)  Acc 100.00% (99.53%)\n",
            " Epoch 168 done in 20.0s | LR: 6.56842e-03 | Train Acc: 99.47% | Loss: 0.0163\n",
            "Validation Epoch 168: Acc 89.55% | Loss 0.4755\n",
            "Epoch 168 complete | Best Acc: 89.60%\n",
            "\n",
            "Epoch [169] [0/196] LR 6.18467e-03  Loss 0.0086 (0.0086)  Acc 99.61% (99.61%)\n",
            "Epoch [169] [100/196] LR 6.18467e-03  Loss 0.0093 (0.0178)  Acc 100.00% (99.42%)\n",
            " Epoch 169 done in 20.0s | LR: 6.18467e-03 | Train Acc: 99.37% | Loss: 0.0193\n",
            "Validation Epoch 169: Acc 89.66% | Loss 0.4765\n",
            "Epoch 169 complete | Best Acc: 89.66%\n",
            "\n",
            "Epoch [170] [0/196] LR 5.81172e-03  Loss 0.0122 (0.0122)  Acc 99.61% (99.61%)\n",
            "Epoch [170] [100/196] LR 5.81172e-03  Loss 0.0216 (0.0184)  Acc 99.22% (99.41%)\n",
            " Epoch 170 done in 19.1s | LR: 5.81172e-03 | Train Acc: 99.44% | Loss: 0.0176\n",
            "Validation Epoch 170: Acc 89.74% | Loss 0.4754\n",
            "Epoch 170 complete | Best Acc: 89.74%\n",
            "\n",
            "Epoch [171] [0/196] LR 5.44967e-03  Loss 0.0204 (0.0204)  Acc 99.22% (99.22%)\n",
            "Epoch [171] [100/196] LR 5.44967e-03  Loss 0.0179 (0.0150)  Acc 99.22% (99.52%)\n",
            " Epoch 171 done in 19.6s | LR: 5.44967e-03 | Train Acc: 99.49% | Loss: 0.0156\n",
            "Validation Epoch 171: Acc 89.60% | Loss 0.4779\n",
            "Epoch 171 complete | Best Acc: 89.74%\n",
            "\n",
            "Epoch [172] [0/196] LR 5.09862e-03  Loss 0.0157 (0.0157)  Acc 99.22% (99.22%)\n",
            "Epoch [172] [100/196] LR 5.09862e-03  Loss 0.0378 (0.0153)  Acc 98.83% (99.53%)\n",
            " Epoch 172 done in 20.3s | LR: 5.09862e-03 | Train Acc: 99.48% | Loss: 0.0167\n",
            "Validation Epoch 172: Acc 89.56% | Loss 0.4667\n",
            "Epoch 172 complete | Best Acc: 89.74%\n",
            "\n",
            "Epoch [173] [0/196] LR 4.75865e-03  Loss 0.0035 (0.0035)  Acc 100.00% (100.00%)\n",
            "Epoch [173] [100/196] LR 4.75865e-03  Loss 0.0070 (0.0159)  Acc 99.61% (99.54%)\n",
            " Epoch 173 done in 19.9s | LR: 4.75865e-03 | Train Acc: 99.55% | Loss: 0.0151\n",
            "Validation Epoch 173: Acc 89.69% | Loss 0.4737\n",
            "Epoch 173 complete | Best Acc: 89.74%\n",
            "\n",
            "Epoch [174] [0/196] LR 4.42984e-03  Loss 0.0543 (0.0543)  Acc 98.83% (98.83%)\n",
            "Epoch [174] [100/196] LR 4.42984e-03  Loss 0.0033 (0.0154)  Acc 100.00% (99.57%)\n",
            " Epoch 174 done in 20.1s | LR: 4.42984e-03 | Train Acc: 99.54% | Loss: 0.0153\n",
            "Validation Epoch 174: Acc 89.80% | Loss 0.4640\n",
            "Epoch 174 complete | Best Acc: 89.80%\n",
            "\n",
            "Epoch [175] [0/196] LR 4.11227e-03  Loss 0.0152 (0.0152)  Acc 99.22% (99.22%)\n",
            "Epoch [175] [100/196] LR 4.11227e-03  Loss 0.0163 (0.0163)  Acc 99.22% (99.49%)\n",
            " Epoch 175 done in 19.6s | LR: 4.11227e-03 | Train Acc: 99.50% | Loss: 0.0156\n",
            "Validation Epoch 175: Acc 89.80% | Loss 0.4605\n",
            "Epoch 175 complete | Best Acc: 89.80%\n",
            "\n",
            "Epoch [176] [0/196] LR 3.80602e-03  Loss 0.0232 (0.0232)  Acc 99.22% (99.22%)\n",
            "Epoch [176] [100/196] LR 3.80602e-03  Loss 0.0034 (0.0144)  Acc 100.00% (99.55%)\n",
            " Epoch 176 done in 19.4s | LR: 3.80602e-03 | Train Acc: 99.57% | Loss: 0.0139\n",
            "Validation Epoch 176: Acc 89.91% | Loss 0.4626\n",
            "Epoch 176 complete | Best Acc: 89.91%\n",
            "\n",
            "Epoch [177] [0/196] LR 3.51118e-03  Loss 0.0161 (0.0161)  Acc 99.61% (99.61%)\n",
            "Epoch [177] [100/196] LR 3.51118e-03  Loss 0.0066 (0.0154)  Acc 99.61% (99.54%)\n",
            " Epoch 177 done in 20.3s | LR: 3.51118e-03 | Train Acc: 99.53% | Loss: 0.0153\n",
            "Validation Epoch 177: Acc 89.94% | Loss 0.4686\n",
            "Epoch 177 complete | Best Acc: 89.94%\n",
            "\n",
            "Epoch [178] [0/196] LR 3.22780e-03  Loss 0.0038 (0.0038)  Acc 100.00% (100.00%)\n",
            "Epoch [178] [100/196] LR 3.22780e-03  Loss 0.0036 (0.0136)  Acc 100.00% (99.59%)\n",
            " Epoch 178 done in 19.6s | LR: 3.22780e-03 | Train Acc: 99.61% | Loss: 0.0130\n",
            "Validation Epoch 178: Acc 89.79% | Loss 0.4647\n",
            "Epoch 178 complete | Best Acc: 89.94%\n",
            "\n",
            "Epoch [179] [0/196] LR 2.95596e-03  Loss 0.0065 (0.0065)  Acc 100.00% (100.00%)\n",
            "Epoch [179] [100/196] LR 2.95596e-03  Loss 0.0320 (0.0127)  Acc 99.61% (99.61%)\n",
            " Epoch 179 done in 20.0s | LR: 2.95596e-03 | Train Acc: 99.63% | Loss: 0.0123\n",
            "Validation Epoch 179: Acc 90.04% | Loss 0.4614\n",
            "Epoch 179 complete | Best Acc: 90.04%\n",
            "\n",
            "Epoch [180] [0/196] LR 2.69573e-03  Loss 0.0139 (0.0139)  Acc 99.61% (99.61%)\n",
            "Epoch [180] [100/196] LR 2.69573e-03  Loss 0.0550 (0.0132)  Acc 98.44% (99.56%)\n",
            " Epoch 180 done in 19.6s | LR: 2.69573e-03 | Train Acc: 99.58% | Loss: 0.0133\n",
            "Validation Epoch 180: Acc 90.16% | Loss 0.4677\n",
            "Epoch 180 complete | Best Acc: 90.16%\n",
            "\n",
            "Epoch [181] [0/196] LR 2.44717e-03  Loss 0.0183 (0.0183)  Acc 99.22% (99.22%)\n",
            "Epoch [181] [100/196] LR 2.44717e-03  Loss 0.0051 (0.0132)  Acc 100.00% (99.59%)\n",
            " Epoch 181 done in 19.5s | LR: 2.44717e-03 | Train Acc: 99.60% | Loss: 0.0124\n",
            "Validation Epoch 181: Acc 90.01% | Loss 0.4672\n",
            "Epoch 181 complete | Best Acc: 90.16%\n",
            "\n",
            "Epoch [182] [0/196] LR 2.21035e-03  Loss 0.0142 (0.0142)  Acc 99.22% (99.22%)\n",
            "Epoch [182] [100/196] LR 2.21035e-03  Loss 0.0028 (0.0104)  Acc 100.00% (99.64%)\n",
            " Epoch 182 done in 20.3s | LR: 2.21035e-03 | Train Acc: 99.63% | Loss: 0.0114\n",
            "Validation Epoch 182: Acc 89.82% | Loss 0.4759\n",
            "Epoch 182 complete | Best Acc: 90.16%\n",
            "\n",
            "Epoch [183] [0/196] LR 1.98532e-03  Loss 0.0038 (0.0038)  Acc 100.00% (100.00%)\n",
            "Epoch [183] [100/196] LR 1.98532e-03  Loss 0.0018 (0.0108)  Acc 100.00% (99.70%)\n",
            " Epoch 183 done in 19.7s | LR: 1.98532e-03 | Train Acc: 99.67% | Loss: 0.0111\n",
            "Validation Epoch 183: Acc 89.95% | Loss 0.4646\n",
            "Epoch 183 complete | Best Acc: 90.16%\n",
            "\n",
            "Epoch [184] [0/196] LR 1.77213e-03  Loss 0.0023 (0.0023)  Acc 100.00% (100.00%)\n",
            "Epoch [184] [100/196] LR 1.77213e-03  Loss 0.0146 (0.0104)  Acc 99.22% (99.70%)\n",
            " Epoch 184 done in 19.6s | LR: 1.77213e-03 | Train Acc: 99.68% | Loss: 0.0108\n",
            "Validation Epoch 184: Acc 90.04% | Loss 0.4637\n",
            "Epoch 184 complete | Best Acc: 90.16%\n",
            "\n",
            "Epoch [185] [0/196] LR 1.57084e-03  Loss 0.0129 (0.0129)  Acc 99.61% (99.61%)\n",
            "Epoch [185] [100/196] LR 1.57084e-03  Loss 0.0039 (0.0103)  Acc 100.00% (99.68%)\n",
            " Epoch 185 done in 19.2s | LR: 1.57084e-03 | Train Acc: 99.69% | Loss: 0.0100\n",
            "Validation Epoch 185: Acc 90.08% | Loss 0.4618\n",
            "Epoch 185 complete | Best Acc: 90.16%\n",
            "\n",
            "Epoch [186] [0/196] LR 1.38150e-03  Loss 0.0128 (0.0128)  Acc 99.61% (99.61%)\n",
            "Epoch [186] [100/196] LR 1.38150e-03  Loss 0.0019 (0.0128)  Acc 100.00% (99.62%)\n",
            " Epoch 186 done in 19.6s | LR: 1.38150e-03 | Train Acc: 99.61% | Loss: 0.0129\n",
            "Validation Epoch 186: Acc 89.81% | Loss 0.4683\n",
            "Epoch 186 complete | Best Acc: 90.16%\n",
            "\n",
            "Epoch [187] [0/196] LR 1.20416e-03  Loss 0.0036 (0.0036)  Acc 100.00% (100.00%)\n",
            "Epoch [187] [100/196] LR 1.20416e-03  Loss 0.0288 (0.0092)  Acc 99.22% (99.76%)\n",
            " Epoch 187 done in 20.2s | LR: 1.20416e-03 | Train Acc: 99.71% | Loss: 0.0101\n",
            "Validation Epoch 187: Acc 89.88% | Loss 0.4739\n",
            "Epoch 187 complete | Best Acc: 90.16%\n",
            "\n",
            "Epoch [188] [0/196] LR 1.03886e-03  Loss 0.0133 (0.0133)  Acc 99.61% (99.61%)\n",
            "Epoch [188] [100/196] LR 1.03886e-03  Loss 0.0206 (0.0099)  Acc 99.22% (99.66%)\n",
            " Epoch 188 done in 20.0s | LR: 1.03886e-03 | Train Acc: 99.66% | Loss: 0.0106\n",
            "Validation Epoch 188: Acc 90.03% | Loss 0.4676\n",
            "Epoch 188 complete | Best Acc: 90.16%\n",
            "\n",
            "Epoch [189] [0/196] LR 8.85637e-04  Loss 0.0020 (0.0020)  Acc 100.00% (100.00%)\n",
            "Epoch [189] [100/196] LR 8.85637e-04  Loss 0.0127 (0.0094)  Acc 99.61% (99.68%)\n",
            " Epoch 189 done in 19.0s | LR: 8.85637e-04 | Train Acc: 99.66% | Loss: 0.0106\n",
            "Validation Epoch 189: Acc 90.11% | Loss 0.4687\n",
            "Epoch 189 complete | Best Acc: 90.16%\n",
            "\n",
            "Epoch [190] [0/196] LR 7.44534e-04  Loss 0.0046 (0.0046)  Acc 100.00% (100.00%)\n",
            "Epoch [190] [100/196] LR 7.44534e-04  Loss 0.0023 (0.0106)  Acc 100.00% (99.68%)\n",
            " Epoch 190 done in 19.4s | LR: 7.44534e-04 | Train Acc: 99.70% | Loss: 0.0100\n",
            "Validation Epoch 190: Acc 90.46% | Loss 0.4585\n",
            "Epoch 190 complete | Best Acc: 90.46%\n",
            "\n",
            "Epoch [191] [0/196] LR 6.15583e-04  Loss 0.0020 (0.0020)  Acc 100.00% (100.00%)\n",
            "Epoch [191] [100/196] LR 6.15583e-04  Loss 0.0062 (0.0097)  Acc 100.00% (99.70%)\n",
            " Epoch 191 done in 19.7s | LR: 6.15583e-04 | Train Acc: 99.71% | Loss: 0.0098\n",
            "Validation Epoch 191: Acc 89.83% | Loss 0.4670\n",
            "Epoch 191 complete | Best Acc: 90.46%\n",
            "\n",
            "Epoch [192] [0/196] LR 4.98817e-04  Loss 0.0043 (0.0043)  Acc 100.00% (100.00%)\n",
            "Epoch [192] [100/196] LR 4.98817e-04  Loss 0.0050 (0.0097)  Acc 100.00% (99.73%)\n",
            " Epoch 192 done in 20.1s | LR: 4.98817e-04 | Train Acc: 99.71% | Loss: 0.0101\n",
            "Validation Epoch 192: Acc 90.01% | Loss 0.4688\n",
            "Epoch 192 complete | Best Acc: 90.46%\n",
            "\n",
            "Epoch [193] [0/196] LR 3.94265e-04  Loss 0.0150 (0.0150)  Acc 99.22% (99.22%)\n",
            "Epoch [193] [100/196] LR 3.94265e-04  Loss 0.0129 (0.0093)  Acc 99.61% (99.75%)\n",
            " Epoch 193 done in 19.7s | LR: 3.94265e-04 | Train Acc: 99.70% | Loss: 0.0101\n",
            "Validation Epoch 193: Acc 89.97% | Loss 0.4696\n",
            "Epoch 193 complete | Best Acc: 90.46%\n",
            "\n",
            "Epoch [194] [0/196] LR 3.01952e-04  Loss 0.0044 (0.0044)  Acc 100.00% (100.00%)\n",
            "Epoch [194] [100/196] LR 3.01952e-04  Loss 0.0303 (0.0103)  Acc 99.61% (99.70%)\n",
            " Epoch 194 done in 19.3s | LR: 3.01952e-04 | Train Acc: 99.70% | Loss: 0.0100\n",
            "Validation Epoch 194: Acc 89.98% | Loss 0.4674\n",
            "Epoch 194 complete | Best Acc: 90.46%\n",
            "\n",
            "Epoch [195] [0/196] LR 2.21902e-04  Loss 0.0016 (0.0016)  Acc 100.00% (100.00%)\n",
            "Epoch [195] [100/196] LR 2.21902e-04  Loss 0.0016 (0.0095)  Acc 100.00% (99.75%)\n",
            " Epoch 195 done in 18.9s | LR: 2.21902e-04 | Train Acc: 99.73% | Loss: 0.0091\n",
            "Validation Epoch 195: Acc 90.43% | Loss 0.4566\n",
            "Epoch 195 complete | Best Acc: 90.46%\n",
            "\n",
            "Epoch [196] [0/196] LR 1.54133e-04  Loss 0.0052 (0.0052)  Acc 99.61% (99.61%)\n",
            "Epoch [196] [100/196] LR 1.54133e-04  Loss 0.0149 (0.0107)  Acc 99.22% (99.68%)\n",
            " Epoch 196 done in 20.0s | LR: 1.54133e-04 | Train Acc: 99.69% | Loss: 0.0100\n",
            "Validation Epoch 196: Acc 90.06% | Loss 0.4657\n",
            "Epoch 196 complete | Best Acc: 90.46%\n",
            "\n",
            "Epoch [197] [0/196] LR 9.86636e-05  Loss 0.0016 (0.0016)  Acc 100.00% (100.00%)\n",
            "Epoch [197] [100/196] LR 9.86636e-05  Loss 0.0124 (0.0108)  Acc 99.61% (99.69%)\n",
            " Epoch 197 done in 20.6s | LR: 9.86636e-05 | Train Acc: 99.71% | Loss: 0.0101\n",
            "Validation Epoch 197: Acc 90.06% | Loss 0.4728\n",
            "Epoch 197 complete | Best Acc: 90.46%\n",
            "\n",
            "Epoch [198] [0/196] LR 5.55063e-05  Loss 0.0127 (0.0127)  Acc 99.61% (99.61%)\n",
            "Epoch [198] [100/196] LR 5.55063e-05  Loss 0.0049 (0.0099)  Acc 99.61% (99.70%)\n",
            " Epoch 198 done in 20.5s | LR: 5.55063e-05 | Train Acc: 99.73% | Loss: 0.0092\n",
            "Validation Epoch 198: Acc 90.45% | Loss 0.4520\n",
            "Epoch 198 complete | Best Acc: 90.46%\n",
            "\n",
            "Epoch [199] [0/196] LR 2.46720e-05  Loss 0.0065 (0.0065)  Acc 100.00% (100.00%)\n",
            "Epoch [199] [100/196] LR 2.46720e-05  Loss 0.0077 (0.0086)  Acc 99.61% (99.77%)\n",
            " Epoch 199 done in 19.8s | LR: 2.46720e-05 | Train Acc: 99.75% | Loss: 0.0092\n",
            "Validation Epoch 199: Acc 90.13% | Loss 0.4684\n",
            "Epoch 199 complete | Best Acc: 90.46%\n",
            "\n",
            "Epoch [200] [0/196] LR 6.16838e-06  Loss 0.0029 (0.0029)  Acc 100.00% (100.00%)\n",
            "Epoch [200] [100/196] LR 6.16838e-06  Loss 0.0021 (0.0103)  Acc 100.00% (99.71%)\n",
            " Epoch 200 done in 19.4s | LR: 6.16838e-06 | Train Acc: 99.73% | Loss: 0.0096\n",
            "Validation Epoch 200: Acc 90.25% | Loss 0.4546\n",
            "Epoch 200 complete | Best Acc: 90.46%\n",
            "\n",
            "Training completed. Best accuracy: 90.46%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decreased-harris",
      "metadata": {
        "id": "decreased-harris"
      },
      "outputs": [],
      "source": [
        "# HW\n",
        "\n",
        "#  1. Train with 4 bits for both weight and activation to achieve >90% accuracy\n",
        "#  2. Find x_int and w_int for the 2nd convolution layer\n",
        "#  3. Check the recovered psum has similar value to the un-quantized original psum\n",
        "#     (such as example 1 in W3S2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "entertaining-queensland",
      "metadata": {
        "id": "entertaining-queensland",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f7459ade-388d-48a8-e592-941b0e6e94b2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for VGG_quant:\n\tsize mismatch for features.34.weight: copying a param with shape torch.Size([16, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for features.34.weight_q: copying a param with shape torch.Size([16, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for features.35.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.35.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.35.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.35.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.37.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for features.37.weight_q: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for features.38.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.38.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.38.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.38.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.40.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for features.40.weight_q: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for features.41.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.41.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.41.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.41.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1934533436.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"result/VGG16_quant/model_best.pth.tar\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2629\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2630\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2631\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VGG_quant:\n\tsize mismatch for features.34.weight: copying a param with shape torch.Size([16, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for features.34.weight_q: copying a param with shape torch.Size([16, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for features.35.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.35.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.35.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.35.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.37.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for features.37.weight_q: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for features.38.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.38.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.38.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.38.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.40.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for features.40.weight_q: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for features.41.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.41.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.41.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features.41.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([512])."
          ]
        }
      ],
      "source": [
        "PATH = \"result/VGG16_quant/model_best.pth.tar\"\n",
        "checkpoint = torch.load(PATH)\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "model.cuda()\n",
        "model.eval()\n",
        "\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device) # loading to GPU\n",
        "        output = model(data)\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "\n",
        "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e451c793"
      },
      "source": [
        "PATH = \"result/VGG16_quant/model_best.pth.tar\"\n",
        "checkpoint = torch.load(PATH)\n",
        "\n",
        "print(\"Keys in the state_dict from model_best.pth.tar:\")\n",
        "for key, value in checkpoint['state_dict'].items():\n",
        "    print(f\"  {key}: {value.shape}\")"
      ],
      "id": "e451c793",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SaveOutput:\n",
        "    def __init__(self):\n",
        "        self.outputs = []\n",
        "    def __call__(self, module, module_in):\n",
        "        self.outputs.append(module_in)\n",
        "    def clear(self):\n",
        "        self.outputs = []\n",
        "\n",
        "######### Save inputs from selected layer ##########\n",
        "save_output = SaveOutput()\n",
        "i = 0\n",
        "\n",
        "for layer in model.modules():\n",
        "    i = i+1\n",
        "    if isinstance(layer, QuantConv2d):\n",
        "        print(i,\"-th layer prehooked\")\n",
        "        layer.register_forward_pre_hook(save_output)\n",
        "####################################################\n",
        "\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "images = images.to(device)\n",
        "out = model(images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBiFVrEYnrdU",
        "outputId": "63e92cea-4175-4719-ad54-c0d008e8564a"
      },
      "id": "tBiFVrEYnrdU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 -th layer prehooked\n",
            "7 -th layer prehooked\n",
            "12 -th layer prehooked\n",
            "16 -th layer prehooked\n",
            "21 -th layer prehooked\n",
            "25 -th layer prehooked\n",
            "29 -th layer prehooked\n",
            "34 -th layer prehooked\n",
            "38 -th layer prehooked\n",
            "42 -th layer prehooked\n",
            "47 -th layer prehooked\n",
            "51 -th layer prehooked\n",
            "55 -th layer prehooked\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight_q = model.features[27].weight_q\n",
        "w_alpha = model.features[27].weight_quant.wgt_alpha\n",
        "w_bit = 4\n",
        "\n",
        "weight_int = weight_q / (w_alpha / (2**(w_bit-1)-1))\n",
        "print(weight_int)"
      ],
      "metadata": {
        "id": "1jl0aVdEoZW8"
      },
      "id": "1jl0aVdEoZW8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "act = save_output.outputs[1][0]   # check this\n",
        "act_alpha  = model.features[27].act_alpha\n",
        "act_bit = 4\n",
        "act_quant_fn = act_quantization(act_bit)\n",
        "\n",
        "act_q = act_quant_fn(act, act_alpha)\n",
        "\n",
        "act_int = act_q / (act_alpha / (2**act_bit-1))\n",
        "print(act_int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQKa_lRtoc-u",
        "outputId": "587ada89-9c3a-4ef2-e950-9ee1a320259a"
      },
      "id": "EQKa_lRtoc-u",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 0.0000,  0.0000,  0.0000,  ...,  9.0000,  9.0000,  6.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 2.0000,  8.0000, 12.0000,  ...,  9.0000, 10.0000,  7.0000],\n",
            "          [ 8.0000,  9.0000, 12.0000,  ...,  9.0000,  7.0000,  5.0000],\n",
            "          [ 0.0000,  0.0000,  2.0000,  ...,  3.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 9.0000,  9.0000,  5.0000,  ...,  9.0000,  9.0000,  8.0000],\n",
            "          [ 3.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  3.0000],\n",
            "          [ 6.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  3.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  5.0000, 10.0000,  ...,  0.0000,  2.0000, 15.0000],\n",
            "          [ 0.0000,  0.0000, 11.0000,  ...,  1.0000,  0.0000, 10.0000],\n",
            "          [ 0.0000,  8.0000, 12.0000,  ..., 13.0000,  7.0000, 15.0000]],\n",
            "\n",
            "         [[ 3.0000,  5.0000,  5.0000,  ...,  8.0000,  8.0000,  8.0000],\n",
            "          [ 0.0000,  2.0000,  5.0000,  ...,  2.0000,  2.0000,  4.0000],\n",
            "          [ 1.0000,  3.0000,  3.0000,  ...,  2.0000,  2.0000,  4.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  3.0000],\n",
            "          [ 1.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  2.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  3.0000,  1.0000,  ...,  6.0000,  6.0000,  6.0000],\n",
            "          [ 0.0000,  2.0000,  3.0000,  ...,  6.0000,  6.0000,  6.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 6.0000,  8.0000,  8.0000,  ...,  7.0000,  6.0000,  3.0000],\n",
            "          [ 7.0000,  9.0000,  6.0000,  ...,  5.0000,  5.0000,  3.0000],\n",
            "          [ 3.0000,  5.0000,  5.0000,  ...,  6.0000,  6.0000,  5.0000]],\n",
            "\n",
            "         [[ 7.0000,  7.0000,  3.0000,  ...,  5.0000,  5.0000,  0.0000],\n",
            "          [ 0.0000,  2.0000,  1.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
            "          [ 0.0000,  0.0000,  2.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  4.0000,  1.0000],\n",
            "          [ 0.0000,  2.0000,  2.0000,  ...,  0.0000,  1.0000,  2.0000],\n",
            "          [ 9.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [12.0000, 15.0000, 10.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 9.0000,  7.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 3.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  2.0000]],\n",
            "\n",
            "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  0.0000],\n",
            "          [15.0000, 12.0000, 12.0000,  ..., 12.0000, 12.0000,  0.0000],\n",
            "          [15.0000, 12.0000, 12.0000,  ..., 12.0000, 12.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  6.0000,  1.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  5.0000,  1.0000],\n",
            "          [ 0.0000,  0.0000,  6.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [12.0000,  3.0000,  3.0000,  ...,  3.0000,  3.0000,  0.0000],\n",
            "          [12.0000,  3.0000,  3.0000,  ...,  3.0000,  3.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  2.0000,  2.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  5.0000,  3.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  8.0000,  8.0000,  2.0000]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  2.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 7.0000, 10.0000, 10.0000,  ..., 10.0000, 10.0000,  4.0000],\n",
            "          [ 2.0000,  7.0000,  7.0000,  ...,  7.0000,  7.0000,  4.0000],\n",
            "          [ 2.0000,  7.0000,  7.0000,  ...,  7.0000,  7.0000,  4.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  5.0000,  5.0000,  2.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  2.0000,  4.0000,  2.0000],\n",
            "          [ 3.0000,  4.0000,  4.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  5.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 2.0000,  3.0000, 11.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 1.0000,  5.0000,  4.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [12.0000, 15.0000, 15.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [15.0000,  7.0000,  6.0000,  ..., 15.0000, 15.0000, 12.0000],\n",
            "          [15.0000,  6.0000,  7.0000,  ..., 15.0000, 15.0000,  5.0000],\n",
            "          [ 2.0000,  0.0000,  0.0000,  ...,  7.0000,  2.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  2.0000,  ...,  1.0000,  1.0000,  0.0000],\n",
            "          [ 5.0000,  8.0000,  6.0000,  ..., 12.0000, 12.0000,  0.0000],\n",
            "          [ 6.0000, 11.0000,  9.0000,  ..., 12.0000, 12.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  4.0000,  0.0000, 11.0000],\n",
            "          [ 0.0000,  2.0000,  2.0000,  ...,  0.0000,  0.0000, 14.0000],\n",
            "          [ 0.0000, 15.0000, 12.0000,  ..., 11.0000, 15.0000, 15.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 8.0000,  0.0000,  0.0000,  ...,  3.0000,  3.0000,  0.0000],\n",
            "          [ 9.0000,  0.0000,  0.0000,  ...,  3.0000,  3.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  7.0000],\n",
            "          [ 0.0000,  1.0000,  0.0000,  ...,  0.0000,  0.0000,  6.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 4.0000,  1.0000,  1.0000,  ...,  3.0000,  4.0000, 11.0000],\n",
            "          [ 4.0000,  1.0000,  4.0000,  ...,  4.0000,  7.0000, 11.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  2.0000]],\n",
            "\n",
            "         [[10.0000, 13.0000, 11.0000,  ..., 10.0000, 10.0000,  4.0000],\n",
            "          [ 7.0000, 11.0000, 10.0000,  ...,  7.0000,  7.0000,  4.0000],\n",
            "          [ 5.0000,  9.0000,  9.0000,  ...,  7.0000,  7.0000,  4.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 7.0000,  6.0000,  7.0000,  ..., 10.0000, 10.0000,  6.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  5.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  3.0000],\n",
            "          [ 6.0000,  2.0000,  2.0000,  ...,  1.0000,  0.0000,  6.0000],\n",
            "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
            "          [15.0000, 14.0000, 14.0000,  ...,  3.0000,  4.0000,  2.0000],\n",
            "          [15.0000, 10.0000, 14.0000,  ...,  9.0000, 11.0000,  7.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
            "          [ 0.0000,  0.0000,  2.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 9.0000, 11.0000, 11.0000,  ...,  9.0000,  6.0000, 13.0000],\n",
            "          [ 0.0000,  6.0000,  6.0000,  ...,  8.0000,  0.0000, 10.0000],\n",
            "          [ 0.0000,  7.0000,  8.0000,  ..., 11.0000,  4.0000, 13.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  6.0000,  3.0000,  ...,  9.0000,  4.0000, 14.0000]],\n",
            "\n",
            "         [[ 5.0000, 15.0000, 15.0000,  ..., 12.0000, 11.0000, 12.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  2.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  3.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  2.0000,  3.0000,  3.0000],\n",
            "          [ 1.0000,  2.0000,  0.0000,  ...,  2.0000,  2.0000,  3.0000],\n",
            "          [ 1.0000,  2.0000,  0.0000,  ...,  0.0000,  0.0000,  2.0000]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[11.0000, 11.0000, 11.0000,  ...,  6.0000,  5.0000,  6.0000],\n",
            "          [ 7.0000,  4.0000,  4.0000,  ...,  0.0000,  0.0000,  3.0000],\n",
            "          [ 5.0000,  1.0000,  3.0000,  ...,  0.0000,  0.0000,  6.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 2.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 1.0000,  0.0000,  2.0000,  ...,  3.0000,  4.0000,  3.0000]],\n",
            "\n",
            "         [[ 8.0000,  0.0000,  0.0000,  ...,  0.0000,  4.0000,  0.0000],\n",
            "          [ 5.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  5.0000],\n",
            "          [15.0000, 14.0000, 12.0000,  ...,  1.0000,  0.0000,  3.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  6.0000,  ...,  4.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 6.0000,  6.0000,  3.0000,  ..., 15.0000, 15.0000, 15.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 9.0000,  0.0000,  0.0000,  ..., 14.0000, 13.0000,  4.0000],\n",
            "          [11.0000,  2.0000,  0.0000,  ..., 15.0000, 10.0000,  4.0000],\n",
            "          [ 3.0000,  0.0000,  0.0000,  ...,  3.0000,  2.0000,  3.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[15.0000,  4.0000,  4.0000,  ..., 15.0000, 15.0000, 13.0000],\n",
            "          [15.0000, 12.0000,  9.0000,  ..., 15.0000, 15.0000, 12.0000],\n",
            "          [15.0000, 12.0000, 10.0000,  ..., 15.0000, 15.0000,  9.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  2.0000,  0.0000,  1.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 3.0000,  5.0000,  0.0000,  ...,  8.0000,  8.0000,  8.0000],\n",
            "          [ 1.0000,  7.0000,  5.0000,  ...,  3.0000,  3.0000,  5.0000],\n",
            "          [ 0.0000,  6.0000,  5.0000,  ...,  0.0000,  0.0000,  3.0000],\n",
            "          ...,\n",
            "          [ 4.0000,  1.0000,  1.0000,  ...,  4.0000,  3.0000,  0.0000],\n",
            "          [ 3.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
            "          [ 5.0000,  1.0000,  1.0000,  ...,  1.0000,  4.0000,  1.0000]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 6.0000,  0.0000,  0.0000,  ...,  7.0000,  8.0000,  0.0000],\n",
            "          [13.0000,  4.0000,  0.0000,  ..., 15.0000, 15.0000, 14.0000],\n",
            "          [ 6.0000,  0.0000,  0.0000,  ..., 15.0000, 15.0000, 12.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 1.0000,  3.0000,  3.0000,  ...,  3.0000,  3.0000,  2.0000],\n",
            "          [ 6.0000,  9.0000,  9.0000,  ...,  5.0000,  5.0000,  4.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  ...,  2.0000,  3.0000,  0.0000],\n",
            "          [ 6.0000,  0.0000,  0.0000,  ...,  5.0000,  6.0000,  0.0000],\n",
            "          [ 2.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 2.0000,  5.0000,  5.0000,  ...,  0.0000,  0.0000,  2.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  1.0000,  2.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 9.0000,  5.0000,  8.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 9.0000,  4.0000,  4.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  3.0000,  1.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  3.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  9.0000,  2.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  4.0000,  8.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 3.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  6.0000],\n",
            "          [ 3.0000,  9.0000, 10.0000,  ...,  0.0000,  0.0000,  4.0000],\n",
            "          [ 0.0000,  0.0000,  1.0000,  ...,  8.0000,  5.0000, 12.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  3.0000,  ...,  4.0000,  1.0000,  0.0000],\n",
            "          [ 1.0000,  0.0000,  2.0000,  ...,  8.0000,  7.0000,  2.0000],\n",
            "          [ 5.0000,  0.0000,  3.0000,  ...,  2.0000,  5.0000,  6.0000],\n",
            "          ...,\n",
            "          [ 2.0000,  7.0000,  6.0000,  ...,  0.0000,  0.0000,  4.0000],\n",
            "          [ 0.0000,  3.0000,  4.0000,  ...,  0.0000,  0.0000,  2.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  2.0000,  0.0000,  2.0000]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
            "          [ 1.0000,  0.0000,  3.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 1.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  0.0000]],\n",
            "\n",
            "         [[ 8.0000,  9.0000,  2.0000,  ...,  0.0000,  0.0000,  2.0000],\n",
            "          [ 0.0000,  5.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  ..., 12.0000,  2.0000,  0.0000],\n",
            "          ...,\n",
            "          [ 0.0000,  0.0000,  0.0000,  ...,  5.0000, 10.0000,  7.0000],\n",
            "          [13.0000, 15.0000,  5.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "          [ 2.0000,  3.0000, 12.0000,  ...,  8.0000, 12.0000, 10.0000]]]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Changed the code to mach the dimensions\n",
        "conv_int = torch.nn.Conv2d(in_channels = 16, out_channels=16, kernel_size = 3, padding=1)\n",
        "conv_int.weight = torch.nn.parameter.Parameter(weight_int)\n",
        "conv_int.bias = model.features[3].bias\n",
        "output_int = conv_int(act_int)\n",
        "output_recovered = output_int * (act_alpha / (2**act_bit-1)) * (w_alpha / (2**(w_bit-1)-1))\n",
        "print(output_recovered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz0_Bfvuof6D",
        "outputId": "9300eeda-94aa-4b5b-af05-70df58cb62db"
      },
      "id": "iz0_Bfvuof6D",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-1.8148e+00, -6.0575e+00, -4.2911e+00,  ..., -4.3314e+00,\n",
            "           -7.4448e+00, -1.1131e+00],\n",
            "          [-3.0247e+00, -6.5173e+00, -4.2669e+00,  ..., -5.4687e+00,\n",
            "           -5.2993e+00, -1.0002e+00],\n",
            "          [-5.3235e+00, -2.5004e+00, -5.8720e+00,  ..., -4.9202e+00,\n",
            "           -5.0977e+00, -2.3875e+00],\n",
            "          ...,\n",
            "          [-1.0445e+01, -1.0203e+01, -1.0566e+01,  ..., -1.4882e+01,\n",
            "            3.4038e+00, -1.4382e+01],\n",
            "          [-1.3970e+01, -7.5336e+00, -1.1187e+01,  ..., -5.4042e-01,\n",
            "           -6.8641e+00, -9.5500e+00],\n",
            "          [-5.8478e+00, -5.2025e+00, -4.0410e+00,  ...,  4.3637e+00,\n",
            "           -2.2988e+00,  3.2909e+00]],\n",
            "\n",
            "         [[ 7.1222e+00,  1.2712e+01,  9.3081e+00,  ...,  9.9856e+00,\n",
            "            8.7999e+00,  4.7831e+00],\n",
            "          [ 1.2494e+01,  9.4210e+00,  7.7675e+00,  ...,  5.4848e+00,\n",
            "            3.3716e+00, -2.5004e-01],\n",
            "          [ 8.7193e+00,  7.7513e+00,  6.7108e+00,  ...,  6.4124e+00,\n",
            "            3.4522e+00,  1.9278e+00],\n",
            "          ...,\n",
            "          [ 7.6384e+00,  6.8722e+00,  5.6784e+00,  ...,  7.1867e+00,\n",
            "            5.1461e+00,  2.8795e+00],\n",
            "          [ 5.4122e+00,  5.8317e+00,  8.4127e+00,  ...,  7.0657e+00,\n",
            "            5.3074e+00, -3.9523e-01],\n",
            "          [ 2.2827e+00,  4.8073e+00,  3.7910e+00,  ..., -1.4035e+00,\n",
            "           -3.9362e+00, -1.9358e+00]],\n",
            "\n",
            "         [[ 9.0016e+00,  1.5019e+01,  1.3857e+01,  ...,  1.6011e+01,\n",
            "            1.6382e+01,  1.4212e+01],\n",
            "          [ 9.3968e+00,  1.3825e+01,  1.2470e+01,  ...,  1.1171e+01,\n",
            "            1.3607e+01,  1.2462e+01],\n",
            "          [ 7.6062e+00,  1.0728e+01,  8.1224e+00,  ...,  9.1951e+00,\n",
            "            1.1341e+01,  1.1341e+01],\n",
            "          ...,\n",
            "          [ 1.5785e+01,  2.0778e+01,  2.6085e+01,  ...,  2.4520e+01,\n",
            "            2.6303e+01,  1.8898e+01],\n",
            "          [ 1.6164e+01,  2.3327e+01,  2.7811e+01,  ...,  2.6609e+01,\n",
            "            2.9763e+01,  2.2722e+01],\n",
            "          [ 1.5785e+01,  2.1286e+01,  2.3617e+01,  ...,  2.2988e+01,\n",
            "            2.7037e+01,  1.8447e+01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.1583e+01, -1.1462e+01, -1.1663e+01,  ..., -6.7673e+00,\n",
            "           -1.6454e+00,  2.6295e+00],\n",
            "          [-1.1091e+01, -9.3968e+00, -1.3309e+01,  ..., -7.6223e+00,\n",
            "           -5.6461e-01, -5.8881e-01],\n",
            "          [-1.0171e+01, -9.5904e+00, -1.1462e+01,  ..., -5.8317e+00,\n",
            "            3.6297e-01, -7.8239e-01],\n",
            "          ...,\n",
            "          [-1.1542e+01, -1.4204e+01, -1.0695e+01,  ..., -5.9768e+00,\n",
            "           -1.1744e+01, -4.3395e+00],\n",
            "          [-9.0177e+00, -1.3994e+01, -1.1905e+01,  ..., -4.0330e+00,\n",
            "           -7.0335e+00, -7.5013e-01],\n",
            "          [-7.9449e+00, -5.0412e+00, -4.3395e+00,  ..., -6.8561e-01,\n",
            "           -6.6947e-01, -2.2827e+00]],\n",
            "\n",
            "         [[-4.4524e+00, -4.4524e+00, -4.8234e+00,  ..., -7.4932e+00,\n",
            "           -1.0736e+01, -6.0252e+00],\n",
            "          [-4.3395e+00, -5.5897e+00, -4.3959e+00,  ..., -8.8241e+00,\n",
            "           -1.2801e+01, -7.1948e+00],\n",
            "          [-3.1618e+00, -3.7023e+00, -5.4042e+00,  ..., -7.2271e+00,\n",
            "           -1.0897e+01, -6.7915e+00],\n",
            "          ...,\n",
            "          [-6.1462e+00, -1.9189e+01, -1.2204e+01,  ..., -1.9253e+01,\n",
            "           -1.9689e+01,  2.2746e+00],\n",
            "          [-7.3077e+00, -1.8148e+01, -1.0107e+01,  ..., -1.3559e+01,\n",
            "           -1.4196e+01, -1.4680e+00],\n",
            "          [-2.6053e+00, -7.5336e+00, -4.0168e+00,  ..., -2.8473e+00,\n",
            "           -5.7268e+00,  2.0165e-01]],\n",
            "\n",
            "         [[ 1.1341e+01,  4.8315e+00,  5.9607e+00,  ...,  7.2997e+00,\n",
            "            5.7429e+00, -8.8726e-02],\n",
            "          [ 1.2349e+01,  2.4036e+00,  6.5979e+00,  ...,  2.2585e-01,\n",
            "           -1.9520e+00, -1.3712e+00],\n",
            "          [ 1.2139e+01,  2.3875e+00,  8.2756e+00,  ...,  4.1459e+00,\n",
            "            1.0889e+00,  1.5809e+00],\n",
            "          ...,\n",
            "          [ 9.2677e+00,  9.7920e+00,  8.8886e+00,  ...,  1.1696e+00,\n",
            "            5.1380e+00,  1.3906e+01],\n",
            "          [ 9.4533e+00,  1.1784e+01,  7.3239e+00,  ..., -2.2827e+00,\n",
            "            8.2192e+00,  5.2670e+00],\n",
            "          [ 8.5095e+00,  9.9453e+00,  3.6861e+00,  ...,  1.7664e+00,\n",
            "            4.2830e+00,  2.1778e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 5.9688e+00, -4.0652e+00, -9.1952e-01,  ..., -9.1952e-01,\n",
            "            3.0892e+00, -4.1701e+00],\n",
            "          [-6.1301e-01, -1.0558e+01, -6.2269e+00,  ..., -6.2269e+00,\n",
            "           -5.2025e+00, -9.9211e+00],\n",
            "          [-3.8394e+00, -9.6468e+00, -6.0494e+00,  ..., -6.0494e+00,\n",
            "           -7.4529e+00, -9.2758e+00],\n",
            "          ...,\n",
            "          [-1.8898e+01, -1.2430e+01,  7.2997e+00,  ...,  3.5651e+00,\n",
            "           -1.3857e+01, -9.0661e+00],\n",
            "          [-1.8818e+01,  6.0252e+00,  6.8964e+00,  ..., -1.1760e+01,\n",
            "           -8.9693e+00, -1.0776e+01],\n",
            "          [-4.5895e+00,  1.3494e+01, -2.0971e-01,  ..., -1.0058e+01,\n",
            "           -7.3884e+00, -8.3402e+00]],\n",
            "\n",
            "         [[ 4.7105e+00,  5.4042e+00,  6.4527e+00,  ...,  6.4527e+00,\n",
            "            4.4121e+00,  8.6225e+00],\n",
            "          [ 4.7750e+00, -5.0009e-01, -1.2744e+00,  ..., -1.2744e+00,\n",
            "           -1.6293e+00,  4.0894e+00],\n",
            "          [ 5.1622e+00,  2.5811e-01,  9.2758e-01,  ...,  9.2758e-01,\n",
            "            2.2423e+00,  4.3314e+00],\n",
            "          ...,\n",
            "          [ 1.2034e+01,  3.5490e-01, -6.7512e+00,  ..., -4.4201e+00,\n",
            "            1.8148e+00,  6.5011e+00],\n",
            "          [ 4.0814e+00, -1.0574e+01, -7.0173e+00,  ...,  2.0971e+00,\n",
            "            4.5169e+00,  8.3321e+00],\n",
            "          [-3.7426e+00, -6.0172e+00, -5.3235e-01,  ...,  1.3712e+00,\n",
            "            5.3477e+00,  3.2263e-02]],\n",
            "\n",
            "         [[ 1.0647e+01,  1.4075e+01,  1.2478e+01,  ...,  1.2478e+01,\n",
            "            1.3978e+01,  1.2486e+01],\n",
            "          [ 1.4293e+01,  1.4640e+01,  1.4180e+01,  ...,  1.4180e+01,\n",
            "            1.4930e+01,  1.4624e+01],\n",
            "          [ 1.3002e+01,  1.2236e+01,  1.3147e+01,  ...,  1.3147e+01,\n",
            "            1.3551e+01,  1.3978e+01],\n",
            "          ...,\n",
            "          [ 1.4841e+01,  2.3399e+01,  2.7368e+01,  ...,  1.3890e+01,\n",
            "            1.3817e+01,  1.3510e+01],\n",
            "          [ 1.5567e+01,  2.3157e+01,  2.6174e+01,  ...,  1.4615e+01,\n",
            "            1.4599e+01,  1.4801e+01],\n",
            "          [ 1.4470e+01,  2.0963e+01,  2.2464e+01,  ...,  9.7194e+00,\n",
            "            8.5176e+00,  8.1062e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.5411e+00, -9.7356e+00, -9.2677e+00,  ..., -9.2677e+00,\n",
            "           -1.0470e+01, -1.3196e+01],\n",
            "          [-1.0591e+01, -1.3478e+01, -1.3059e+01,  ..., -1.3059e+01,\n",
            "           -1.5608e+01, -1.3196e+01],\n",
            "          [-1.2897e+01, -1.4825e+01, -1.3744e+01,  ..., -1.3744e+01,\n",
            "           -1.5285e+01, -1.1244e+01],\n",
            "          ...,\n",
            "          [-5.8397e+00,  7.9611e+00,  8.6547e+00,  ..., -1.5543e+01,\n",
            "           -1.7680e+01, -1.0534e+01],\n",
            "          [ 5.5493e+00,  1.1276e+01,  6.4043e+00,  ..., -1.7269e+01,\n",
            "           -1.8535e+01, -9.8485e+00],\n",
            "          [ 8.9370e+00,  4.5734e+00,  1.4196e+00,  ..., -1.2599e+01,\n",
            "           -1.0687e+01, -3.7103e-01]],\n",
            "\n",
            "         [[-7.9853e+00, -6.1462e+00, -9.4613e+00,  ..., -9.4613e+00,\n",
            "           -9.1306e+00, -2.2262e+00],\n",
            "          [-3.2990e+00, -6.6866e+00, -1.0510e+01,  ..., -1.0510e+01,\n",
            "           -1.0187e+01, -4.8395e+00],\n",
            "          [-2.2988e+00, -5.4364e+00, -1.0195e+01,  ..., -1.0195e+01,\n",
            "           -9.5178e+00, -5.5574e+00],\n",
            "          ...,\n",
            "          [-7.5578e+00, -1.7479e+01, -1.3994e+01,  ...,  1.3470e+00,\n",
            "           -7.4207e-01,  4.9283e+00],\n",
            "          [-8.3321e+00, -1.6148e+01, -1.1623e+01,  ...,  1.4035e+00,\n",
            "           -7.5174e+00,  1.5083e+00],\n",
            "          [-6.0414e+00, -8.5741e+00, -5.0331e+00,  ...,  5.5493e+00,\n",
            "           -6.1301e-01,  1.6454e+00]],\n",
            "\n",
            "         [[ 7.7513e+00,  1.6003e+01,  1.2422e+01,  ...,  1.2422e+01,\n",
            "            9.2193e+00,  1.3986e+01],\n",
            "          [ 2.0899e+01,  2.7222e+01,  2.3440e+01,  ...,  2.3440e+01,\n",
            "            2.2456e+01,  1.7987e+01],\n",
            "          [ 1.8681e+01,  2.1205e+01,  1.8439e+01,  ...,  1.8439e+01,\n",
            "            1.8907e+01,  1.3478e+01],\n",
            "          ...,\n",
            "          [ 1.4938e+01,  4.3556e-01,  1.7584e+00,  ...,  1.4341e+01,\n",
            "            2.2383e+01,  1.2793e+01],\n",
            "          [ 6.7915e+00, -3.4361e+00,  6.2511e+00,  ...,  1.7519e+01,\n",
            "            2.0455e+01,  1.2542e+01],\n",
            "          [ 5.0735e+00,  9.8565e+00,  1.7656e+01,  ...,  1.1220e+01,\n",
            "            1.5898e+01,  5.9204e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 7.6787e+00, -4.7105e+00, -7.4529e+00,  ..., -9.1952e-01,\n",
            "            3.0892e+00, -4.1701e+00],\n",
            "          [ 1.0082e+00, -1.1244e+01, -9.9695e+00,  ..., -6.2269e+00,\n",
            "           -5.2025e+00, -9.9211e+00],\n",
            "          [-2.2020e+00, -1.5938e+01, -1.0881e+01,  ..., -6.0494e+00,\n",
            "           -7.4529e+00, -9.2758e+00],\n",
            "          ...,\n",
            "          [-9.2032e+00,  4.2911e+00, -6.3398e+00,  ..., -3.0554e+01,\n",
            "           -1.5995e+01,  8.9048e+00],\n",
            "          [-4.5572e+00,  1.8390e+00, -5.8397e+00,  ..., -2.0439e+01,\n",
            "            3.2667e+00,  1.1187e+01],\n",
            "          [ 6.5011e+00,  2.1859e+00,  2.0810e+00,  ...,  7.6626e+00,\n",
            "            1.6914e+01,  6.2995e+00]],\n",
            "\n",
            "         [[ 3.5813e+00,  5.7026e+00,  8.2918e+00,  ...,  6.4527e+00,\n",
            "            4.4121e+00,  8.6225e+00],\n",
            "          [ 4.5976e+00,  5.0251e+00,  7.5174e+00,  ..., -1.2744e+00,\n",
            "           -1.6293e+00,  4.0894e+00],\n",
            "          [ 7.9369e+00,  8.4369e+00,  6.1301e+00,  ...,  9.2758e-01,\n",
            "            2.2423e+00,  4.3314e+00],\n",
            "          ...,\n",
            "          [ 4.0733e+00, -3.7103e-01, -3.7103e-01,  ...,  2.6698e+01,\n",
            "            8.1224e+00, -5.1057e+00],\n",
            "          [ 1.1534e+00, -8.9532e-01, -2.0971e+00,  ...,  7.8078e+00,\n",
            "           -7.7997e+00, -1.2155e+01],\n",
            "          [-1.9923e+00, -3.5732e+00, -1.9923e+00,  ..., -8.1143e+00,\n",
            "           -1.1058e+01, -4.3233e+00]],\n",
            "\n",
            "         [[ 9.4855e+00,  1.3220e+01,  1.2559e+01,  ...,  1.2478e+01,\n",
            "            1.3978e+01,  1.2486e+01],\n",
            "          [ 1.3994e+01,  1.6616e+01,  2.0536e+01,  ...,  1.4180e+01,\n",
            "            1.4930e+01,  1.4624e+01],\n",
            "          [ 1.3107e+01,  1.4236e+01,  2.0576e+01,  ...,  1.3147e+01,\n",
            "            1.3551e+01,  1.3978e+01],\n",
            "          ...,\n",
            "          [ 1.6245e+01,  2.0883e+01,  1.9592e+01,  ...,  2.2205e+01,\n",
            "            2.6674e+01,  2.1705e+01],\n",
            "          [ 1.6164e+01,  2.3012e+01,  2.2964e+01,  ...,  2.6553e+01,\n",
            "            2.9787e+01,  2.5222e+01],\n",
            "          [ 1.6269e+01,  2.3464e+01,  2.2883e+01,  ...,  2.5916e+01,\n",
            "            2.8973e+01,  2.1254e+01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.3880e+00, -1.1115e+01, -5.6300e+00,  ..., -9.2677e+00,\n",
            "           -1.0470e+01, -1.3196e+01],\n",
            "          [-1.5495e+01, -2.1326e+01, -1.3365e+01,  ..., -1.3059e+01,\n",
            "           -1.5608e+01, -1.3196e+01],\n",
            "          [-2.1826e+01, -2.1794e+01, -1.3156e+01,  ..., -1.3744e+01,\n",
            "           -1.5285e+01, -1.1244e+01],\n",
            "          ...,\n",
            "          [ 3.3070e+00,  1.0889e+00,  6.6866e+00,  ..., -1.2139e+01,\n",
            "            6.4366e+00,  4.8395e+00],\n",
            "          [ 9.0177e+00,  5.0815e+00,  9.6146e+00,  ...,  6.2914e+00,\n",
            "            1.7543e+01,  7.2997e+00],\n",
            "          [ 6.0414e+00,  1.1696e+00,  8.8725e-02,  ...,  1.1655e+01,\n",
            "            8.0256e+00, -2.9521e+00]],\n",
            "\n",
            "         [[-6.3398e+00, -4.4201e+00, -1.1929e+01,  ..., -9.4613e+00,\n",
            "           -9.1306e+00, -2.2262e+00],\n",
            "          [-3.5409e+00, -7.6384e+00, -1.6898e+01,  ..., -1.0510e+01,\n",
            "           -1.0187e+01, -4.8395e+00],\n",
            "          [-9.1144e-01, -7.6949e+00, -1.7680e+01,  ..., -1.0195e+01,\n",
            "           -9.5178e+00, -5.5574e+00],\n",
            "          ...,\n",
            "          [-9.4936e+00, -6.6786e+00, -8.8725e+00,  ..., -1.3156e+01,\n",
            "           -2.1359e+01, -1.3252e+01],\n",
            "          [-9.6468e+00, -6.0010e+00, -1.2091e+01,  ..., -1.7551e+01,\n",
            "           -2.2052e+01, -8.2111e+00],\n",
            "          [-5.7349e+00, -2.0084e+00, -6.8318e+00,  ..., -1.2986e+01,\n",
            "           -1.0034e+01,  2.4198e-01]],\n",
            "\n",
            "         [[ 4.7508e+00,  1.3680e+01,  9.3968e+00,  ...,  1.2422e+01,\n",
            "            9.2193e+00,  1.3986e+01],\n",
            "          [ 1.8092e+01,  2.5198e+01,  1.4406e+01,  ...,  2.3440e+01,\n",
            "            2.2456e+01,  1.7987e+01],\n",
            "          [ 1.8584e+01,  2.3601e+01,  8.2595e+00,  ...,  1.8439e+01,\n",
            "            1.8907e+01,  1.3478e+01],\n",
            "          ...,\n",
            "          [ 7.1464e+00, -1.2905e+00, -1.4277e+00,  ...,  1.5898e+01,\n",
            "            8.2273e-01, -8.8724e-02],\n",
            "          [ 5.1299e+00,  6.0494e+00,  2.2423e+00,  ...,  4.7589e-01,\n",
            "           -2.4924e+00,  5.2590e+00],\n",
            "          [ 7.6062e+00,  1.3325e+01,  9.3323e+00,  ...,  2.6456e+00,\n",
            "            7.0012e+00,  1.3188e+01]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.0695e+01, -3.6861e+00, -1.0405e+01,  ...,  3.7829e+00,\n",
            "           -1.1486e+01, -1.8632e+00],\n",
            "          [-1.2050e+01, -5.8074e-01, -1.0760e+01,  ..., -7.5336e+00,\n",
            "           -1.4430e+01, -1.3228e+00],\n",
            "          [-6.3156e+00,  5.4041e-01, -4.3153e+00,  ..., -1.3430e+01,\n",
            "           -1.0833e+01, -3.1296e+00],\n",
            "          ...,\n",
            "          [ 5.8155e+00, -4.4121e+00, -8.9128e+00,  ...,  1.2615e+01,\n",
            "           -1.3857e+01, -1.8826e+01],\n",
            "          [ 2.0810e+00, -8.3966e+00, -1.9286e+01,  ..., -2.7424e+00,\n",
            "           -2.7763e+01, -1.8794e+00],\n",
            "          [-2.6134e+00, -8.6709e+00, -6.1382e+00,  ..., -7.6707e+00,\n",
            "           -6.9367e+00,  1.2236e+01]],\n",
            "\n",
            "         [[ 8.8564e+00,  5.0170e+00,  3.8958e+00,  ..., -1.3389e+00,\n",
            "            3.9604e+00,  1.6938e-01],\n",
            "          [ 5.3638e+00,  1.2502e+00, -2.9037e-01,  ...,  5.4122e+00,\n",
            "            7.6868e+00,  2.0326e+00],\n",
            "          [-2.4199e-02, -4.0491e+00, -6.7673e+00,  ...,  1.1002e+01,\n",
            "            6.3882e+00, -4.3556e-01],\n",
            "          ...,\n",
            "          [-1.5487e+00, -1.0970e+00,  8.7515e+00,  ..., -5.0977e+00,\n",
            "            4.3233e+00,  5.7510e+00],\n",
            "          [ 4.2830e+00,  7.5739e+00,  1.1889e+01,  ...,  5.9123e+00,\n",
            "            8.4127e+00, -8.0659e-01],\n",
            "          [ 4.3314e+00,  4.0088e+00, -1.0808e+00,  ...,  5.7429e+00,\n",
            "           -1.2663e+00, -1.8471e+00]],\n",
            "\n",
            "         [[ 1.7455e+01,  2.2697e+01,  2.3327e+01,  ...,  2.0504e+01,\n",
            "            1.8858e+01,  1.5519e+01],\n",
            "          [ 1.9592e+01,  2.5375e+01,  2.7529e+01,  ...,  2.0794e+01,\n",
            "            2.0713e+01,  1.6301e+01],\n",
            "          [ 2.0108e+01,  2.8876e+01,  2.8392e+01,  ...,  2.1302e+01,\n",
            "            2.2189e+01,  1.7495e+01],\n",
            "          ...,\n",
            "          [ 1.0671e+01,  1.5979e+01,  1.6374e+01,  ...,  2.0237e+01,\n",
            "            1.7043e+01,  8.1224e+00],\n",
            "          [ 8.4853e+00,  1.2768e+01,  1.4930e+01,  ...,  2.0504e+01,\n",
            "            1.7059e+01,  1.2897e+01],\n",
            "          [ 8.8644e+00,  1.0058e+01,  1.2446e+01,  ...,  1.8181e+01,\n",
            "            1.9278e+01,  1.3777e+01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.0566e+00, -5.6865e+00, -2.8553e+00,  ..., -1.4922e+00,\n",
            "           -1.0163e+00,  2.3956e+00],\n",
            "          [ 1.9358e+00, -5.0493e+00,  1.5487e+00,  ..., -1.2373e+01,\n",
            "           -6.2753e+00, -3.7587e+00],\n",
            "          [ 1.2115e+01,  1.0486e+00,  6.5576e+00,  ..., -8.1869e+00,\n",
            "           -1.2260e+00,  1.2099e-01],\n",
            "          ...,\n",
            "          [-3.2748e+00, -8.3240e+00, -1.4349e+01,  ..., -6.3559e+00,\n",
            "           -1.2260e+01, -4.4846e+00],\n",
            "          [-8.7838e+00, -1.1187e+01, -6.0898e+00,  ..., -1.1502e+01,\n",
            "           -3.2748e+00,  8.0659e-02],\n",
            "          [-8.2998e+00, -5.0009e+00,  3.7265e+00,  ..., -6.2833e+00,\n",
            "           -1.5729e+00, -4.0088e+00]],\n",
            "\n",
            "         [[-7.8804e+00, -9.3807e+00, -1.2446e+01,  ..., -4.2023e+00,\n",
            "           -8.2111e+00, -9.7759e+00],\n",
            "          [-1.2905e+01, -1.0905e+01, -1.8116e+01,  ..., -3.7990e+00,\n",
            "           -1.1671e+01, -9.6549e+00],\n",
            "          [-1.0857e+01, -1.0494e+01, -1.8777e+01,  ..., -5.2590e+00,\n",
            "           -1.3615e+01, -7.5094e+00],\n",
            "          ...,\n",
            "          [-5.5897e+00,  2.8957e+00, -6.5253e+00,  ..., -2.1939e+00,\n",
            "            2.8231e+00, -8.1385e+00],\n",
            "          [-4.5330e+00,  1.6938e+00, -1.0034e+01,  ..., -8.0639e-03,\n",
            "           -2.8150e+00, -1.1139e+01],\n",
            "          [-2.4204e-02,  1.6938e+00, -7.0738e+00,  ...,  2.1133e+00,\n",
            "           -4.7589e+00, -6.0494e+00]],\n",
            "\n",
            "         [[ 1.3301e+01,  1.0026e+01,  1.6043e+01,  ...,  1.3422e+01,\n",
            "            1.7366e+01,  8.0256e+00],\n",
            "          [ 5.2106e+00,  1.2905e+00,  5.6945e+00,  ...,  8.7112e+00,\n",
            "            7.4771e+00,  4.7266e+00],\n",
            "          [ 4.2346e+00,  6.0494e+00,  5.2348e+00,  ...,  8.0982e+00,\n",
            "            7.5578e+00,  3.0408e+00],\n",
            "          ...,\n",
            "          [ 3.1538e+00,  4.4040e+00,  7.7917e+00,  ...,  4.7670e+00,\n",
            "            1.8398e+01,  1.1752e+01],\n",
            "          [ 7.9691e+00,  1.1421e+01,  9.5823e+00,  ...,  1.0994e+01,\n",
            "            1.5261e+01,  2.6779e+00],\n",
            "          [ 8.3966e+00,  9.5904e+00,  2.3311e+00,  ...,  1.3075e+01,\n",
            "            5.8478e+00,  3.9765e+00]]],\n",
            "\n",
            "\n",
            "        [[[-6.6705e+00,  4.4040e+00,  5.4364e+00,  ..., -1.1752e+01,\n",
            "           -3.4683e-01,  4.8557e+00],\n",
            "          [ 3.1618e+00,  1.0478e+01,  5.2428e+00,  ...,  5.9123e+00,\n",
            "            1.0413e+01, -6.7189e+00],\n",
            "          [ 5.8801e+00,  2.5972e+00,  8.0659e-01,  ...,  6.2350e+00,\n",
            "           -1.1462e+01, -9.9937e+00],\n",
            "          ...,\n",
            "          [-3.5490e+00, -6.1946e+00, -5.3800e+00,  ..., -2.0302e+01,\n",
            "           -7.5739e+00, -2.4278e+00],\n",
            "          [-1.3954e+00, -6.4124e+00, -3.9120e+00,  ..., -1.6196e+01,\n",
            "           -2.8392e+00, -4.4282e+00],\n",
            "          [-1.8068e+00, -7.3480e+00, -4.6137e+00,  ..., -3.8394e+00,\n",
            "            1.7584e+00, -8.0175e+00]],\n",
            "\n",
            "         [[ 8.5821e+00,  3.3070e+00, -5.0896e+00,  ...,  1.4519e+01,\n",
            "            4.8395e+00,  7.8240e-01],\n",
            "          [ 1.8310e+00, -6.2995e+00, -8.2837e+00,  ..., -9.2435e+00,\n",
            "           -7.2351e+00, -2.1294e+00],\n",
            "          [ 1.8713e+00, -2.3310e+00, -5.5897e+00,  ..., -6.3075e+00,\n",
            "            1.9358e-01,  1.0566e+00],\n",
            "          ...,\n",
            "          [ 1.3793e+00, -2.0003e+00, -8.3886e-01,  ...,  7.5416e+00,\n",
            "            6.6624e+00,  4.8073e+00],\n",
            "          [-3.4199e+00, -3.7668e+00, -8.3886e-01,  ...,  1.0728e+01,\n",
            "            4.8234e+00,  5.2025e+00],\n",
            "          [ 3.8555e+00,  2.6698e+00,  6.1220e+00,  ...,  3.4603e+00,\n",
            "            4.9363e+00,  3.9684e+00]],\n",
            "\n",
            "         [[ 1.2325e+01,  1.7592e+01,  1.5559e+01,  ...,  2.7384e+01,\n",
            "            2.7642e+01,  2.0528e+01],\n",
            "          [ 1.4946e+01,  2.3044e+01,  1.8011e+01,  ...,  3.3425e+01,\n",
            "            3.2820e+01,  2.4004e+01],\n",
            "          [ 1.3397e+01,  2.2714e+01,  1.6350e+01,  ...,  3.0336e+01,\n",
            "            2.9892e+01,  2.3899e+01],\n",
            "          ...,\n",
            "          [ 1.0776e+01,  1.2147e+01,  1.2478e+01,  ...,  1.4519e+01,\n",
            "            1.4753e+01,  1.2970e+01],\n",
            "          [ 1.4083e+01,  1.7705e+01,  1.8334e+01,  ...,  1.6842e+01,\n",
            "            1.7124e+01,  1.6019e+01],\n",
            "          [ 1.2244e+01,  1.3994e+01,  1.4277e+01,  ...,  1.2704e+01,\n",
            "            1.2922e+01,  1.0615e+01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.4524e+00,  7.9207e+00,  9.5984e+00,  ..., -3.8878e+00,\n",
            "            3.0005e+00,  4.9202e-01],\n",
            "          [ 6.2027e+00,  1.0808e+00,  1.4519e-01,  ...,  7.5658e+00,\n",
            "           -4.4363e-01, -4.9444e+00],\n",
            "          [ 4.3556e+00, -3.0973e+00, -4.7105e+00,  ..., -7.0657e+00,\n",
            "           -6.0172e+00, -6.1785e+00],\n",
            "          ...,\n",
            "          [-8.0498e+00, -9.5420e+00, -6.4689e+00,  ..., -1.6156e+01,\n",
            "           -1.1066e+01, -3.6297e+00],\n",
            "          [-8.8322e+00, -1.4269e+01, -1.4954e+01,  ..., -1.3623e+01,\n",
            "           -1.0131e+01, -6.9286e+00],\n",
            "          [-1.2188e+01, -1.2470e+01, -1.1704e+01,  ..., -4.8799e+00,\n",
            "           -9.5904e+00, -3.2264e+00]],\n",
            "\n",
            "         [[-9.4049e+00, -2.3633e+00,  8.7919e-01,  ..., -1.2155e+01,\n",
            "           -1.0324e+01,  4.1943e-01],\n",
            "          [-1.0502e+01,  4.1056e+00,  6.2591e+00,  ..., -1.2914e+01,\n",
            "           -1.0695e+01,  1.9116e+00],\n",
            "          [-6.9609e+00,  1.0316e+01,  8.7999e+00,  ..., -8.7918e+00,\n",
            "           -9.0016e+00,  2.8634e+00],\n",
            "          ...,\n",
            "          [ 2.4278e+00, -3.7668e+00, -3.5974e+00,  ..., -3.4925e+00,\n",
            "           -1.0139e+01, -3.4603e+00],\n",
            "          [-5.9688e-01, -6.6947e+00, -8.4853e+00,  ..., -9.4210e+00,\n",
            "           -9.0016e+00, -2.2101e+00],\n",
            "          [ 4.7347e+00,  2.4198e-01, -1.4680e+00,  ..., -3.0489e+00,\n",
            "            3.3877e-01,  5.9688e-01]],\n",
            "\n",
            "         [[ 2.9844e+00, -3.1134e+00, -1.6535e+00,  ...,  6.7592e+00,\n",
            "           -9.1145e-01,  4.0329e-02],\n",
            "          [-4.0329e-01, -8.8725e-01,  8.0417e+00,  ..., -3.5248e+00,\n",
            "           -1.2986e+00,  6.9206e+00],\n",
            "          [ 4.3556e+00,  3.2828e+00,  1.0647e+01,  ...,  1.3147e+01,\n",
            "            1.5051e+01,  6.2430e+00],\n",
            "          ...,\n",
            "          [ 5.9446e+00,  9.2355e+00,  6.7028e+00,  ...,  1.8253e+01,\n",
            "            7.3561e+00,  3.4441e+00],\n",
            "          [ 7.2513e+00,  1.3825e+01,  1.1575e+01,  ...,  9.9533e+00,\n",
            "            6.0414e+00,  7.1787e+00],\n",
            "          [ 1.1946e+01,  1.4140e+01,  1.3341e+01,  ...,  3.6055e+00,\n",
            "            6.4850e+00,  8.4531e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 3.4280e+00, -2.0068e+01, -9.7678e+00,  ...,  4.6460e+00,\n",
            "            3.4764e+00, -6.5334e+00],\n",
            "          [-1.1300e+01, -2.2738e+01,  2.0391e+01,  ...,  8.1062e+00,\n",
            "           -8.5902e+00, -1.2752e+01],\n",
            "          [-1.6874e+01, -1.6374e+00,  2.3899e+01,  ..., -5.9446e+00,\n",
            "           -1.9834e+01, -2.5488e+00],\n",
            "          ...,\n",
            "          [-6.7431e+00, -1.4277e+00, -1.8261e+01,  ..., -1.2736e+01,\n",
            "           -4.7750e+00,  9.2919e+00],\n",
            "          [-5.9042e+00,  3.6861e+00,  1.6858e+00,  ..., -1.0954e+01,\n",
            "            1.3817e+01, -2.5730e+00],\n",
            "          [ 9.5258e+00,  2.4843e+00,  5.1622e-01,  ...,  6.8480e+00,\n",
            "            8.9532e+00, -3.6942e+00]],\n",
            "\n",
            "         [[ 6.0333e+00,  1.2639e+01,  5.6459e-02,  ...,  1.7019e+00,\n",
            "            3.7829e+00,  9.2839e+00],\n",
            "          [ 1.3567e+01,  5.3235e+00, -1.0115e+01,  ..., -1.3228e+00,\n",
            "            3.5571e+00,  4.7508e+00],\n",
            "          [ 1.1349e+01, -1.7019e+00, -9.7436e+00,  ...,  4.4201e+00,\n",
            "            1.2446e+01,  5.3477e+00],\n",
            "          ...,\n",
            "          [ 1.4744e+01,  1.4930e+01,  9.5097e+00,  ...,  5.2993e+00,\n",
            "           -3.1699e+00, -4.8154e+00],\n",
            "          [ 4.2749e+00, -1.6535e+00, -2.0810e+00,  ...,  2.4520e+00,\n",
            "           -3.1215e+00, -2.2585e-01],\n",
            "          [ 1.0486e-01, -3.3716e+00, -2.2262e+00,  ..., -4.1701e+00,\n",
            "           -4.3314e+00, -4.4282e+00]],\n",
            "\n",
            "         [[ 1.0913e+01,  1.1978e+01,  1.2018e+01,  ...,  1.4849e+01,\n",
            "            1.3954e+01,  1.1284e+01],\n",
            "          [ 1.3631e+01,  1.4478e+01,  1.4769e+01,  ...,  1.7092e+01,\n",
            "            1.3156e+01,  1.1575e+01],\n",
            "          [ 1.2373e+01,  1.6132e+01,  1.5729e+01,  ...,  1.4519e+01,\n",
            "            1.0824e+01,  1.0978e+01],\n",
            "          ...,\n",
            "          [ 1.1373e+01,  1.5793e+01,  1.5737e+01,  ...,  1.6584e+01,\n",
            "            1.5099e+01,  1.2260e+01],\n",
            "          [ 1.3010e+01,  1.8511e+01,  1.7430e+01,  ...,  1.8221e+01,\n",
            "            1.7931e+01,  1.2462e+01],\n",
            "          [ 1.0889e+01,  1.5833e+01,  1.4261e+01,  ...,  1.6269e+01,\n",
            "            1.6196e+01,  1.0066e+01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.1679e+01, -9.3000e+00,  9.0258e+00,  ..., -6.2915e-01,\n",
            "           -6.0656e+00, -8.6467e+00],\n",
            "          [-1.5212e+01, -2.7585e+00,  2.3391e+00,  ..., -1.0728e+01,\n",
            "           -1.3131e+01, -8.4853e+00],\n",
            "          [-8.2514e+00,  4.6298e+00, -8.8402e+00,  ..., -1.7600e+01,\n",
            "           -1.0139e+01, -4.7670e+00],\n",
            "          ...,\n",
            "          [-8.0095e+00, -1.0284e+01, -5.0412e+00,  ..., -4.3879e+00,\n",
            "            5.0654e+00, -4.6056e+00],\n",
            "          [ 2.0326e+00, -3.0650e-01,  1.6938e+00,  ...,  3.9765e+00,\n",
            "            2.8231e-01, -3.3554e+00],\n",
            "          [ 3.0973e+00,  1.3147e+00,  8.8726e-02,  ...,  9.2758e-01,\n",
            "           -3.9846e+00, -4.8960e+00]],\n",
            "\n",
            "         [[-2.7182e+00, -7.9611e+00, -1.2115e+01,  ...,  3.4361e+00,\n",
            "            2.5811e+00, -1.8874e+00],\n",
            "          [-1.5406e+00, -1.2180e+01, -1.0187e+01,  ...,  8.9370e+00,\n",
            "            3.7506e+00, -5.5090e+00],\n",
            "          [-2.3472e+00, -1.4607e+01, -2.9037e-01,  ...,  3.7023e+00,\n",
            "           -4.3072e+00, -5.8962e+00],\n",
            "          ...,\n",
            "          [-7.3884e+00, -1.6212e+00, -8.7999e+00,  ..., -8.1627e+00,\n",
            "           -1.4607e+01, -1.9358e-01],\n",
            "          [-6.2188e+00,  2.4198e-01, -9.3242e+00,  ..., -9.5823e+00,\n",
            "           -1.3946e+01,  1.1857e+00],\n",
            "          [-4.0168e+00,  2.3875e+00, -4.8396e+00,  ..., -7.2513e+00,\n",
            "           -6.8560e+00,  3.9926e+00]],\n",
            "\n",
            "         [[ 8.6225e+00,  1.5019e+01, -7.4206e-01,  ...,  8.5499e-01,\n",
            "            1.9681e+00,  4.8234e+00],\n",
            "          [ 1.9979e+01,  5.6461e+00, -8.9048e+00,  ...,  3.0812e+00,\n",
            "            1.3220e+01,  4.4282e+00],\n",
            "          [ 1.5124e+01, -1.9923e+00,  4.4040e+00,  ...,  1.0816e+01,\n",
            "            9.1145e+00,  7.3400e-01],\n",
            "          ...,\n",
            "          [ 8.0982e+00,  4.9525e+00,  8.1304e+00,  ...,  7.6062e+00,\n",
            "           -9.2758e-01,  5.0170e+00],\n",
            "          [ 3.1457e+00, -1.6938e+00, -4.1136e+00,  ...,  4.5411e+00,\n",
            "           -1.9439e+00,  9.8727e+00],\n",
            "          [ 2.0810e+00,  7.0980e+00,  7.0819e+00,  ...,  3.0812e+00,\n",
            "            8.5337e+00,  7.0899e+00]]]], device='cuda:0',\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_ref = torch.nn.Conv2d(in_channels = 16, out_channels=16, kernel_size = 3, padding=1)\n",
        "conv_ref.weight = model.features[3].weight_q\n",
        "conv_ref.bias = model.features[3].bias\n",
        "output_ref = conv_ref(act)\n",
        "print(output_ref)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQ04ZjFXoraI",
        "outputId": "90d26147-ebda-4ec2-fce7-54547079cbd2"
      },
      "id": "OQ04ZjFXoraI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-1.5978e+00, -6.5127e+00, -5.5128e+00,  ..., -5.1591e+00,\n",
            "           -8.7421e+00,  1.3061e-01],\n",
            "          [-7.9954e-01, -7.7770e+00, -4.2379e+00,  ..., -4.6463e+00,\n",
            "           -4.2488e+00,  9.7652e-01],\n",
            "          [-5.0718e+00, -4.9145e+00, -6.3376e+00,  ..., -4.8920e+00,\n",
            "           -4.0186e+00, -1.4195e+00],\n",
            "          ...,\n",
            "          [-1.2219e+01, -1.1925e+01, -1.0580e+01,  ..., -1.8536e+01,\n",
            "            7.7679e+00, -1.4140e+01],\n",
            "          [-1.6131e+01, -1.0034e+01, -1.3484e+01,  ..., -1.8274e+00,\n",
            "           -4.9837e+00, -7.2900e+00],\n",
            "          [-7.3832e+00, -9.5918e+00, -3.6329e+00,  ...,  7.6079e+00,\n",
            "           -1.7443e-01,  7.1397e+00]],\n",
            "\n",
            "         [[ 7.8068e+00,  1.6535e+01,  1.0463e+01,  ...,  1.1326e+01,\n",
            "            9.2186e+00,  3.4775e+00],\n",
            "          [ 1.5739e+01,  1.2125e+01,  7.5423e+00,  ...,  5.5177e+00,\n",
            "            6.8292e-01, -4.1647e+00],\n",
            "          [ 9.5548e+00,  7.2418e+00,  6.0641e+00,  ...,  6.8435e+00,\n",
            "            1.1637e+00, -6.5314e-01],\n",
            "          ...,\n",
            "          [ 1.1280e+01,  5.7424e+00,  5.8696e+00,  ...,  9.1491e+00,\n",
            "            2.4242e+00,  1.1633e+00],\n",
            "          [ 5.6103e+00,  6.4178e+00,  1.0718e+01,  ...,  1.0430e+01,\n",
            "            4.4242e+00, -9.5425e-01],\n",
            "          [ 3.4204e+00,  5.2349e+00,  4.2090e+00,  ..., -2.3465e+00,\n",
            "           -5.1125e+00, -3.0515e+00]],\n",
            "\n",
            "         [[ 1.2091e+01,  2.0422e+01,  1.9287e+01,  ...,  2.0147e+01,\n",
            "            2.0377e+01,  1.7282e+01],\n",
            "          [ 1.2631e+01,  1.8010e+01,  1.5468e+01,  ...,  1.3493e+01,\n",
            "            1.6666e+01,  1.5477e+01],\n",
            "          [ 1.0508e+01,  1.3140e+01,  8.7123e+00,  ...,  9.8301e+00,\n",
            "            1.3664e+01,  1.3855e+01],\n",
            "          ...,\n",
            "          [ 2.0046e+01,  2.2709e+01,  2.6273e+01,  ...,  2.5011e+01,\n",
            "            3.0057e+01,  2.2089e+01],\n",
            "          [ 2.1343e+01,  2.8581e+01,  3.2931e+01,  ...,  3.2863e+01,\n",
            "            3.6738e+01,  2.8215e+01],\n",
            "          [ 2.0628e+01,  2.7869e+01,  3.0592e+01,  ...,  3.0897e+01,\n",
            "            3.4705e+01,  2.3409e+01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.4721e+01, -1.4709e+01, -1.3003e+01,  ..., -7.5585e+00,\n",
            "            1.5354e+00,  4.6465e+00],\n",
            "          [-1.2754e+01, -9.4926e+00, -1.3309e+01,  ..., -6.9194e+00,\n",
            "            4.4701e+00,  7.6075e-01],\n",
            "          [-1.0664e+01, -9.9545e+00, -1.2136e+01,  ..., -5.6233e+00,\n",
            "            4.6432e+00, -1.0462e+00],\n",
            "          ...,\n",
            "          [-1.3518e+01, -1.5582e+01, -1.0749e+01,  ..., -7.1461e+00,\n",
            "           -1.1693e+01, -1.9643e+00],\n",
            "          [-1.3367e+01, -1.7461e+01, -1.3805e+01,  ..., -4.4659e+00,\n",
            "           -5.4950e+00,  1.2971e+00],\n",
            "          [-1.0785e+01, -5.0260e+00, -3.3810e+00,  ...,  1.8637e+00,\n",
            "            2.6483e-01, -1.8053e+00]],\n",
            "\n",
            "         [[-3.5812e+00, -8.5031e-01, -4.4317e+00,  ..., -6.9196e+00,\n",
            "           -1.1490e+01, -6.4725e+00],\n",
            "          [ 4.1050e-01, -2.2512e-01, -5.6970e+00,  ..., -8.6571e+00,\n",
            "           -1.4462e+01, -6.7015e+00],\n",
            "          [ 3.9926e+00,  2.3452e+00, -6.0616e+00,  ..., -6.8915e+00,\n",
            "           -1.2380e+01, -6.1537e+00],\n",
            "          ...,\n",
            "          [-4.1952e+00, -2.5288e+01, -1.2858e+01,  ..., -2.3946e+01,\n",
            "           -2.5946e+01,  9.5604e+00],\n",
            "          [-5.8034e+00, -2.4949e+01, -1.2144e+01,  ..., -1.7519e+01,\n",
            "           -1.8539e+01,  3.4441e+00],\n",
            "          [-1.2557e+00, -1.1411e+01, -5.8208e+00,  ..., -5.2318e+00,\n",
            "           -7.5871e+00,  2.9514e+00]],\n",
            "\n",
            "         [[ 1.4001e+01,  6.1509e+00,  6.4675e+00,  ...,  7.8740e+00,\n",
            "            4.1471e+00, -2.4752e+00],\n",
            "          [ 1.4267e+01, -8.0092e-01,  4.0613e+00,  ..., -2.3609e+00,\n",
            "           -6.3292e+00, -3.5536e+00],\n",
            "          [ 1.5380e+01,  4.6150e-01,  9.7186e+00,  ...,  4.0650e+00,\n",
            "           -1.4898e+00,  1.3312e+00],\n",
            "          ...,\n",
            "          [ 1.1867e+01,  1.0557e+01,  8.3162e+00,  ...,  7.3226e-01,\n",
            "            4.2658e+00,  1.5592e+01],\n",
            "          [ 1.3091e+01,  1.5391e+01,  7.8265e+00,  ..., -4.4723e+00,\n",
            "            7.2985e+00,  6.0165e+00],\n",
            "          [ 1.2710e+01,  1.2798e+01,  3.5308e+00,  ..., -2.4169e-01,\n",
            "            3.7095e+00,  2.0580e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 1.8844e+01, -4.4563e+00,  2.2944e+00,  ...,  2.2944e+00,\n",
            "            1.3981e+01, -1.1782e+01],\n",
            "          [ 3.5282e+00, -1.7603e+01, -5.9789e+00,  ..., -5.9789e+00,\n",
            "           -4.6253e+00, -2.5263e+01],\n",
            "          [-7.2147e+00, -1.2713e+01, -6.1139e+00,  ..., -6.1139e+00,\n",
            "           -6.3451e+00, -1.9626e+01],\n",
            "          ...,\n",
            "          [-2.7504e+01, -1.6537e+01,  1.0520e+01,  ...,  4.1976e+00,\n",
            "           -1.2388e+01, -1.4886e+01],\n",
            "          [-2.6311e+01,  5.9987e+00,  1.0095e+01,  ..., -1.3004e+01,\n",
            "           -8.6279e+00, -2.2663e+01],\n",
            "          [-7.6080e+00,  1.7157e+01,  1.6691e+00,  ..., -1.0266e+01,\n",
            "           -1.1277e+01, -1.8435e+01]],\n",
            "\n",
            "         [[ 6.6204e+00,  3.0550e+00,  4.7233e+00,  ...,  4.7233e+00,\n",
            "            6.7667e+00,  2.2309e+01],\n",
            "          [ 8.9235e+00,  3.8377e-01,  2.2578e+00,  ...,  2.2578e+00,\n",
            "            9.5854e+00,  1.8103e+01],\n",
            "          [ 6.4810e+00, -1.4008e+00,  1.1870e+00,  ...,  1.1870e+00,\n",
            "            6.9748e+00,  1.3407e+01],\n",
            "          ...,\n",
            "          [ 1.7610e+01,  2.4377e+00, -7.8967e+00,  ..., -5.3037e+00,\n",
            "            3.5141e+00,  1.3034e+01],\n",
            "          [ 7.7681e+00, -9.0282e+00, -7.7563e+00,  ...,  2.2841e+00,\n",
            "            1.0040e+01,  2.0553e+01],\n",
            "          [-3.1277e+00, -6.8405e+00, -1.3754e+00,  ...,  8.1763e+00,\n",
            "            1.8130e+01,  8.2663e+00]],\n",
            "\n",
            "         [[ 1.8234e+01,  2.2552e+01,  1.7661e+01,  ...,  1.7661e+01,\n",
            "            1.8674e+01,  1.9771e+01],\n",
            "          [ 2.0016e+01,  1.4363e+01,  1.2371e+01,  ...,  1.2371e+01,\n",
            "            1.2244e+01,  1.9232e+01],\n",
            "          [ 1.8461e+01,  1.0519e+01,  1.3116e+01,  ...,  1.3116e+01,\n",
            "            1.2952e+01,  2.1027e+01],\n",
            "          ...,\n",
            "          [ 1.5656e+01,  2.4305e+01,  2.7774e+01,  ...,  1.5028e+01,\n",
            "            1.2896e+01,  1.6191e+01],\n",
            "          [ 1.6928e+01,  2.4879e+01,  2.8902e+01,  ...,  1.6730e+01,\n",
            "            1.6089e+01,  1.8948e+01],\n",
            "          [ 1.6975e+01,  2.3552e+01,  2.5899e+01,  ...,  1.1295e+01,\n",
            "            9.8286e+00,  1.2108e+01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.2538e+00, -1.1478e+01, -1.0028e+01,  ..., -1.0028e+01,\n",
            "           -1.6526e+01, -2.7633e+01],\n",
            "          [-1.7845e+01, -2.3992e+01, -2.0410e+01,  ..., -2.0410e+01,\n",
            "           -2.8863e+01, -2.4798e+01],\n",
            "          [-1.5920e+01, -1.8144e+01, -1.3647e+01,  ..., -1.3647e+01,\n",
            "           -2.0595e+01, -1.7640e+01],\n",
            "          ...,\n",
            "          [-9.5933e+00,  9.2619e+00,  1.2600e+01,  ..., -1.6284e+01,\n",
            "           -2.0877e+01, -1.5027e+01],\n",
            "          [ 4.0057e+00,  1.4482e+01,  1.0506e+01,  ..., -2.1348e+01,\n",
            "           -2.7953e+01, -1.7011e+01],\n",
            "          [ 1.0810e+01,  8.1747e+00,  2.7911e+00,  ..., -1.8361e+01,\n",
            "           -1.8947e+01, -5.1040e-01]],\n",
            "\n",
            "         [[-5.5807e+00, -1.7254e+00, -7.3645e+00,  ..., -7.3645e+00,\n",
            "           -1.2636e+00, -4.8098e+00],\n",
            "          [-1.0673e+00, -8.4877e+00, -1.3381e+01,  ..., -1.3381e+01,\n",
            "           -2.5586e+00, -8.5466e+00],\n",
            "          [ 7.5309e+00, -6.9445e+00, -1.0237e+01,  ..., -1.0237e+01,\n",
            "            1.3129e+00, -9.7183e+00],\n",
            "          ...,\n",
            "          [-9.0131e+00, -2.6841e+01, -2.1186e+01,  ...,  1.4331e+00,\n",
            "            6.3917e+00,  2.0847e+00],\n",
            "          [-1.2221e+01, -2.4743e+01, -1.6851e+01,  ..., -6.3958e-01,\n",
            "           -1.5404e+00, -8.7932e-01],\n",
            "          [-9.9166e+00, -1.4872e+01, -8.2418e+00,  ...,  7.0421e+00,\n",
            "            7.2890e+00,  2.6143e+00]],\n",
            "\n",
            "         [[ 5.6015e+00,  2.1433e+01,  1.4759e+01,  ...,  1.4759e+01,\n",
            "            8.5642e+00,  2.5315e+01],\n",
            "          [ 3.0380e+01,  3.5409e+01,  2.9235e+01,  ...,  2.9235e+01,\n",
            "            3.2737e+01,  2.8367e+01],\n",
            "          [ 2.1764e+01,  2.1938e+01,  1.8352e+01,  ...,  1.8352e+01,\n",
            "            2.2640e+01,  1.9722e+01],\n",
            "          ...,\n",
            "          [ 1.9227e+01, -1.7972e+00, -2.4339e+00,  ...,  1.4355e+01,\n",
            "            2.4984e+01,  1.6039e+01],\n",
            "          [ 9.3694e+00, -7.0194e+00,  3.1447e+00,  ...,  2.0126e+01,\n",
            "            2.3827e+01,  1.8861e+01],\n",
            "          [ 6.1048e+00,  7.5945e+00,  1.7394e+01,  ...,  1.4295e+01,\n",
            "            2.3154e+01,  8.1448e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4877e+01, -4.7664e+00, -1.0104e+01,  ...,  2.2944e+00,\n",
            "            1.3981e+01, -1.1782e+01],\n",
            "          [ 5.0504e+00, -1.5220e+01, -1.0779e+01,  ..., -5.9789e+00,\n",
            "           -4.6253e+00, -2.5263e+01],\n",
            "          [-2.8499e+00, -2.1120e+01, -1.2472e+01,  ..., -6.1139e+00,\n",
            "           -6.3451e+00, -1.9626e+01],\n",
            "          ...,\n",
            "          [-1.3475e+01,  7.1758e+00, -6.1704e+00,  ..., -3.6775e+01,\n",
            "           -2.3768e+01,  1.6533e+01],\n",
            "          [-9.3417e+00,  7.1406e+00, -5.2640e+00,  ..., -2.7130e+01,\n",
            "            3.7412e+00,  2.4065e+01],\n",
            "          [ 8.1496e+00,  6.2647e+00,  3.1207e+00,  ...,  9.8673e+00,\n",
            "            2.7509e+01,  1.4371e+01]],\n",
            "\n",
            "         [[ 4.6484e+00,  4.5624e+00,  8.3389e+00,  ...,  4.7233e+00,\n",
            "            6.7667e+00,  2.2309e+01],\n",
            "          [ 7.1664e+00,  6.8478e+00,  1.0086e+01,  ...,  2.2578e+00,\n",
            "            9.5854e+00,  1.8103e+01],\n",
            "          [ 9.6955e+00,  9.5476e+00,  6.9040e+00,  ...,  1.1870e+00,\n",
            "            6.9748e+00,  1.3407e+01],\n",
            "          ...,\n",
            "          [ 8.8513e+00,  1.6503e-02, -4.1753e-01,  ...,  3.2215e+01,\n",
            "            6.8659e+00, -1.4669e+01],\n",
            "          [ 4.6701e+00, -5.8937e-01, -2.1386e+00,  ...,  1.2739e+01,\n",
            "           -1.0226e+01, -2.1522e+01],\n",
            "          [-1.4286e+00, -6.3798e+00, -2.8368e+00,  ..., -8.9376e+00,\n",
            "           -1.6768e+01, -1.1197e+01]],\n",
            "\n",
            "         [[ 1.4959e+01,  1.9530e+01,  1.7051e+01,  ...,  1.7661e+01,\n",
            "            1.8674e+01,  1.9771e+01],\n",
            "          [ 1.8771e+01,  1.8909e+01,  2.2753e+01,  ...,  1.2371e+01,\n",
            "            1.2244e+01,  1.9232e+01],\n",
            "          [ 1.7023e+01,  1.4559e+01,  2.1665e+01,  ...,  1.3116e+01,\n",
            "            1.2952e+01,  2.1027e+01],\n",
            "          ...,\n",
            "          [ 1.7695e+01,  2.0850e+01,  1.9691e+01,  ...,  2.3631e+01,\n",
            "            3.0223e+01,  2.6781e+01],\n",
            "          [ 1.9563e+01,  2.6062e+01,  2.5843e+01,  ...,  3.3270e+01,\n",
            "            4.1114e+01,  3.6110e+01],\n",
            "          [ 2.0768e+01,  2.7949e+01,  2.6869e+01,  ...,  3.5867e+01,\n",
            "            4.3642e+01,  3.2222e+01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.4063e+00, -1.3367e+01, -5.2624e+00,  ..., -1.0028e+01,\n",
            "           -1.6526e+01, -2.7633e+01],\n",
            "          [-1.9565e+01, -2.8183e+01, -1.5984e+01,  ..., -2.0410e+01,\n",
            "           -2.8863e+01, -2.4798e+01],\n",
            "          [-2.5845e+01, -2.5291e+01, -1.3123e+01,  ..., -1.3647e+01,\n",
            "           -2.0595e+01, -1.7640e+01],\n",
            "          ...,\n",
            "          [ 2.1492e+00,  1.5516e+00,  6.9665e+00,  ..., -1.5111e+01,\n",
            "            1.7940e+01,  9.9932e+00],\n",
            "          [ 1.0510e+01,  8.0679e+00,  1.1639e+01,  ...,  8.4194e+00,\n",
            "            3.5738e+01,  1.1235e+01],\n",
            "          [ 9.1159e+00,  3.6447e+00,  3.8156e-01,  ...,  1.5722e+01,\n",
            "            1.6351e+01, -9.6394e+00]],\n",
            "\n",
            "         [[-6.1085e+00, -9.0388e+00, -1.8350e+01,  ..., -7.3645e+00,\n",
            "           -1.2636e+00, -4.8098e+00],\n",
            "          [-2.9792e+00, -1.4510e+01, -2.5302e+01,  ..., -1.3381e+01,\n",
            "           -2.5586e+00, -8.5466e+00],\n",
            "          [ 3.4605e+00, -1.3493e+01, -2.2995e+01,  ..., -1.0237e+01,\n",
            "            1.3129e+00, -9.7183e+00],\n",
            "          ...,\n",
            "          [-1.4210e+01, -9.3652e+00, -8.8796e+00,  ..., -1.5771e+01,\n",
            "           -3.9253e+01, -1.8549e+01],\n",
            "          [-1.6172e+01, -7.8389e+00, -1.3002e+01,  ..., -2.5925e+01,\n",
            "           -3.9253e+01, -7.5567e+00],\n",
            "          [-1.1702e+01, -2.3458e+00, -7.7120e+00,  ..., -2.0346e+01,\n",
            "           -1.9390e+01,  6.9675e+00]],\n",
            "\n",
            "         [[ 1.6841e+00,  1.9079e+01,  1.0380e+01,  ...,  1.4759e+01,\n",
            "            8.5642e+00,  2.5315e+01],\n",
            "          [ 2.1693e+01,  3.2339e+01,  1.4908e+01,  ...,  2.9235e+01,\n",
            "            3.2737e+01,  2.8367e+01],\n",
            "          [ 2.0151e+01,  2.8191e+01,  6.2122e+00,  ...,  1.8352e+01,\n",
            "            2.2640e+01,  1.9722e+01],\n",
            "          ...,\n",
            "          [ 8.7707e+00, -5.4465e+00, -1.4861e+00,  ...,  1.7832e+01,\n",
            "           -1.2106e+00, -2.1553e+00],\n",
            "          [ 6.1868e+00,  1.7235e+00,  1.2469e+00,  ..., -3.4299e-01,\n",
            "           -9.1410e+00,  3.5169e+00],\n",
            "          [ 7.7434e+00,  1.2395e+01,  1.0037e+01,  ...,  3.0722e+00,\n",
            "            4.8619e+00,  1.7656e+01]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-2.1147e+01, -1.1868e+00, -1.0011e+01,  ...,  5.4389e+00,\n",
            "           -1.5779e+01,  2.1909e+00],\n",
            "          [-2.1851e+01,  3.4189e+00, -1.1556e+01,  ..., -9.1878e+00,\n",
            "           -1.6557e+01,  4.1820e+00],\n",
            "          [-8.4028e+00,  5.4356e+00, -4.5989e+00,  ..., -1.3344e+01,\n",
            "           -1.3052e+01, -1.5903e-02],\n",
            "          ...,\n",
            "          [ 6.9923e+00, -3.6553e+00, -1.1027e+01,  ...,  1.5354e+01,\n",
            "           -1.6005e+01, -2.1593e+01],\n",
            "          [ 2.6341e+00, -9.7170e+00, -2.2460e+01,  ..., -1.6479e+00,\n",
            "           -3.0356e+01,  4.3885e-01],\n",
            "          [-2.9088e+00, -1.0508e+01, -6.6382e+00,  ..., -8.2612e+00,\n",
            "           -7.5375e+00,  1.5461e+01]],\n",
            "\n",
            "         [[ 2.1715e+01,  1.1035e+01,  4.3231e+00,  ..., -1.1383e+00,\n",
            "            3.8063e+00, -4.3668e+00],\n",
            "          [ 1.0066e+01, -3.0436e+00, -7.2028e+00,  ...,  1.1722e+00,\n",
            "            1.4857e+00, -5.3031e+00],\n",
            "          [ 2.7926e+00, -5.4341e+00, -7.3739e+00,  ...,  1.1187e+01,\n",
            "            4.2680e+00, -4.7144e+00],\n",
            "          ...,\n",
            "          [-2.1631e+00, -2.6027e-01,  1.2017e+01,  ..., -5.4838e+00,\n",
            "            5.7664e+00,  6.2442e+00],\n",
            "          [ 4.6383e+00,  9.9741e+00,  1.3960e+01,  ...,  7.3001e+00,\n",
            "            9.5491e+00, -2.0049e+00],\n",
            "          [ 4.6664e+00,  4.1012e+00, -1.9500e+00,  ...,  5.5613e+00,\n",
            "           -2.5672e+00, -4.2305e+00]],\n",
            "\n",
            "         [[ 2.6385e+01,  3.1682e+01,  3.1331e+01,  ...,  2.5917e+01,\n",
            "            2.2997e+01,  1.9963e+01],\n",
            "          [ 2.4599e+01,  2.5757e+01,  2.6869e+01,  ...,  2.0265e+01,\n",
            "            2.2087e+01,  1.8657e+01],\n",
            "          [ 2.4076e+01,  2.9589e+01,  2.9202e+01,  ...,  2.1338e+01,\n",
            "            2.4361e+01,  2.0615e+01],\n",
            "          ...,\n",
            "          [ 1.0877e+01,  1.6347e+01,  1.6467e+01,  ...,  2.0941e+01,\n",
            "            1.7873e+01,  9.0428e+00],\n",
            "          [ 8.3118e+00,  1.2826e+01,  1.5130e+01,  ...,  2.1877e+01,\n",
            "            1.9412e+01,  1.4476e+01],\n",
            "          [ 8.8065e+00,  1.0268e+01,  1.2893e+01,  ...,  1.9770e+01,\n",
            "            2.1913e+01,  1.5275e+01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.3774e+00, -6.7746e+00, -7.7503e-01,  ..., -3.4336e-01,\n",
            "            5.3887e+00,  7.9869e+00],\n",
            "          [-1.9331e+00, -9.3740e+00, -1.6807e+00,  ..., -1.4615e+01,\n",
            "           -1.7828e+00, -2.0905e+00],\n",
            "          [ 1.2932e+01,  1.4568e+00,  7.4068e+00,  ..., -8.4848e+00,\n",
            "            5.0733e+00,  1.7799e+00],\n",
            "          ...,\n",
            "          [-3.5600e+00, -1.0311e+01, -1.7587e+01,  ..., -7.4703e+00,\n",
            "           -1.2107e+01, -3.5131e+00],\n",
            "          [-9.2461e+00, -1.2415e+01, -6.7817e+00,  ..., -1.1728e+01,\n",
            "           -4.7922e-01,  1.6247e+00],\n",
            "          [-8.2939e+00, -5.1423e+00,  4.4586e+00,  ..., -6.8301e+00,\n",
            "            1.6984e-01, -4.8038e+00]],\n",
            "\n",
            "         [[-1.1625e+01, -1.8176e+01, -1.7495e+01,  ..., -5.3746e+00,\n",
            "           -1.6558e+01, -1.3373e+01],\n",
            "          [-1.8760e+01, -1.7549e+01, -2.0800e+01,  ..., -3.2458e+00,\n",
            "           -2.1203e+01, -1.0753e+01],\n",
            "          [-1.5133e+01, -1.2401e+01, -2.0138e+01,  ..., -4.9383e+00,\n",
            "           -2.2762e+01, -9.7268e+00],\n",
            "          ...,\n",
            "          [-5.3899e+00,  4.6276e+00, -8.1046e+00,  ..., -1.8630e+00,\n",
            "            1.2621e+00, -9.9248e+00],\n",
            "          [-4.9726e+00,  2.8572e+00, -1.1624e+01,  ...,  9.4426e-01,\n",
            "           -4.9416e+00, -1.2219e+01],\n",
            "          [ 1.4540e-01,  3.4154e+00, -8.5227e+00,  ...,  3.1601e+00,\n",
            "           -6.6195e+00, -6.2547e+00]],\n",
            "\n",
            "         [[ 2.2635e+01,  1.3314e+01,  2.2276e+01,  ...,  1.6847e+01,\n",
            "            2.1588e+01,  8.3964e+00],\n",
            "          [ 7.8426e+00, -4.3926e+00,  3.8432e+00,  ...,  7.8468e+00,\n",
            "            5.9432e+00,  4.0180e+00],\n",
            "          [ 5.7116e+00,  1.9212e+00,  5.1535e+00,  ...,  7.8443e+00,\n",
            "            6.5853e+00,  2.6193e+00],\n",
            "          ...,\n",
            "          [ 2.4733e+00,  4.5308e+00,  8.8336e+00,  ...,  4.3601e+00,\n",
            "            2.0675e+01,  1.2300e+01],\n",
            "          [ 8.6246e+00,  1.3049e+01,  1.0848e+01,  ...,  1.1824e+01,\n",
            "            1.6602e+01,  2.1163e+00],\n",
            "          [ 8.7200e+00,  1.0570e+01,  1.9754e+00,  ...,  1.4642e+01,\n",
            "            5.2513e+00,  3.9170e+00]]],\n",
            "\n",
            "\n",
            "        [[[-8.0694e+00,  8.4925e+00,  9.4731e+00,  ..., -1.5121e+01,\n",
            "           -1.4483e+00,  6.0633e+00],\n",
            "          [ 5.4911e+00,  1.6237e+01,  8.3635e+00,  ...,  6.8306e+00,\n",
            "            1.4179e+01, -7.5524e+00],\n",
            "          [ 9.9252e+00,  5.1697e+00,  1.0682e+00,  ...,  6.4955e+00,\n",
            "           -1.0569e+01, -1.0987e+01],\n",
            "          ...,\n",
            "          [-4.2900e+00, -6.2318e+00, -5.2448e+00,  ..., -2.2150e+01,\n",
            "           -7.9557e+00, -2.9854e+00],\n",
            "          [ 1.8498e-01, -7.3512e+00, -4.3950e+00,  ..., -1.9788e+01,\n",
            "           -9.3896e-01, -6.2254e+00],\n",
            "          [-6.5743e-01, -8.1158e+00, -4.1001e+00,  ..., -2.5670e+00,\n",
            "            2.6823e+00, -1.1460e+01]],\n",
            "\n",
            "         [[ 8.6407e+00,  8.7362e-01, -9.5286e+00,  ...,  2.1527e+01,\n",
            "            7.1820e+00,  9.1982e-01],\n",
            "          [-4.8213e-01, -1.0662e+01, -1.2402e+01,  ..., -9.2143e+00,\n",
            "           -1.1944e+01, -6.2936e+00],\n",
            "          [-1.1949e+00, -7.2178e+00, -9.6372e+00,  ..., -5.8519e+00,\n",
            "           -6.1638e-01,  3.2553e-01],\n",
            "          ...,\n",
            "          [ 1.7341e+00, -3.3729e+00, -1.2800e+00,  ...,  9.3301e+00,\n",
            "            8.1392e+00,  5.8879e+00],\n",
            "          [-6.4628e+00, -4.4893e+00,  1.0474e-01,  ...,  1.4733e+01,\n",
            "            5.5949e+00,  8.6485e+00],\n",
            "          [ 7.8495e+00,  8.6691e+00,  1.3329e+01,  ...,  7.1430e+00,\n",
            "            9.8989e+00,  8.6670e+00]],\n",
            "\n",
            "         [[ 1.4398e+01,  2.0665e+01,  1.8174e+01,  ...,  4.2671e+01,\n",
            "            4.1375e+01,  2.8809e+01],\n",
            "          [ 1.7854e+01,  2.6901e+01,  1.9832e+01,  ...,  4.6488e+01,\n",
            "            4.5495e+01,  3.3020e+01],\n",
            "          [ 1.6016e+01,  2.5770e+01,  1.6878e+01,  ...,  3.6825e+01,\n",
            "            3.7274e+01,  2.9979e+01],\n",
            "          ...,\n",
            "          [ 1.2304e+01,  1.2721e+01,  1.3035e+01,  ...,  1.4422e+01,\n",
            "            1.4339e+01,  1.3564e+01],\n",
            "          [ 1.6592e+01,  2.0150e+01,  2.0501e+01,  ...,  1.8790e+01,\n",
            "            1.7844e+01,  1.6883e+01],\n",
            "          [ 1.4404e+01,  1.5643e+01,  1.5859e+01,  ...,  1.4720e+01,\n",
            "            1.3411e+01,  1.1534e+01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 9.4314e+00,  1.2170e+01,  1.3775e+01,  ..., -7.0257e+00,\n",
            "            7.0583e+00,  1.6758e+00],\n",
            "          [ 1.1764e+01,  1.8064e+00, -7.8782e-01,  ...,  1.0580e+01,\n",
            "            5.3784e+00, -4.2876e+00],\n",
            "          [ 8.1552e+00, -3.1093e+00, -4.7620e+00,  ..., -5.2022e+00,\n",
            "           -2.0684e+00, -6.1077e+00],\n",
            "          ...,\n",
            "          [-7.7528e+00, -8.5315e+00, -5.8527e+00,  ..., -1.6859e+01,\n",
            "           -1.2020e+01, -4.1291e+00],\n",
            "          [-1.0208e+01, -1.7148e+01, -1.8283e+01,  ..., -1.6878e+01,\n",
            "           -1.2425e+01, -1.0258e+01],\n",
            "          [-1.6997e+01, -1.7156e+01, -1.5808e+01,  ..., -6.1920e+00,\n",
            "           -1.2836e+01, -3.8263e+00]],\n",
            "\n",
            "         [[-1.3088e+01, -1.6174e+00,  4.3402e-02,  ..., -9.8733e+00,\n",
            "           -9.1630e+00,  2.3566e+00],\n",
            "          [-1.3302e+01,  7.5788e+00,  4.6126e+00,  ..., -1.3174e+01,\n",
            "           -1.0071e+01,  3.7118e+00],\n",
            "          [-9.7649e+00,  1.6817e+01,  1.0270e+01,  ..., -7.5083e+00,\n",
            "           -7.7328e+00,  4.2652e+00],\n",
            "          ...,\n",
            "          [ 3.5697e+00, -5.7580e+00, -3.6967e+00,  ..., -4.0247e+00,\n",
            "           -8.5789e+00, -4.5869e+00],\n",
            "          [-2.5470e+00, -1.1910e+01, -8.9635e+00,  ..., -1.1686e+01,\n",
            "           -6.3450e+00, -2.9179e+00],\n",
            "          [ 6.6134e+00, -6.6665e-01,  9.0055e-01,  ..., -3.6694e+00,\n",
            "            4.7339e+00,  1.3121e+00]],\n",
            "\n",
            "         [[ 9.7134e-01, -6.8947e+00, -2.8396e+00,  ...,  7.0102e+00,\n",
            "           -7.4238e+00, -5.1287e+00],\n",
            "          [-3.4087e+00, -3.0025e+00,  1.0088e+01,  ..., -9.9921e+00,\n",
            "           -9.9760e+00,  4.5461e+00],\n",
            "          [ 9.0420e-01,  2.0018e+00,  1.1364e+01,  ...,  1.5061e+01,\n",
            "            1.2994e+01,  5.8485e+00],\n",
            "          ...,\n",
            "          [ 5.3412e+00,  1.0156e+01,  7.2169e+00,  ...,  1.9620e+01,\n",
            "            7.8432e+00,  3.2628e+00],\n",
            "          [ 6.7165e+00,  1.4750e+01,  1.2183e+01,  ...,  1.1359e+01,\n",
            "            5.9445e+00,  9.3538e+00],\n",
            "          [ 1.4432e+01,  1.6001e+01,  1.5168e+01,  ...,  2.4814e+00,\n",
            "            8.1340e+00,  1.0025e+01]]],\n",
            "\n",
            "\n",
            "        [[[ 5.2990e+00, -3.0313e+01, -1.1507e+01,  ...,  6.5519e+00,\n",
            "            5.3768e+00, -8.7107e+00],\n",
            "          [-1.7960e+01, -3.2303e+01,  2.6233e+01,  ...,  1.0724e+01,\n",
            "           -9.6657e+00, -1.6107e+01],\n",
            "          [-2.7823e+01,  4.2001e-01,  2.9455e+01,  ..., -7.2944e+00,\n",
            "           -2.4694e+01, -2.5476e+00],\n",
            "          ...,\n",
            "          [-8.4686e+00, -1.9194e+00, -2.1950e+01,  ..., -1.5137e+01,\n",
            "           -5.1114e+00,  1.0815e+01],\n",
            "          [-7.2922e+00,  4.8374e+00,  2.5721e+00,  ..., -1.3228e+01,\n",
            "            1.6847e+01, -2.5263e+00],\n",
            "          [ 1.0809e+01,  3.8535e+00,  5.6010e-01,  ...,  7.4596e+00,\n",
            "            1.0815e+01, -4.3081e+00]],\n",
            "\n",
            "         [[ 1.2374e+01,  2.1123e+01,  7.1283e-01,  ...,  1.1858e+00,\n",
            "            4.6247e+00,  1.2295e+01],\n",
            "          [ 2.4060e+01,  1.0238e+01, -1.2659e+01,  ..., -1.2839e+00,\n",
            "            7.2545e+00,  7.0450e+00],\n",
            "          [ 1.6399e+01, -4.5828e+00, -1.2242e+01,  ...,  7.4383e+00,\n",
            "            1.6331e+01,  5.6062e+00],\n",
            "          ...,\n",
            "          [ 1.7799e+01,  1.9003e+01,  1.2541e+01,  ...,  6.8712e+00,\n",
            "           -3.8616e+00, -6.8981e+00],\n",
            "          [ 4.8800e+00, -1.9849e+00, -3.6954e+00,  ...,  3.3539e+00,\n",
            "           -4.0932e+00, -8.2298e-01],\n",
            "          [-4.9816e-01, -5.3151e+00, -2.7643e+00,  ..., -4.9600e+00,\n",
            "           -5.5407e+00, -4.1549e+00]],\n",
            "\n",
            "         [[ 1.1568e+01,  1.3207e+01,  1.1999e+01,  ...,  1.5548e+01,\n",
            "            1.5065e+01,  1.2399e+01],\n",
            "          [ 1.5731e+01,  1.6311e+01,  1.5215e+01,  ...,  1.7934e+01,\n",
            "            1.3757e+01,  1.2543e+01],\n",
            "          [ 1.4937e+01,  1.7708e+01,  1.6743e+01,  ...,  1.5947e+01,\n",
            "            1.1384e+01,  1.1515e+01],\n",
            "          ...,\n",
            "          [ 1.1250e+01,  1.6295e+01,  1.6910e+01,  ...,  1.6964e+01,\n",
            "            1.6011e+01,  1.3537e+01],\n",
            "          [ 1.3705e+01,  1.9684e+01,  1.9421e+01,  ...,  1.8645e+01,\n",
            "            1.8861e+01,  1.3637e+01],\n",
            "          [ 1.1995e+01,  1.7563e+01,  1.6083e+01,  ...,  1.6690e+01,\n",
            "            1.6514e+01,  1.0544e+01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.9161e+01, -1.4301e+01,  1.0478e+01,  ...,  1.7857e-01,\n",
            "           -7.0586e+00, -1.0125e+01],\n",
            "          [-2.2197e+01, -4.2044e+00,  4.7265e+00,  ..., -1.2182e+01,\n",
            "           -1.4570e+01, -9.1432e+00],\n",
            "          [-8.4517e+00,  6.3069e+00, -8.9538e+00,  ..., -2.1197e+01,\n",
            "           -1.2100e+01, -4.6234e+00],\n",
            "          ...,\n",
            "          [-1.0133e+01, -1.2204e+01, -5.5837e+00,  ..., -5.9159e+00,\n",
            "            6.4376e+00, -4.8611e+00],\n",
            "          [ 2.5071e+00,  1.2433e+00,  3.2544e+00,  ...,  4.7298e+00,\n",
            "            1.5652e+00, -4.0135e+00],\n",
            "          [ 4.2418e+00,  2.4560e+00, -3.8220e-01,  ...,  1.6191e+00,\n",
            "           -3.0472e+00, -5.5795e+00]],\n",
            "\n",
            "         [[-5.0080e+00, -1.3306e+01, -1.5332e+01,  ...,  3.4130e+00,\n",
            "            3.6272e+00, -2.0661e+00],\n",
            "          [-1.5427e+00, -2.1886e+01, -1.2391e+01,  ...,  8.5078e+00,\n",
            "            4.5587e+00, -5.6726e+00],\n",
            "          [-7.2513e-02, -2.6498e+01, -4.7915e-01,  ...,  4.7346e+00,\n",
            "           -4.4116e+00, -5.9710e+00],\n",
            "          ...,\n",
            "          [-9.6345e+00, -2.3154e+00, -7.3140e+00,  ..., -9.0183e+00,\n",
            "           -1.7975e+01,  3.2341e-01],\n",
            "          [-8.5814e+00,  1.3618e+00, -9.3338e+00,  ..., -9.7633e+00,\n",
            "           -1.6096e+01,  1.9958e+00],\n",
            "          [-6.6565e+00,  2.4775e+00, -5.2075e+00,  ..., -8.5282e+00,\n",
            "           -8.6760e+00,  4.3214e+00]],\n",
            "\n",
            "         [[ 1.1378e+01,  1.8199e+01, -2.7430e+00,  ..., -7.9344e-01,\n",
            "            9.8596e-01,  5.6365e+00],\n",
            "          [ 2.8821e+01,  3.3882e+00, -1.4700e+01,  ...,  2.4391e+00,\n",
            "            1.5522e+01,  5.0369e+00],\n",
            "          [ 1.9493e+01, -7.7785e+00,  2.8977e+00,  ...,  1.3201e+01,\n",
            "            1.0564e+01, -1.2347e-01],\n",
            "          ...,\n",
            "          [ 9.8504e+00,  5.1927e+00,  8.7044e+00,  ...,  8.2243e+00,\n",
            "           -2.9824e+00,  4.5608e+00],\n",
            "          [ 2.7757e+00, -3.4817e+00, -5.7334e+00,  ...,  4.5365e+00,\n",
            "           -4.7611e+00,  1.0335e+01],\n",
            "          [ 1.7130e+00,  6.5813e+00,  8.1130e+00,  ...,  2.0161e+00,\n",
            "            7.9501e+00,  7.5690e+00]]]], device='cuda:0',\n",
            "       grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# act_int.size = torch.Size([128, 64, 32, 32])  <- batch_size, input_ch, ni, nj\n",
        "a_int = act_int[0,:,:,:]  # pick only one input out of batch\n",
        "# a_int.size() = [64, 32, 32]\n",
        "\n",
        "# conv_int.weight.size() = torch.Size([64, 64, 3, 3])  <- output_ch, input_ch, ki, kj\n",
        "w_int = torch.reshape(weight_int, (weight_int.size(0), weight_int.size(1), -1))  # merge ki, kj index to kij\n",
        "# w_int.weight.size() = torch.Size([64, 64, 9])\n",
        "\n",
        "padding = 1\n",
        "stride = 1\n",
        "# He hard coded these values :(\n",
        "#array_size = 64 # row and column number\n",
        "array_size = w_int.size(0)\n",
        "\n",
        "nig = range(a_int.size(1))  ## ni group [0,1,...31]\n",
        "njg = range(a_int.size(2))  ## nj group\n",
        "\n",
        "icg = range(int(w_int.size(1)))  ## input channel [0,...63]\n",
        "ocg = range(int(w_int.size(0)))  ## output channel\n",
        "\n",
        "\n",
        "kijg = range(w_int.size(2)) # [0, .. 8]\n",
        "ki_dim = int(math.sqrt(w_int.size(2)))  ## Kernel's 1 dim size\n",
        "\n",
        "######## Padding before Convolution #######\n",
        "a_pad = torch.zeros(len(icg), len(nig)+padding*2, len(njg)+padding*2).cuda()\n",
        "# a_pad.size() = [64, 32+2pad, 32+2pad]\n",
        "a_pad[ :, padding:padding+len(nig), padding:padding+len(njg)] = a_int.cuda()\n",
        "a_pad = torch.reshape(a_pad, (a_pad.size(0), -1))  ## mergin ni and nj index into nij\n",
        "# a_pad.size() = [64, (32+2pad)*(32+2pad)]"
      ],
      "metadata": {
        "id": "F08nlhGQo-3Q"
      },
      "id": "F08nlhGQo-3Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(act_int.shape)\n",
        "print(weight_int.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7szG_d_pA9F",
        "outputId": "10612856-3256-479d-afe6-ef52eaee72dd"
      },
      "id": "r7szG_d_pA9F",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512, 64, 32, 32])\n",
            "torch.Size([64, 64, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###########################################\n",
        "\n",
        "p_nijg = range(a_pad.size(1)) ## paded activation's nij group [0, ...34*34-1]\n",
        "\n",
        "psum = torch.zeros( array_size, len(p_nijg), len(kijg)).cuda()\n",
        "\n",
        "for kij in kijg:\n",
        "    for nij in p_nijg:     # time domain, sequentially given input\n",
        "        m = nn.Linear(array_size, array_size, bias=False)\n",
        "        m.weight = torch.nn.Parameter(w_int[:,:,kij])\n",
        "        psum[:, nij, kij] = m(a_pad[:,nij]).cuda()\n",
        "\n"
      ],
      "metadata": {
        "id": "zaUxXysCpFxX"
      },
      "id": "zaUxXysCpFxX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "a_pad_ni_dim = int(math.sqrt(a_pad.size(1))) # 32 + 2*pad = 34\n",
        "\n",
        "o_ni_dim = int((a_pad_ni_dim - (ki_dim- 1) - 1)/stride + 1) #34 - 2 - 1 + 1 = 32\n",
        "o_nijg = range(o_ni_dim**2) # [0, 32*32-1]\n",
        "\n",
        "out = torch.zeros(len(ocg), len(o_nijg)).cuda()\n",
        "\n",
        "\n",
        "### SFP accumulation ###\n",
        "for o_nij in o_nijg:\n",
        "    for kij in kijg:  #[0, ... 8]\n",
        "        out[:,o_nij] = out[:,o_nij] + \\\n",
        "        psum[:, int(o_nij/o_ni_dim)*a_pad_ni_dim + o_nij%o_ni_dim + int(kij/ki_dim)*a_pad_ni_dim + kij%ki_dim, kij]\n",
        "                ## 2nd index = (int(o_nij/30)*32 + o_nij%30) + (int(kij/3)*32 + kij%3)"
      ],
      "metadata": {
        "id": "fVsnMp3IpIbi"
      },
      "id": "fVsnMp3IpIbi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_2D = torch.reshape(out, (out.size(0), o_ni_dim, -1)) # nij -> ni & nj\n",
        "difference = (out_2D - output_int[0,:,:,:])\n",
        "print(difference.abs().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0lTwPdppLdE",
        "outputId": "e8f952e5-422b-4bec-b569-4bb4eeb57d94"
      },
      "id": "X0lTwPdppLdE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(14.8769, device='cuda:0', grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_int[0,:,:,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBFrmynTpOPc",
        "outputId": "c8080db0-ef1e-4d82-803d-5de685f6a64a"
      },
      "id": "fBFrmynTpOPc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-8.9000e+01, -5.1600e+02, -3.6500e+02,  ..., -3.4700e+02,\n",
              "          -6.0600e+02,  1.5000e+01],\n",
              "         [-1.6100e+02, -5.5600e+02, -3.2300e+02,  ..., -3.2600e+02,\n",
              "          -3.1500e+02,  4.3000e+01],\n",
              "         [-3.9300e+02, -2.5600e+02, -4.9300e+02,  ..., -3.4900e+02,\n",
              "          -3.2800e+02, -1.0200e+02],\n",
              "         ...,\n",
              "         [-9.0300e+02, -8.0800e+02, -7.5500e+02,  ..., -1.3290e+03,\n",
              "           4.5600e+02, -1.0630e+03],\n",
              "         [-1.1820e+03, -6.6500e+02, -9.6600e+02,  ..., -1.2900e+02,\n",
              "          -3.9900e+02, -6.2800e+02],\n",
              "         [-5.7500e+02, -5.8900e+02, -2.5200e+02,  ...,  5.2300e+02,\n",
              "          -7.2000e+01,  4.5900e+02]],\n",
              "\n",
              "        [[ 5.9800e+02,  1.1620e+03,  7.8200e+02,  ...,  8.0200e+02,\n",
              "           6.6300e+02,  2.8200e+02],\n",
              "         [ 1.1190e+03,  7.7300e+02,  5.2300e+02,  ...,  3.7900e+02,\n",
              "           7.5000e+01, -2.6900e+02],\n",
              "         [ 6.7500e+02,  4.6600e+02,  4.2600e+02,  ...,  4.9700e+02,\n",
              "           1.0900e+02, -2.8000e+01],\n",
              "         ...,\n",
              "         [ 7.5300e+02,  4.7400e+02,  4.2500e+02,  ...,  6.5000e+02,\n",
              "           2.6400e+02,  1.3300e+02],\n",
              "         [ 4.0800e+02,  5.0300e+02,  8.3100e+02,  ...,  7.3100e+02,\n",
              "           3.5500e+02, -7.3000e+01],\n",
              "         [ 2.1100e+02,  3.7800e+02,  3.0600e+02,  ..., -1.8300e+02,\n",
              "          -4.3300e+02, -1.9400e+02]],\n",
              "\n",
              "        [[ 8.4000e+02,  1.4140e+03,  1.3110e+03,  ...,  1.4670e+03,\n",
              "           1.4920e+03,  1.2780e+03],\n",
              "         [ 8.5100e+02,  1.2530e+03,  1.0710e+03,  ...,  1.0190e+03,\n",
              "           1.2500e+03,  1.1570e+03],\n",
              "         [ 7.0400e+02,  9.3500e+02,  6.4400e+02,  ...,  7.8900e+02,\n",
              "           1.0560e+03,  1.0550e+03],\n",
              "         ...,\n",
              "         [ 1.3950e+03,  1.6110e+03,  1.9450e+03,  ...,  1.8940e+03,\n",
              "           2.1310e+03,  1.5620e+03],\n",
              "         [ 1.4340e+03,  1.9690e+03,  2.3540e+03,  ...,  2.3370e+03,\n",
              "           2.5990e+03,  2.0250e+03],\n",
              "         [ 1.4370e+03,  1.9370e+03,  2.1440e+03,  ...,  2.1420e+03,\n",
              "           2.4570e+03,  1.6820e+03]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-1.0160e+03, -9.9700e+02, -9.5600e+02,  ..., -5.1400e+02,\n",
              "           1.0400e+02,  3.3100e+02],\n",
              "         [-8.9500e+02, -7.4300e+02, -1.0150e+03,  ..., -5.1500e+02,\n",
              "           2.8000e+02,  4.0000e+01],\n",
              "         [-7.9000e+02, -8.0500e+02, -8.9000e+02,  ..., -4.0200e+02,\n",
              "           3.1200e+02, -7.6000e+01],\n",
              "         ...,\n",
              "         [-9.8600e+02, -1.1390e+03, -8.0900e+02,  ..., -5.0300e+02,\n",
              "          -8.6600e+02, -2.5200e+02],\n",
              "         [-9.0900e+02, -1.2210e+03, -9.7600e+02,  ..., -3.0600e+02,\n",
              "          -4.6500e+02,  5.1000e+01],\n",
              "         [-7.2700e+02, -3.4900e+02, -2.4400e+02,  ...,  9.4000e+01,\n",
              "          -6.1035e-05, -1.2700e+02]],\n",
              "\n",
              "        [[-2.7700e+02, -1.9100e+02, -3.5100e+02,  ..., -5.4400e+02,\n",
              "          -8.7200e+02, -4.5000e+02],\n",
              "         [-1.4000e+02, -2.1600e+02, -3.9600e+02,  ..., -6.3500e+02,\n",
              "          -1.0520e+03, -4.6100e+02],\n",
              "         [ 5.6000e+01, -4.6000e+01, -4.2000e+02,  ..., -5.0100e+02,\n",
              "          -9.0600e+02, -4.2100e+02],\n",
              "         ...,\n",
              "         [-3.5000e+02, -1.7930e+03, -9.5200e+02,  ..., -1.7180e+03,\n",
              "          -1.8450e+03,  5.0900e+02],\n",
              "         [-5.1700e+02, -1.7730e+03, -8.6900e+02,  ..., -1.2580e+03,\n",
              "          -1.3160e+03,  1.4000e+02],\n",
              "         [-1.2500e+02, -7.9200e+02, -4.0400e+02,  ..., -3.6500e+02,\n",
              "          -5.4300e+02,  1.8100e+02]],\n",
              "\n",
              "        [[ 9.9300e+02,  3.7300e+02,  4.7200e+02,  ...,  5.5300e+02,\n",
              "           3.0800e+02, -1.5200e+02],\n",
              "         [ 1.0010e+03, -3.0000e+01,  3.7600e+02,  ..., -1.5700e+02,\n",
              "          -4.0300e+02, -2.1900e+02],\n",
              "         [ 1.0560e+03,  6.4000e+01,  7.3500e+02,  ...,  2.8900e+02,\n",
              "          -7.8000e+01,  1.0100e+02],\n",
              "         ...,\n",
              "         [ 8.0400e+02,  7.5500e+02,  6.0400e+02,  ...,  6.4000e+01,\n",
              "           3.3600e+02,  1.1590e+03],\n",
              "         [ 8.8200e+02,  1.0890e+03,  5.6200e+02,  ..., -3.0800e+02,\n",
              "           5.8000e+02,  4.7100e+02],\n",
              "         [ 8.6300e+02,  8.7200e+02,  2.4100e+02,  ...,  5.9999e+00,\n",
              "           3.1200e+02,  1.5100e+02]]], device='cuda:0',\n",
              "       grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "corresponding-significance",
      "metadata": {
        "id": "corresponding-significance"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exposed-witch",
      "metadata": {
        "id": "exposed-witch"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "entitled-barbados",
      "metadata": {
        "id": "entitled-barbados"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "minimal-serbia",
      "metadata": {
        "id": "minimal-serbia"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
