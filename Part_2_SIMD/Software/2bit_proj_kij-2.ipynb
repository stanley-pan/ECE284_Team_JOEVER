{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "radical-fifty",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from tensorboardX import SummaryWriter      \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# print('=> Building model...')\n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"VGG16_quant\"\n",
    "model = VGG16_quant()\n",
    "# print(model.features)\n",
    "# device = torch.device(\"cuda\") \n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [150, 225]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 8966/10000 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.classifier = nn.Linear(16, 10) # Fix classifier size to match checkpoint\n",
    "\n",
    "PATH = \"result/mimi_VGG16_quant/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceramic-nigeria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 -th layer prehooked\n",
      "7 -th layer prehooked\n",
      "12 -th layer prehooked\n",
      "16 -th layer prehooked\n",
      "21 -th layer prehooked\n",
      "25 -th layer prehooked\n",
      "29 -th layer prehooked\n",
      "34 -th layer prehooked\n",
      "38 -th layer prehooked\n",
      "41 -th layer prehooked\n",
      "46 -th layer prehooked\n",
      "50 -th layer prehooked\n",
      "54 -th layer prehooked\n"
     ]
    }
   ],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "i = 0\n",
    "\n",
    "for layer in model.modules():\n",
    "    i = i+1\n",
    "    if isinstance(layer, QuantConv2d):\n",
    "        print(i,\"-th layer prehooked\")\n",
    "        layer.register_forward_pre_hook(save_output)             \n",
    "####################################################\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "out = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "251e9b4a-1183-4bd2-b621-231b0417009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_q = model.features[27].weight_q\n",
    "w_alpha = model.features[27].weight_quant.wgt_alpha\n",
    "\n",
    "# print(weight_q)\n",
    "w_bit = 2\n",
    "\n",
    "weight_int = weight_q / (w_alpha / (2**(w_bit-1)-1))\n",
    "# print(weight_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84f667f0-b48a-47ad-8c09-f774e4b81c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 16, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "k = 8  # input to your 8→8 conv layer\n",
    "\n",
    "act = save_output.outputs[k][0]      # [128, 8, 4, 4]\n",
    "act_alpha = model.features[27].act_alpha\n",
    "act_bit = 2\n",
    "act_quant_fn = act_quantization(act_bit)\n",
    "\n",
    "# quantize + convert to integer\n",
    "act_q   = act_quant_fn(act, act_alpha)\n",
    "act_int = act_q / (act_alpha / (2**act_bit - 1))\n",
    "\n",
    "# sanity check\n",
    "print(act_int.shape)  # should be [128, 16, 4, 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1bd26d5-0f61-4d65-a06e-366c78c733ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 16, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "conv_int = torch.nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, padding=1, bias=False)\n",
    "conv_int.weight = torch.nn.Parameter(weight_int.float())\n",
    "\n",
    "output_int = conv_int(act_int.float())\n",
    "\n",
    "output_recovered = output_int * (act_alpha / (2**act_bit - 1)) * (w_alpha / (2**(w_bit-1) - 1))\n",
    "\n",
    "print(output_recovered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0ab3a8b-3a6c-4cce-8995-e8ea6ca8aa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean error: 3.0168860121193575e-07\n"
     ]
    }
   ],
   "source": [
    "device = act.device\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv_ref = model.features[27]\n",
    "    output_ref = conv_ref(act.float())\n",
    "\n",
    "err = (output_ref - output_recovered).abs().mean()\n",
    "print(\"mean error:\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "subsequent-oracle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_int shape: torch.Size([16, 16, 9])\n"
     ]
    }
   ],
   "source": [
    "# act_int.size = torch.Size([128, 64, 32, 32])  <- batch_size, input_ch, ni, nj\n",
    "a_int = act_int[0,:,:,:]  # pick only one input out of batch\n",
    "\n",
    "w_int = torch.reshape(weight_int, (weight_int.size(0), weight_int.size(1), -1))  # merge ki, kj index to kij\n",
    " # weight_int: [out_ch, in_ch, ki, kj]  ->  [out_ch, in_ch, kij]\n",
    "print(\"w_int shape:\", w_int.shape)   # should be [16, 16, 9]\n",
    "# print(a_int.shape)\n",
    "padding = 1\n",
    "stride = 1\n",
    "array_size = 8 # row and column number\n",
    "\n",
    "nig = range(a_int.size(1))  ## ni group\n",
    "njg = range(a_int.size(2))  ## nj group\n",
    "\n",
    "icg = range(int(w_int.size(1)))  ## input channel \n",
    "ocg = range(int(w_int.size(0)))  ## output channel\n",
    "\n",
    "ic_tileg = range(int(len(icg)/array_size))\n",
    "oc_tileg = range(int(len(ocg)/array_size))\n",
    "\n",
    "kijg = range(w_int.size(2))\n",
    "ki_dim = int(math.sqrt(w_int.size(2)))  ## Kernel's 1 dim size\n",
    "\n",
    "######## Padding before Convolution #######\n",
    "a_pad = torch.zeros(len(icg), len(nig)+padding*2, len(nig)+padding*2).cuda()\n",
    "# a_pad.size() = [64, 32+2pad, 32+2pad]\n",
    "a_pad[ :, padding:padding+len(nig), padding:padding+len(njg)] = a_int.cuda()\n",
    "a_pad = torch.reshape(a_pad, (a_pad.size(0), -1))\n",
    "# a_pad.size() = [64, (32+2pad)*(32+2pad)]\n",
    "\n",
    "\n",
    "a_tile = torch.zeros(len(ic_tileg), array_size,    a_pad.size(1)).cuda() \n",
    "w_tile = torch.zeros(len(oc_tileg)*len(ic_tileg), array_size, array_size, len(kijg)).cuda() \n",
    "\n",
    "for ic_tile in ic_tileg:\n",
    "    a_tile[ic_tile,:,:] = a_pad[ic_tile*array_size:(ic_tile+1)*array_size,:]\n",
    "\n",
    "for ic_tile in ic_tileg:\n",
    "    for oc_tile in oc_tileg:\n",
    "        w_tile[oc_tile*len(oc_tileg) + ic_tile,:,:,:] = w_int[oc_tile*array_size:(oc_tile+1)*array_size, ic_tile*array_size:(ic_tile+1)*array_size, :]\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "p_nijg = range(a_pad.size(1)) ## psum nij group\n",
    "\n",
    "psum = torch.zeros(len(ic_tileg), len(oc_tileg), array_size, len(p_nijg), len(kijg)).cuda()\n",
    "# psum = torch.zeros(16, 4*4).cuda()\n",
    "\n",
    "for kij in kijg:\n",
    "    for ic_tile in ic_tileg:\n",
    "        for oc_tile in oc_tileg:\n",
    "            for nij in p_nijg:\n",
    "                psum[ic_tile, oc_tile, :, nij, kij] = \\\n",
    "                torch.matmul(\n",
    "                    w_tile[len(oc_tileg)*oc_tile + ic_tile,:,:,kij].float(),\n",
    "                    a_tile[ic_tile,:,nij].float()\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "skilled-projector",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "activation.txt code (2-bit)\n",
    "'''\n",
    "act_bits = 2\n",
    "\n",
    "a_int = act_int[0, :, :, :]        # now shape [16, 4, 4]\n",
    "act_tile = a_int\n",
    "C, H, W = act_tile.shape           # C=16, H=4, W=4\n",
    "\n",
    "with open(\"2bit_activation.txt\", \"w\") as f:\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            vals = []\n",
    "            for ch in range(C):  # 16 channels\n",
    "                val = int(act_tile[ch, i, j].item())\n",
    "                val = max(0, min(val, (1<<act_bits)-1))   # clip to 0–3\n",
    "                vals.append(val)\n",
    "\n",
    "            # pack into 32 bits now\n",
    "            word = 0\n",
    "            for v in vals:\n",
    "                word = (word << act_bits) | v\n",
    "\n",
    "            bits32 = f\"{word:032b}\"\n",
    "            f.write(bits32 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d185e29e-ea25-4493-9bd5-7bc68fd8bbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tile_out_int shape: torch.Size([16, 4, 4])\n",
      "Wrote ReLU outputs to 2bit_output_relu.txt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# === Choose layer–27 tile ===\n",
    "a_full = act_int[0,:,:,:]       # [16,4,4]\n",
    "w_full = weight_int[:,:,:,:]    # [16,16,3,3]\n",
    "\n",
    "# Use whole 4x4 as tile\n",
    "a_tile = a_full                 # [16,4,4]\n",
    "\n",
    "# Conv wants [N,C,H,W]\n",
    "a_tile_batch = a_tile.unsqueeze(0).float()  # [1,16,4,4]\n",
    "w_full_f     = w_full.float()               # [16,16,3,3]\n",
    "\n",
    "# === Conv: stride=1, padding=1 => keep size 4x4 ===\n",
    "tile_out = F.conv2d(\n",
    "    a_tile_batch,\n",
    "    w_full_f,\n",
    "    bias=None,\n",
    "    stride=1,\n",
    "    padding=1\n",
    ")   # [1,16,4,4]\n",
    "\n",
    "# === ReLU ===\n",
    "tile_out_relu = torch.clamp(tile_out, min=0.0)   # [1,16,4,4]\n",
    "\n",
    "# === Convert to integer psum ===\n",
    "tile_out_int = torch.round(tile_out_relu).to(torch.int32)\n",
    "tile_out_int = tile_out_int[0]   # [16,4,4]\n",
    "\n",
    "print(\"tile_out_int shape:\", tile_out_int.shape)\n",
    "\n",
    "# === Write to txt: each line = 256 bits ===\n",
    "out_path = \"2bit_output_relu.txt\"\n",
    "\n",
    "with open(out_path, \"w\") as f:\n",
    "    for h in range(tile_out_int.size(1)):     # 4\n",
    "        for w in range(tile_out_int.size(2)): # 4\n",
    "            vals = tile_out_int[:, h, w]      # [16]\n",
    "\n",
    "            bits_per_pixel = []\n",
    "            for v in vals:\n",
    "                v16 = int(v.item()) & 0xFFFF\n",
    "                bits16 = format(v16, \"016b\")\n",
    "                bits_per_pixel.append(bits16)\n",
    "\n",
    "            line = \"\".join(bits_per_pixel)    # 16×16 = 256 bits\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "print(\"Wrote ReLU outputs to\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "accomplished-folks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_kij = 9\n",
      "Saved weight_kij/weight_k0.txt\n",
      "Saved weight_kij/weight_k1.txt\n",
      "Saved weight_kij/weight_k2.txt\n",
      "Saved weight_kij/weight_k3.txt\n",
      "Saved weight_kij/weight_k4.txt\n",
      "Saved weight_kij/weight_k5.txt\n",
      "Saved weight_kij/weight_k6.txt\n",
      "Saved weight_kij/weight_k7.txt\n",
      "Saved weight_kij/weight_k8.txt\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Generate weight_kij files for layer 27 (16→16)\n",
    "weight_k0.txt ... weight_k8.txt\n",
    "Each file: 16 rows, each row = 16 weights (4-bit)\n",
    "packed into 64 bits\n",
    "'''\n",
    "\n",
    "import os\n",
    "\n",
    "w_bit = 4\n",
    "\n",
    "# weight_int shape now [16,16,3,3], flatten to [16,16,9]\n",
    "w_int = weight_int.view(weight_int.size(0), weight_int.size(1), -1).int()\n",
    "\n",
    "os.makedirs(\"weight_kij\", exist_ok=True)\n",
    "\n",
    "num_kij = w_int.size(2)   # should now be 9\n",
    "print(\"num_kij =\", num_kij)\n",
    "\n",
    "for kij in range(num_kij):\n",
    "    fname = f\"weight_kij/weight_k{kij}.txt\"\n",
    "    with open(fname, \"w\") as f:\n",
    "        for oc in range(w_int.size(0)):   # 0..15\n",
    "            vals = []\n",
    "            for ic in range(w_int.size(1)):   # 0..15\n",
    "                v = int(w_int[oc, ic, kij].item())\n",
    "                if v < 0:\n",
    "                    v += (1 << w_bit)\n",
    "                v &= (1 << w_bit) - 1\n",
    "                vals.append(v)\n",
    "\n",
    "            # pack into 64-bit\n",
    "            word = 0\n",
    "            for v in vals:\n",
    "                word = (word << w_bit) | v\n",
    "\n",
    "            f.write(f\"{word:064b}\\n\")\n",
    "\n",
    "    print(f\"Saved {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da8ffaa3-0649-4e72-9e9a-3bd29dd45cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_int shape: torch.Size([1, 16, 4, 4])\n",
      "w_int shape: torch.Size([16, 16, 3, 3])\n",
      "psum_recovered shape: torch.Size([1, 16, 4, 4])\n",
      "max |psum_recovered − conv27+relu28|: 2.843454122543335\n",
      "max |psum_recovered − act29_in| : 1.7434358596801758e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ========= 1. Quantization parameters ============\n",
    "w_bit = 4   # 4-bit weights\n",
    "act_bit = 2 # 2-bit activations\n",
    "\n",
    "# === Weights for layer 27 ===\n",
    "weight_q = model.features[27].weight_q.detach()      # [16,16,3,3]\n",
    "w_alpha = model.features[27].weight_quant.wgt_alpha.detach()\n",
    "w_delta = w_alpha / (2**(w_bit-1) - 1)\n",
    "w_int = weight_q / w_delta                          # [16,16,3,3]\n",
    "\n",
    "# === Activations input to layer 27 ===\n",
    "act27_in = act_int[0].to(device)                    # [16,4,4]\n",
    "a_full = act27_in.unsqueeze(0)                      # [1,16,4,4]\n",
    "\n",
    "act_alpha = model.features[27].act_alpha.detach()\n",
    "a_delta = act_alpha / (2**act_bit - 1)              # scale\n",
    "# DO NOT re–quantize, `act_int` was already quantized\n",
    "\n",
    "print(\"a_int shape:\", a_full.shape)\n",
    "print(\"w_int shape:\", w_int.shape)\n",
    "\n",
    "# ========= 2. Integer Conv2d =====================\n",
    "conv2 = nn.Conv2d(\n",
    "    in_channels = 16,\n",
    "    out_channels = 16,\n",
    "    kernel_size = 3,\n",
    "    padding = 1,\n",
    "    bias = False,\n",
    ").to(device)\n",
    "\n",
    "conv2.weight = nn.Parameter(w_int.float().to(device))\n",
    "\n",
    "# ========= 3. Integer psum then ReLU ===============\n",
    "with torch.no_grad():\n",
    "    psum_int = conv2(a_full.float())    # [1,16,4,4]\n",
    "    psum_int = torch.clamp(psum_int, min=0)\n",
    "    psum_recovered = psum_int * w_delta * a_delta\n",
    "\n",
    "print(\"psum_recovered shape:\", psum_recovered.shape)\n",
    "\n",
    "# ========= 4. Compute true reference (conv27→relu28) ==\n",
    "with torch.no_grad():\n",
    "    y = model.features[27](a_full)  # conv27\n",
    "    y = model.features[28](y)       # relu28\n",
    "    y_ref = y[0]                    # [16,4,4]\n",
    "\n",
    "# ========= 5. Get true next layer input ============\n",
    "# From SaveOutput:\n",
    "# k = 8 for layer 27 input\n",
    "# k+1 = 9 for layer 29 input (after ReLU)\n",
    "act29_in = save_output.outputs[9][0]  # [128,16,4,4]\n",
    "act29_in = act29_in[0].to(device)     # [16,4,4]\n",
    "\n",
    "# ========= 6. Compute errors ========================\n",
    "err_ref  = (psum_recovered[0] - y_ref).abs().max()\n",
    "err_next = (psum_recovered[0] - act29_in).abs().max()\n",
    "\n",
    "print(\"max |psum_recovered − conv27+relu28|:\", err_ref.item())\n",
    "print(\"max |psum_recovered − act29_in| :\", err_next.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cec03092-fb86-48a5-a85a-353720f38077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max |activation.txt - act_int[0]| = 0\n",
      "max |weight_k*.txt - weight_int|   = 0\n",
      "max |output_relu_16ch.txt - tile_out_int| = 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "\n",
    "def load_activation_txt(path, C=16, H=4, W=4, act_bits=2):\n",
    "    \"\"\"Reverse of your activation writer:\n",
    "       word = 0; for v in vals: word = (word<<act_bits) | v;\n",
    "       bits = f\"{word:032b}\"  (MSB = ch0).\n",
    "    \"\"\"\n",
    "    act_rec = torch.zeros(C, H, W, dtype=torch.int32)\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    assert len(lines) == H*W, f\"activation lines {len(lines)} != {H*W}\"\n",
    "\n",
    "    idx = 0\n",
    "    mask = (1 << act_bits) - 1\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            bits = lines[idx]\n",
    "            idx += 1\n",
    "            assert len(bits) == C*act_bits, f\"act line len {len(bits)} != {C*act_bits}\"\n",
    "            word = int(bits, 2)\n",
    "\n",
    "            # unpack ch15..ch0 (reverse of write)\n",
    "            for ch in reversed(range(C)):\n",
    "                v = word & mask\n",
    "                word >>= act_bits\n",
    "                act_rec[ch, i, j] = v\n",
    "\n",
    "    return act_rec\n",
    "\n",
    "\n",
    "def load_weights_kij(folder, C_in=16, C_out=16, w_bit=4, num_kij=9):\n",
    "    \"\"\"Reverse of weight writer:\n",
    "       vals(ic0..ic15), then:\n",
    "       word = 0; for v in vals: word = (word<<w_bit) | v;\n",
    "       bits64 = f\"{word:064b}\".\n",
    "    \"\"\"\n",
    "    w_rec = torch.zeros(C_out, C_in, num_kij, dtype=torch.int32)\n",
    "    mask = (1 << w_bit) - 1\n",
    "    sign_bit = 1 << (w_bit - 1)\n",
    "\n",
    "    for kij in range(num_kij):\n",
    "        path = os.path.join(folder, f\"weight_k{kij}.txt\")\n",
    "        with open(path, \"r\") as f:\n",
    "            lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "        assert len(lines) == C_out, f\"k={kij}: rows {len(lines)} != {C_out}\"\n",
    "\n",
    "        for oc, bits in enumerate(lines):\n",
    "            assert len(bits) == C_in * w_bit, f\"k={kij}, oc={oc}: len {len(bits)}\"\n",
    "            word = int(bits, 2)\n",
    "\n",
    "            # unpack ic15..ic0\n",
    "            for ic in reversed(range(C_in)):\n",
    "                v = word & mask\n",
    "                word >>= w_bit\n",
    "                # back to signed 4-bit two's complement\n",
    "                if v & sign_bit:\n",
    "                    v -= (1 << w_bit)\n",
    "                w_rec[oc, ic, kij] = v\n",
    "\n",
    "    return w_rec\n",
    "\n",
    "\n",
    "def load_output_relu(path, C=16, H=4, W=4):\n",
    "    \"\"\"Reverse of your output writer:\n",
    "       for v in vals (ch0..ch15):\n",
    "           bits16 = f\"{v16:016b}\"\n",
    "       line = ''.join(bits16_list)\n",
    "    \"\"\"\n",
    "    out_rec = torch.zeros(C, H, W, dtype=torch.int32)\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    assert len(lines) == H*W, f\"output lines {len(lines)} != {H*W}\"\n",
    "\n",
    "    idx = 0\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            bits = lines[idx]\n",
    "            idx += 1\n",
    "            assert len(bits) == 16 * C, f\"out line len {len(bits)} != {16*C}\"\n",
    "\n",
    "            for ch in range(C):\n",
    "                seg = bits[16*ch : 16*(ch+1)]   # ch0 first, same as write\n",
    "                v = int(seg, 2)\n",
    "                # if you treat psums as signed 16-bit:\n",
    "                if v >= 0x8000:\n",
    "                    v -= 0x10000\n",
    "                out_rec[ch, i, j] = v\n",
    "\n",
    "    return out_rec\n",
    "\n",
    "\n",
    "# ---------- 1) Check activation.txt ----------\n",
    "\n",
    "a_ref = act_int[0].cpu().int()          # [16,4,4] – quantized activations you used\n",
    "act_rec = load_activation_txt(\"2bit_activation.txt\", C=16, H=4, W=4, act_bits=2)\n",
    "\n",
    "diff_act = (act_rec - a_ref).abs().max()\n",
    "print(\"max |activation.txt - act_int[0]| =\", diff_act.item())\n",
    "\n",
    "\n",
    "# ---------- 2) Check weight_k*.txt ----------\n",
    "\n",
    "# flatten ki,kj -> kij to match the files\n",
    "w_ref = weight_int.view(weight_int.size(0), weight_int.size(1), -1).cpu().int()  # [16,16,9]\n",
    "num_kij = w_ref.size(2)\n",
    "\n",
    "w_rec = load_weights_kij(\"weight_kij\", C_in=16, C_out=16, w_bit=4, num_kij=num_kij)\n",
    "\n",
    "diff_w = (w_rec - w_ref).abs().max()\n",
    "print(\"max |weight_k*.txt - weight_int|   =\", diff_w.item())\n",
    "\n",
    "\n",
    "# ---------- 3) Check output_relu_16ch.txt ----------\n",
    "\n",
    "out_ref = tile_out_int.cpu().int()      # [16,4,4] – psums you wrote out\n",
    "out_rec = load_output_relu(\"2bit_output_relu.txt\", C=16, H=4, W=4)\n",
    "\n",
    "diff_out = (out_rec - out_ref).abs().max()\n",
    "print(\"max |output_relu_16ch.txt - tile_out_int| =\", diff_out.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317ed2b7-3089-4cfa-a314-3b9e5fbc58c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
