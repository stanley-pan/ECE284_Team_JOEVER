{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os, time, shutil\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a825a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device Selection\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True\n",
    "torch.backends.cudnn.fastest = True   # maximize RTX 3080 throughput\n",
    "print(f\"=> Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a70fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "batch_size = 128\n",
    "epochs = 200\n",
    "lr = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88cd758",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "save_dir = \"result/VGG16_quant\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = VGG16_quant().to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"=> Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[25, 40], gamma=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447],\n",
    "                                 std=[0.247, 0.243, 0.262])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "#                                            shuffle=True, num_workers=4, pin_memory=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "#                                           shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=256,              # 128 â†’ 256 (fits in 10 GB easily)\n",
    "    shuffle=True,\n",
    "    num_workers=os.cpu_count(),  # use all CPU cores\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4,           # overlap data loading with compute\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=512,              # eval can use larger batch\n",
    "    shuffle=False,\n",
    "    num_workers=os.cpu_count(),\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self): self.reset()\n",
    "    def reset(self): self.val=self.avg=self.sum=self.count=0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val; self.sum += val*n; self.count += n; self.avg = self.sum/self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    maxk = max(topk)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / target.size(0)))\n",
    "    return res\n",
    "\n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "# def train(train_loader, model, criterion, optimizer, epoch):\n",
    "#     model.train()\n",
    "#     losses, top1 = AverageMeter(), AverageMeter()\n",
    "#     start = time.time()\n",
    "\n",
    "#     for i, (inputs, targets) in enumerate(train_loader):\n",
    "#         inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "\n",
    "#         prec1 = accuracy(outputs, targets)[0]\n",
    "#         losses.update(loss.item(), inputs.size(0))\n",
    "#         top1.update(prec1.item(), inputs.size(0))\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if i % 100 == 0:\n",
    "#             print(f\"Epoch [{epoch}] [{i}/{len(train_loader)}] \"\n",
    "#                   f\"Loss {losses.val:.4f} ({losses.avg:.4f})  \"\n",
    "#                   f\"Acc {top1.val:.2f}% ({top1.avg:.2f}%)\")\n",
    "\n",
    "#     print(f\" Epoch {epoch} done in {time.time()-start:.1f}s | Train Acc: {top1.avg:.2f}% | Loss: {losses.avg:.4f}\")\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    losses, top1 = AverageMeter(), AverageMeter()\n",
    "    start = time.time()\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        # accuracy in full precision is fine (small overhead)\n",
    "        prec1 = accuracy(outputs, targets)[0]\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}] [{i}/{len(train_loader)}] \"\n",
    "                f\"LR {current_lr:.5e}  \"\n",
    "                f\"Loss {losses.val:.4f} ({losses.avg:.4f})  \"\n",
    "                f\"Acc {top1.val:.2f}% ({top1.avg:.2f}%)\"\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        f\" Epoch {epoch} done in {time.time()-start:.1f}s | \"\n",
    "        f\"LR: {current_lr:.5e} | Train Acc: {top1.avg:.2f}% | Loss: {losses.avg:.4f}\"\n",
    "    )\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch):\n",
    "    model.eval()\n",
    "    losses, top1 = AverageMeter(), AverageMeter()\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in enumerate(val_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            prec1 = accuracy(outputs, targets)[0]\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "    print(f\"Validation Epoch {epoch}: Acc {top1.avg:.2f}% | Loss {losses.avg:.4f}\")\n",
    "    return top1.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb4dbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "for epoch in range(1, epochs+1):\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "    val_acc = validate(test_loader, model, criterion, epoch)\n",
    "    scheduler.step()\n",
    "\n",
    "    is_best = val_acc > best_acc\n",
    "    best_acc = max(val_acc, best_acc)\n",
    "\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, save_dir)\n",
    "\n",
    "    print(f\"Epoch {epoch} complete | Best Acc: {best_acc:.2f}%\\n\")\n",
    "\n",
    "print(\"Training completed. Best accuracy: {:.2f}%\".format(best_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"result/VGG16_quant/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "i = 0\n",
    "\n",
    "for layer in model.modules():\n",
    "    i = i+1\n",
    "    if isinstance(layer, QuantConv2d):\n",
    "        print(i,\"-th layer prehooked\")\n",
    "        layer.register_forward_pre_hook(save_output)             \n",
    "####################################################\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "out = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e589f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_q = model.features[3].weight_q\n",
    "w_alpha = model.features[3].weight_quant.wgt_alpha\n",
    "w_bit = 4\n",
    "\n",
    "weight_int = weight_q / (w_alpha / (2**(w_bit-1)-1))\n",
    "print(weight_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "act = save_output.outputs[1][0]\n",
    "act_alpha  = model.features[3].act_alpha\n",
    "act_bit = 4\n",
    "act_quant_fn = act_quantization(act_bit)\n",
    "\n",
    "act_q = act_quant_fn(act, act_alpha)\n",
    "\n",
    "act_int = act_q / (act_alpha / (2**act_bit-1))\n",
    "print(act_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changed the code to mach the dimensions\n",
    "conv_int = torch.nn.Conv2d(in_channels = 8, out_channels=8, kernel_size = 3, padding=1)\n",
    "conv_int.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "conv_int.bias = model.features[3].bias\n",
    "output_int = conv_int(act_int)\n",
    "output_recovered = output_int * (act_alpha / (2**act_bit-1)) * (w_alpha / (2**(w_bit-1)-1))\n",
    "print(output_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_ref = torch.nn.Conv2d(in_channels = 8, out_channels=8, kernel_size = 3, padding=1)\n",
    "conv_ref.weight = model.features[3].weight_q\n",
    "conv_ref.bias = model.features[3].bias\n",
    "output_ref = conv_ref(act)\n",
    "print(output_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "blind-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "# act_int.size = torch.Size([128, 64, 32, 32])  <- batch_size, input_ch, ni, nj\n",
    "a_int = act_int[0,:,:,:]  # pick only one input out of batch\n",
    "# a_int.size() = [64, 32, 32]\n",
    "\n",
    "# conv_int.weight.size() = torch.Size([64, 64, 3, 3])  <- output_ch, input_ch, ki, kj\n",
    "w_int = torch.reshape(weight_int, (weight_int.size(0), weight_int.size(1), -1))  # merge ki, kj index to kij\n",
    "# w_int.weight.size() = torch.Size([64, 64, 9])\n",
    "                      \n",
    "padding = 1\n",
    "stride = 1\n",
    "# He hard coded these values :(\n",
    "#array_size = 64 # row and column number\n",
    "array_size = w_int.size(0) \n",
    "\n",
    "nig = range(a_int.size(1))  ## ni group [0,1,...31]\n",
    "njg = range(a_int.size(2))  ## nj group\n",
    " \n",
    "icg = range(int(w_int.size(1)))  ## input channel [0,...63]\n",
    "ocg = range(int(w_int.size(0)))  ## output channel\n",
    "\n",
    "\n",
    "kijg = range(w_int.size(2)) # [0, .. 8]\n",
    "ki_dim = int(math.sqrt(w_int.size(2)))  ## Kernel's 1 dim size\n",
    "\n",
    "######## Padding before Convolution #######\n",
    "a_pad = torch.zeros(len(icg), len(nig)+padding*2, len(njg)+padding*2).cuda()\n",
    "# a_pad.size() = [64, 32+2pad, 32+2pad]\n",
    "a_pad[ :, padding:padding+len(nig), padding:padding+len(njg)] = a_int.cuda()\n",
    "a_pad = torch.reshape(a_pad, (a_pad.size(0), -1))  ## mergin ni and nj index into nij\n",
    "# a_pad.size() = [64, (32+2pad)*(32+2pad)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d0e49160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8, 32, 32])\n",
      "torch.Size([8, 8, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(act_int.shape)\n",
    "print(weight_int.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "subsequent-oracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "\n",
    "p_nijg = range(a_pad.size(1)) ## paded activation's nij group [0, ...34*34-1]\n",
    "\n",
    "psum = torch.zeros( array_size, len(p_nijg), len(kijg)).cuda() \n",
    "\n",
    "for kij in kijg:       \n",
    "    for nij in p_nijg:     # time domain, sequentially given input\n",
    "        m = nn.Linear(array_size, array_size, bias=False)\n",
    "        m.weight = torch.nn.Parameter(w_int[:,:,kij])\n",
    "        psum[:, nij, kij] = m(a_pad[:,nij]).cuda()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "exposed-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "a_pad_ni_dim = int(math.sqrt(a_pad.size(1))) # 32 + 2*pad = 34\n",
    "\n",
    "o_ni_dim = int((a_pad_ni_dim - (ki_dim- 1) - 1)/stride + 1) #34 - 2 - 1 + 1 = 32\n",
    "o_nijg = range(o_ni_dim**2) # [0, 32*32-1]    \n",
    "    \n",
    "out = torch.zeros(len(ocg), len(o_nijg)).cuda()\n",
    "  \n",
    "   \n",
    "### SFP accumulation ###\n",
    "for o_nij in o_nijg: \n",
    "    for kij in kijg:  #[0, ... 8]\n",
    "        out[:,o_nij] = out[:,o_nij] + \\\n",
    "        psum[:, int(o_nij/o_ni_dim)*a_pad_ni_dim + o_nij%o_ni_dim + int(kij/ki_dim)*a_pad_ni_dim + kij%ki_dim, kij]\n",
    "                ## 2nd index = (int(o_nij/30)*32 + o_nij%30) + (int(kij/3)*32 + kij%3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "entitled-barbados",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0031, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_2D = torch.reshape(out, (out.size(0), o_ni_dim, -1)) # nij -> ni & nj\n",
    "difference = (out_2D - output_int[0,:,:,:])\n",
    "print(difference.abs().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9972d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[   0.,   -8.,    5.,  ...,   43.,   66.,   79.],\n",
       "         [  26.,   -2.,    6.,  ...,    4.,   -7.,  -17.],\n",
       "         [   2.,  -16.,  -13.,  ...,   -5.,  -17.,  -10.],\n",
       "         ...,\n",
       "         [ -34.,  -48.,  -39.,  ...,  -14.,  -32.,  -28.],\n",
       "         [ -23.,  -26.,  -21.,  ...,  -12.,   -2.,   46.],\n",
       "         [  13.,   25.,   37.,  ...,   71.,   41.,  -15.]],\n",
       "\n",
       "        [[  20.,  -15.,    1.,  ...,    2.,    8.,   36.],\n",
       "         [   4.,  -68.,  -54.,  ...,  -30.,  -25.,   23.],\n",
       "         [  42.,  -21.,  -32.,  ...,  -16.,  -16.,   24.],\n",
       "         ...,\n",
       "         [  60.,  198.,  155.,  ...,  207.,  228.,  100.],\n",
       "         [  80.,  214.,  197.,  ...,  237.,  234.,  147.],\n",
       "         [  17.,   81.,   78.,  ...,  105.,   89.,   70.]],\n",
       "\n",
       "        [[ -26.,   17.,  -16.,  ...,  -10.,   -4.,  142.],\n",
       "         [ -52.,   -1.,  -11.,  ...,  -25.,  -44.,  166.],\n",
       "         [ -31.,    5.,   -4.,  ...,  -14.,  -55.,  165.],\n",
       "         ...,\n",
       "         [ -99.,  -44.,   38.,  ..., -119.,    2.,  165.],\n",
       "         [ -47.,  -99.,   27.,  ...,  -27., -141.,  219.],\n",
       "         [ -27.,  -50.,  -32.,  ...,   26., -123.,  115.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ -20.,  -12.,  -25.,  ...,    1.,   55.,  -93.],\n",
       "         [  -3.,  -24.,   -5.,  ...,   -5.,   30., -133.],\n",
       "         [ -10.,  -14.,  -32.,  ...,   -9.,   21., -124.],\n",
       "         ...,\n",
       "         [  27.,   17.,   -9.,  ...,   38.,  -47.,  -95.],\n",
       "         [  31.,    9.,   -2.,  ...,  -16.,   21., -142.],\n",
       "         [  28.,   21.,  -15.,  ...,  -58.,  -19., -137.]],\n",
       "\n",
       "        [[  19.,   14.,   12.,  ...,   26.,   38.,   -1.],\n",
       "         [  57.,   64.,   61.,  ...,   87.,  100.,   16.],\n",
       "         [  46.,   66.,   59.,  ...,   76.,  100.,   18.],\n",
       "         ...,\n",
       "         [  14.,    1.,   -4.,  ...,   31.,   14.,  -33.],\n",
       "         [  -4.,   -5.,  -12.,  ...,    2.,   28.,  -38.],\n",
       "         [   3.,   10.,   27.,  ...,   11.,   36.,  -21.]],\n",
       "\n",
       "        [[  36.,  109.,   50.,  ...,   49.,   14.,  -15.],\n",
       "         [  84.,  198.,  119.,  ...,  111.,   77.,   -6.],\n",
       "         [  69.,  185.,  119.,  ...,   99.,   84.,    9.],\n",
       "         ...,\n",
       "         [-104., -124.,  -47.,  ...,   -1.,  -40.,   77.],\n",
       "         [ -98., -104.,  -79.,  ...,  -37.,  -16.,   22.],\n",
       "         [ -77.,  -72.,  -90.,  ...,  -87.,  -16.,  -57.]]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_int[0,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Easier 2D version ########\n",
    "\n",
    "import math\n",
    "\n",
    "kig = range(int(math.sqrt(len(kijg))))\n",
    "kjg = range(int(math.sqrt(len(kijg))))\n",
    "    \n",
    "o_nig = range(int((math.sqrt(len(nijg))+2*padding -(math.sqrt(len(kijg))- 1) - 1)/stride + 1))\n",
    "o_njg = range(int((math.sqrt(len(nijg))+2*padding -(math.sqrt(len(kijg)) - 1) - 1)/stride + 1))\n",
    "    \n",
    "    \n",
    "out = torch.zeros(len(ocg), len(o_nig), len(o_njg)).cuda()\n",
    "  \n",
    "   \n",
    "### SFP accumulation ###\n",
    "for ni in o_nig:\n",
    "    for nj in o_njg:\n",
    "        for ki in kig:\n",
    "            for kj in kjg:\n",
    "                for ic_tile in ic_tileg:    \n",
    "                    for oc_tile in oc_tileg:   \n",
    "                        out[oc_tile*array_size:(oc_tile+1)*array_size, ni, nj] = out[oc_tile*array_size:(oc_tile+1)*array_size, ni, nj] + \\\n",
    "                        psum[ic_tile, oc_tile, :, int(math.sqrt(len(nijg)))*(ni+ki) + (nj+kj), len(kig)*ki+kj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cf7fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-reach",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-sheffield",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
