{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG_quant(\n",
      "  (features): Sequential(\n",
      "    (0): QuantConv2d(\n",
      "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): QuantConv2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): QuantConv2d(\n",
      "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): QuantConv2d(\n",
      "      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): QuantConv2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantConv2d(\n",
      "      256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (25): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): QuantConv2d(\n",
      "      8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (28): ReLU(inplace=True)\n",
      "    (29): QuantConv2d(\n",
      "      8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (30): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (33): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): ReLU(inplace=True)\n",
      "    (39): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (41): ReLU(inplace=True)\n",
      "    (42): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (43): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from tensorboardX import SummaryWriter      \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# print('=> Building model...')\n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"VGG16_quant\"\n",
    "model = VGG16_quant()\n",
    "print(model)\n",
    "# device = torch.device(\"cuda\") \n",
    "# model = VGG16_quant().to(device)\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(f\"=> Using {torch.cuda.device_count()} GPUs\")\n",
    "#     model = nn.DataParallel(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [150, 225]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 9295/10000 (93%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/VGG16_quant/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceramic-nigeria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 -th layer prehooked\n",
      "7 -th layer prehooked\n",
      "12 -th layer prehooked\n",
      "16 -th layer prehooked\n",
      "21 -th layer prehooked\n",
      "25 -th layer prehooked\n",
      "29 -th layer prehooked\n",
      "34 -th layer prehooked\n",
      "38 -th layer prehooked\n",
      "41 -th layer prehooked\n",
      "46 -th layer prehooked\n",
      "50 -th layer prehooked\n",
      "54 -th layer prehooked\n"
     ]
    }
   ],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "i = 0\n",
    "\n",
    "for layer in model.modules():\n",
    "    # print(layer)\n",
    "    i = i+1\n",
    "    if isinstance(layer, QuantConv2d):\n",
    "        # print(layer)\n",
    "        print(i,\"-th layer prehooked\")\n",
    "        layer.register_forward_pre_hook(save_output)             \n",
    "####################################################\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "out = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "251e9b4a-1183-4bd2-b621-231b0417009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_q = model.features[27].weight_q\n",
    "w_alpha = model.features[27].weight_quant.wgt_alpha\n",
    "\n",
    "# print(weight_q)\n",
    "w_bit = 4\n",
    "\n",
    "weight_int = weight_q / (w_alpha / (2**(w_bit-1)-1))\n",
    "# print(weight_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84f667f0-b48a-47ad-8c09-f774e4b81c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 8, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "k = 8  # input to your 8→8 conv layer\n",
    "\n",
    "act = save_output.outputs[8][0]      # [128, 8, 4, 4]\n",
    "act_alpha = model.features[27].act_alpha\n",
    "act_bit = 4\n",
    "act_quant_fn = act_quantization(act_bit)\n",
    "\n",
    "# quantize + convert to integer\n",
    "act_q   = act_quant_fn(act, act_alpha)\n",
    "act_int = act_q / (act_alpha / (2**act_bit - 1))\n",
    "\n",
    "# sanity check\n",
    "print(act_int.shape)  # should be [128, 8, 4, 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1bd26d5-0f61-4d65-a06e-366c78c733ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_int = torch.nn.Conv2d(in_channels = 8, out_channels=8, kernel_size = 3, padding=1)\n",
    "conv_int.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "conv_int.bias = model.features[27].bias\n",
    "output_int = conv_int(act_int)\n",
    "output_recovered = output_int * (act_alpha / (2**act_bit-1)) * (w_alpha / (2**(w_bit-1)-1))\n",
    "# print(output_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ab3a8b-3a6c-4cce-8995-e8ea6ca8aa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.6042e-08, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device = act.device  # should be 'cuda:0'\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv_ref = model.features[27]   # already on cuda with the model\n",
    "    output_ref = conv_ref(act)      # act is on same device\n",
    "\n",
    "print((output_ref - output_recovered).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "subsequent-oracle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_int shape: torch.Size([8, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "# act_int.size = torch.Size([128, 64, 32, 32])  <- batch_size, input_ch, ni, nj\n",
    "a_int = act_int[0,:,:,:]  # pick only one input out of batch\n",
    "\n",
    "w_int = torch.reshape(weight_int, (weight_int.size(0), weight_int.size(1), -1))  # merge ki, kj index to kij\n",
    " # weight_int: [out_ch, in_ch, ki, kj]  ->  [out_ch, in_ch, kij]\n",
    "print(\"w_int shape:\", w_int.shape)   # should be [8, 8, 9]\n",
    "# print(a_int.shape)\n",
    "padding = 1\n",
    "stride = 1\n",
    "array_size = 8 # row and column number\n",
    "\n",
    "nig = range(a_int.size(1))  ## ni group\n",
    "njg = range(a_int.size(2))  ## nj group\n",
    "\n",
    "icg = range(int(w_int.size(1)))  ## input channel \n",
    "ocg = range(int(w_int.size(0)))  ## output channel\n",
    "\n",
    "ic_tileg = range(int(len(icg)/array_size))\n",
    "oc_tileg = range(int(len(ocg)/array_size))\n",
    "\n",
    "kijg = range(w_int.size(2))\n",
    "ki_dim = int(math.sqrt(w_int.size(2)))  ## Kernel's 1 dim size\n",
    "\n",
    "######## Padding before Convolution #######\n",
    "a_pad = torch.zeros(len(icg), len(nig)+padding*2, len(nig)+padding*2).cuda()\n",
    "# a_pad.size() = [64, 32+2pad, 32+2pad]\n",
    "a_pad[ :, padding:padding+len(nig), padding:padding+len(njg)] = a_int.cuda()\n",
    "a_pad = torch.reshape(a_pad, (a_pad.size(0), -1))\n",
    "# a_pad.size() = [64, (32+2pad)*(32+2pad)]\n",
    "\n",
    "\n",
    "a_tile = torch.zeros(len(ic_tileg), array_size,    a_pad.size(1)).cuda() \n",
    "w_tile = torch.zeros(len(oc_tileg)*len(ic_tileg), array_size, array_size, len(kijg)).cuda() \n",
    "\n",
    "for ic_tile in ic_tileg:\n",
    "    a_tile[ic_tile,:,:] = a_pad[ic_tile*array_size:(ic_tile+1)*array_size,:]\n",
    "\n",
    "for ic_tile in ic_tileg:\n",
    "    for oc_tile in oc_tileg:\n",
    "        w_tile[oc_tile*len(oc_tileg) + ic_tile,:,:,:] = w_int[oc_tile*array_size:(oc_tile+1)*array_size, ic_tile*array_size:(ic_tile+1)*array_size, :]\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "p_nijg = range(a_pad.size(1)) ## psum nij group\n",
    "\n",
    "psum = torch.zeros(len(ic_tileg), len(oc_tileg), array_size, len(p_nijg), len(kijg)).cuda() \n",
    "\n",
    "for kij in kijg:\n",
    "    for ic_tile in ic_tileg:       # Tiling into array_sizeXarray_size array\n",
    "        for oc_tile in oc_tileg:   # Tiling into array_sizeXarray_size array        \n",
    "            for nij in p_nijg:       # time domain, sequentially given input\n",
    "                    m = nn.Linear(array_size, array_size, bias=False)\n",
    "                    #m.weight = torch.nn.Parameter(w_int[oc_tile*array_size:(oc_tile+1)*array_size, ic_tile*array_size:(ic_tile+1)*array_size, kij])\n",
    "                    m.weight = torch.nn.Parameter(w_tile[len(oc_tileg)*oc_tile+ic_tile,:,:,kij])\n",
    "                    psum[ic_tile, oc_tile, :, nij, kij] = m(a_tile[ic_tile,:,nij]).cuda()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "skilled-projector",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_core shape   : torch.Size([8, 4, 4])\n",
      "a_padded shape : torch.Size([8, 6, 6])\n",
      "Wrote 36 nij (6x6) activations to activation.txt\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "activation.txt code\n",
    "'''\n",
    "############################################################\n",
    "# 6x6 activation.txt for HW TB (36 nij, 8ch, 4 bits each)\n",
    "# - assumes act_int is already defined:\n",
    "#   act_int.shape == [batch, 8, 4, 4]\n",
    "#   (from layer 27 prehook in proj_kij notebook)\n",
    "############################################################\n",
    "\n",
    "act_bits = 4\n",
    "\n",
    "# take one sample from the batch\n",
    "a_core = act_int[0]        # [8, 4, 4]\n",
    "C, H, W = a_core.shape     # C=8, H=W=4\n",
    "\n",
    "# target spatial size for HW tile\n",
    "Hpad, Wpad = 6, 6\n",
    "\n",
    "# zero-pad to 6x6, with 4x4 centered\n",
    "a_padded = torch.zeros(C, Hpad, Wpad, dtype=a_core.dtype, device=a_core.device)\n",
    "a_padded[:, 1:1+H, 1:1+W] = a_core  # put 4x4 in the middle\n",
    "\n",
    "print(\"a_core shape   :\", a_core.shape)    # [8, 4, 4]\n",
    "print(\"a_padded shape :\", a_padded.shape)  # [8, 6, 6] ← 36 nij\n",
    "\n",
    "# ---------- write activation.txt ----------\n",
    "with open(\"activation.txt\", \"w\") as f:\n",
    "    # ni, nj now run over 6x6 = 36 nij\n",
    "    for i in range(Hpad):        # rows 0..5\n",
    "        for j in range(Wpad):    # cols 0..5\n",
    "            vals = []\n",
    "            for ch in range(C):  # 8 channels\n",
    "                v = int(round(a_padded[ch, i, j].item()))\n",
    "                # wrap negatives into 4-bit unsigned two’s-complement style\n",
    "                if v < 0:\n",
    "                    v += (1 << act_bits)\n",
    "                vals.append(v & ((1 << act_bits) - 1))\n",
    "\n",
    "            # Pack ch0..ch7 into 32 bits (MSB = ch0’s MSB)\n",
    "            word = 0\n",
    "            for v in vals:\n",
    "                word = (word << act_bits) | v\n",
    "            bits32 = f\"{word:032b}\"\n",
    "            f.write(bits32 + \"\\n\")\n",
    "\n",
    "print(\"Wrote 36 nij (6x6) activations to activation.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d185e29e-ea25-4493-9bd5-7bc68fd8bbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tile_out_int shape: torch.Size([8, 4, 4])\n",
      "Wrote ReLU outputs to output_relu_tile.txt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ===== Choose layer-27 tile =====\n",
    "# act_int shape at layer 27: [batch, 8, 4, 4]\n",
    "# weight_int shape: [8, 8, 3, 3]\n",
    "a_full = act_int[0, :8, :, :]        # [8, 4, 4]\n",
    "w_full = weight_int[:8, :8, :, :]    # [8, 8, 3, 3]\n",
    "\n",
    "# Use the whole 4x4 as the tile\n",
    "a_tile = a_full                      # [8, 4, 4]\n",
    "\n",
    "# Conv wants [N, C, H, W]\n",
    "a_tile_batch = a_tile.unsqueeze(0).float()   # [1, 8, 4, 4]\n",
    "w_full_f     = w_full.float()               # [8, 8, 3, 3]\n",
    "\n",
    "# ===== Conv: padding=1, stride=1 → 4x4 output =====\n",
    "tile_out = F.conv2d(\n",
    "    a_tile_batch,\n",
    "    w_full_f,\n",
    "    bias=None,\n",
    "    stride=1,\n",
    "    padding=1,    # keep spatial size 4x4\n",
    ")   # [1, 8, 4, 4]\n",
    "\n",
    "# ===== ReLU =====\n",
    "tile_out_relu = torch.clamp(tile_out, min=0.0)   # [1, 8, 4, 4]\n",
    "\n",
    "# ===== Convert to 16-bit integer psums =====\n",
    "tile_out_int = torch.round(tile_out_relu).to(torch.int16)  # [1, 8, 4, 4]\n",
    "tile_out_int = tile_out_int[0]                              # [8, 4, 4]\n",
    "\n",
    "print(\"tile_out_int shape:\", tile_out_int.shape)  # -> [8, 4, 4]\n",
    "\n",
    "# ===== Write to txt: each line = 128 bits (8 ch x 16 bits) =====\n",
    "out_path = \"output_relu_tile.txt\"\n",
    "\n",
    "with open(out_path, \"w\") as f:\n",
    "    for h in range(tile_out_int.size(1)):     # 4 rows\n",
    "        for w in range(tile_out_int.size(2)): # 4 cols\n",
    "            vals = tile_out_int[:, h, w]      # [8]\n",
    "\n",
    "            bits_per_pixel = []\n",
    "            for v in vals:\n",
    "                v16   = int(v.item()) & 0xFFFF\n",
    "                bits16 = format(v16, \"016b\")  # 16-bit binary, MSB first\n",
    "                bits_per_pixel.append(bits16)\n",
    "\n",
    "            line = \"\".join(bits_per_pixel)    # 8 * 16 = 128 bits\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "print(\"Wrote ReLU outputs to\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "accomplished-folks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_kij = 9\n",
      "Saved weight_kij/weight_k0.txt\n",
      "Saved weight_kij/weight_k1.txt\n",
      "Saved weight_kij/weight_k2.txt\n",
      "Saved weight_kij/weight_k3.txt\n",
      "Saved weight_kij/weight_k4.txt\n",
      "Saved weight_kij/weight_k5.txt\n",
      "Saved weight_kij/weight_k6.txt\n",
      "Saved weight_kij/weight_k7.txt\n",
      "Saved weight_kij/weight_k8.txt\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Generate weight_kij files for layer 27\n",
    "weight_k0.txt ... weight_k8.txt\n",
    "Each file: 8 rows (oc0..oc7), each row = 8 weights (4-bit two's complement) packed into 32 bits\n",
    "'''\n",
    "\n",
    "import os\n",
    "\n",
    "w_bit = 4\n",
    "\n",
    "# weight_int for layer 27 should be [8, 8, 3, 3]\n",
    "# if it's larger, slice first 8x8\n",
    "w27_int = weight_int[:8, :8, :, :]          # [8, 8, 3, 3]\n",
    "w_int   = w27_int.view(8, 8, -1)            # [8, 8, 9]  -> merge ki,kj into kij\n",
    "\n",
    "os.makedirs(\"weight_kij\", exist_ok=True)\n",
    "\n",
    "num_kij = w_int.size(2)   # 9\n",
    "print(\"num_kij =\", num_kij)\n",
    "\n",
    "for kij in range(num_kij):\n",
    "    fname = f\"weight_kij/weight_k{kij}.txt\"\n",
    "    with open(fname, \"w\") as f:\n",
    "        # For each output channel (column)\n",
    "        for oc in range(w_int.size(0)):   # 0..7\n",
    "            vals = []\n",
    "\n",
    "            # 8 input channels -> 8 x 4 bits\n",
    "            # iC7..iC0 -> MSB..LSB\n",
    "            for r in range(w_int.size(1)):    # 0..7\n",
    "                ic = w_int.size(1) - 1 - r    # reverse so ic7 is written first\n",
    "                v = int(round(w_int[oc, ic, kij].item()))\n",
    "                if v < 0:\n",
    "                    v += (1 << w_bit)\n",
    "                v &= (1 << w_bit) - 1         # keep 4 bits\n",
    "                vals.append(v)\n",
    "\n",
    "            # Pack into 32-bit word\n",
    "            word = 0\n",
    "            for v in vals:                    # vals[0] = ic7 ... vals[7] = ic0\n",
    "                word = (word << w_bit) | v\n",
    "\n",
    "            bits32 = f\"{word:032b}\"\n",
    "            f.write(bits32 + \"\\n\")\n",
    "\n",
    "    print(f\"Saved {fname}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da8ffaa3-0649-4e72-9e9a-3bd29dd45cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_int shape: torch.Size([1, 8, 4, 4])\n",
      "w_int shape: torch.Size([8, 8, 3, 3])\n",
      "psum_recovered shape: torch.Size([1, 8, 4, 4])\n",
      "max |psum_recovered - conv27+relu28|: 4.76837158203125e-07\n",
      "max |psum_recovered - act29_in|    : 1.6319186687469482\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ---------- 1. Quantization parameters ----------\n",
    "w_bit   = 4\n",
    "act_bit = 4\n",
    "\n",
    "# weights for layer 27\n",
    "weight_q  = model.features[27].weight_q.detach()              # [8, 8, 3, 3]\n",
    "w_alpha   = model.features[27].weight_quant.wgt_alpha.detach()\n",
    "w_delta   = w_alpha / (2**(w_bit-1) - 1)                      # scalar\n",
    "w_int     = weight_q / w_delta                                # [8, 8, 3, 3]\n",
    "\n",
    "# activations for layer 27 (input to layer 27)\n",
    "act_alpha    = model.features[27].act_alpha.detach()\n",
    "act_quant_fn = act_quantization(act_bit)\n",
    "\n",
    "# x27: float input to layer 27, shape [1, 8, 4, 4]\n",
    "# assumes `act_int` was computed earlier from the same hook:\n",
    "x27 = act_int[0].unsqueeze(0)                                 # [1, 8, 4, 4]\n",
    "\n",
    "x_q    = act_quant_fn(x27, act_alpha)                         # [1, 8, 4, 4]\n",
    "a_delta = act_alpha / (2**act_bit - 1)\n",
    "a_int   = x_q / a_delta                                       # [1, 8, 4, 4]\n",
    "\n",
    "print(\"a_int shape:\", a_int.shape)\n",
    "print(\"w_int shape:\", w_int.shape)\n",
    "\n",
    "# ---------- 2. Conv2d with integer weights ----------\n",
    "conv2 = nn.Conv2d(\n",
    "    in_channels = 8,\n",
    "    out_channels = 8,\n",
    "    kernel_size = 3,\n",
    "    padding = 1,\n",
    "    bias = False,\n",
    ").to(device)\n",
    "\n",
    "conv2.weight = nn.Parameter(w_int.float().to(device))\n",
    "\n",
    "# integer psum (computed as float), then ReLU\n",
    "with torch.no_grad():\n",
    "    psum_int = conv2(a_int.float().to(device))                # [1, 8, 4, 4]\n",
    "    psum_int = torch.clamp(psum_int, min=0)\n",
    "    psum_recovered = psum_int * w_delta * a_delta             # [1, 8, 4, 4]\n",
    "\n",
    "print(\"psum_recovered shape:\", psum_recovered.shape)\n",
    "\n",
    "# ---------- 3. True reference: Conv27 + ReLU28 ----------\n",
    "with torch.no_grad():\n",
    "    y = model.features[27](x27.to(device))                    # QuantConv2d 8->8\n",
    "    y = model.features[28](y)                                 # ReLU\n",
    "    y_ref = y[0]                                              # [8, 4, 4]\n",
    "\n",
    "# ---------- 3b. Get act29_in from saved prehook outputs ----------\n",
    "# save_output.outputs[k] stores the input to each QuantConv2d (in order).\n",
    "# Earlier you used k = 8 for layer 27, so layer 29 is the next conv: k = 9.\n",
    "act29_batch = save_output.outputs[9][0]              # [batch, 8, 4, 4]\n",
    "act29_in    = act29_batch[0]                                  # [8, 4, 4]\n",
    "\n",
    "# ---------- 4. Compare errors ----------\n",
    "err_ref  = (psum_recovered[0] - y_ref.to(device)).abs().max()\n",
    "err_next = (psum_recovered[0] - act29_in.to(device)).abs().max()\n",
    "\n",
    "print(\"max |psum_recovered - conv27+relu28|:\", err_ref.item())\n",
    "print(\"max |psum_recovered - act29_in|    :\", err_next.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec03092-fb86-48a5-a85a-353720f38077",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
