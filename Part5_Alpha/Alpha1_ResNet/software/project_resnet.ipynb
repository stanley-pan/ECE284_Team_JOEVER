{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "ResNet_Cifar_Single8(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          32, 8, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        8, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "     \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"project_resnet\"\n",
    "model = resnet20_quant_alpha()\n",
    "\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [150, 225]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "junior-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 0.316 (0.316)\tData 0.284 (0.284)\tLoss 0.4332 (0.4332)\tPrec 89.844% (89.844%)\n",
      "Epoch: [0][100/391]\tTime 0.023 (0.026)\tData 0.001 (0.004)\tLoss 0.4513 (0.3701)\tPrec 85.156% (87.191%)\n",
      "Epoch: [0][200/391]\tTime 0.023 (0.024)\tData 0.001 (0.003)\tLoss 0.4260 (0.3684)\tPrec 83.594% (87.084%)\n",
      "Epoch: [0][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.002)\tLoss 0.3217 (0.3647)\tPrec 89.062% (87.220%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.293 (0.293)\tLoss 0.4866 (0.4866)\tPrec 85.156% (85.156%)\n",
      " * Prec 83.490% \n",
      "best acc: 83.490000\n",
      "Epoch 0 — LR: [5e-05]\n",
      "Epoch: [1][0/391]\tTime 0.367 (0.367)\tData 0.337 (0.337)\tLoss 0.4220 (0.4220)\tPrec 84.375% (84.375%)\n",
      "Epoch: [1][100/391]\tTime 0.024 (0.027)\tData 0.001 (0.005)\tLoss 0.3256 (0.3372)\tPrec 89.844% (87.925%)\n",
      "Epoch: [1][200/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.1669 (0.3293)\tPrec 96.094% (88.285%)\n",
      "Epoch: [1][300/391]\tTime 0.022 (0.025)\tData 0.001 (0.003)\tLoss 0.3127 (0.3196)\tPrec 87.500% (88.650%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.290 (0.290)\tLoss 0.3689 (0.3689)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.920% \n",
      "best acc: 85.920000\n",
      "Epoch 1 — LR: [0.0001]\n",
      "Epoch: [2][0/391]\tTime 0.368 (0.368)\tData 0.338 (0.338)\tLoss 0.2922 (0.2922)\tPrec 90.625% (90.625%)\n",
      "Epoch: [2][100/391]\tTime 0.023 (0.027)\tData 0.001 (0.005)\tLoss 0.2555 (0.3033)\tPrec 90.625% (89.527%)\n",
      "Epoch: [2][200/391]\tTime 0.022 (0.025)\tData 0.001 (0.003)\tLoss 0.2827 (0.3012)\tPrec 89.844% (89.525%)\n",
      "Epoch: [2][300/391]\tTime 0.022 (0.024)\tData 0.001 (0.002)\tLoss 0.3265 (0.2946)\tPrec 87.500% (89.709%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.211 (0.211)\tLoss 0.3700 (0.3700)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.800% \n",
      "best acc: 85.920000\n",
      "Epoch 2 — LR: [0.00015]\n",
      "Epoch: [3][0/391]\tTime 0.385 (0.385)\tData 0.354 (0.354)\tLoss 0.3501 (0.3501)\tPrec 87.500% (87.500%)\n",
      "Epoch: [3][100/391]\tTime 0.022 (0.026)\tData 0.001 (0.005)\tLoss 0.3127 (0.2950)\tPrec 88.281% (89.612%)\n",
      "Epoch: [3][200/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.1218 (0.2883)\tPrec 96.875% (89.921%)\n",
      "Epoch: [3][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.003)\tLoss 0.3114 (0.2885)\tPrec 92.188% (89.979%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.139 (0.139)\tLoss 0.4207 (0.4207)\tPrec 85.156% (85.156%)\n",
      " * Prec 85.650% \n",
      "best acc: 85.920000\n",
      "Epoch 3 — LR: [0.0002]\n",
      "Epoch: [4][0/391]\tTime 0.418 (0.418)\tData 0.388 (0.388)\tLoss 0.3599 (0.3599)\tPrec 88.281% (88.281%)\n",
      "Epoch: [4][100/391]\tTime 0.022 (0.027)\tData 0.001 (0.005)\tLoss 0.3852 (0.2724)\tPrec 87.500% (90.478%)\n",
      "Epoch: [4][200/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.4003 (0.2853)\tPrec 88.281% (90.034%)\n",
      "Epoch: [4][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.003)\tLoss 0.3496 (0.2866)\tPrec 85.938% (90.070%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.188 (0.188)\tLoss 0.3863 (0.3863)\tPrec 83.594% (83.594%)\n",
      " * Prec 85.910% \n",
      "best acc: 85.920000\n",
      "Epoch 4 — LR: [0.00025]\n",
      "Epoch: [5][0/391]\tTime 0.431 (0.431)\tData 0.402 (0.402)\tLoss 0.2524 (0.2524)\tPrec 90.625% (90.625%)\n",
      "Epoch: [5][100/391]\tTime 0.022 (0.027)\tData 0.001 (0.005)\tLoss 0.3100 (0.2817)\tPrec 91.406% (90.114%)\n",
      "Epoch: [5][200/391]\tTime 0.022 (0.025)\tData 0.001 (0.003)\tLoss 0.3337 (0.2874)\tPrec 86.719% (89.871%)\n",
      "Epoch: [5][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.003)\tLoss 0.2010 (0.2860)\tPrec 92.969% (89.942%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.116 (0.116)\tLoss 0.4042 (0.4042)\tPrec 85.938% (85.938%)\n",
      " * Prec 85.940% \n",
      "best acc: 85.940000\n",
      "Epoch 5 — LR: [0.0003]\n",
      "Epoch: [6][0/391]\tTime 0.347 (0.347)\tData 0.317 (0.317)\tLoss 0.1946 (0.1946)\tPrec 96.094% (96.094%)\n",
      "Epoch: [6][100/391]\tTime 0.027 (0.027)\tData 0.002 (0.005)\tLoss 0.1869 (0.2882)\tPrec 92.969% (89.720%)\n",
      "Epoch: [6][200/391]\tTime 0.025 (0.025)\tData 0.004 (0.003)\tLoss 0.2554 (0.2840)\tPrec 91.406% (89.914%)\n",
      "Epoch: [6][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.003)\tLoss 0.2270 (0.2865)\tPrec 90.625% (89.779%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.218 (0.218)\tLoss 0.4073 (0.4073)\tPrec 86.719% (86.719%)\n",
      " * Prec 86.270% \n",
      "best acc: 86.270000\n",
      "Epoch 6 — LR: [0.00035]\n",
      "Epoch: [7][0/391]\tTime 0.363 (0.363)\tData 0.333 (0.333)\tLoss 0.3063 (0.3063)\tPrec 90.625% (90.625%)\n",
      "Epoch: [7][100/391]\tTime 0.023 (0.026)\tData 0.001 (0.005)\tLoss 0.3284 (0.2839)\tPrec 88.281% (90.122%)\n",
      "Epoch: [7][200/391]\tTime 0.030 (0.025)\tData 0.007 (0.003)\tLoss 0.3508 (0.2859)\tPrec 88.281% (89.991%)\n",
      "Epoch: [7][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.003)\tLoss 0.2767 (0.2818)\tPrec 89.062% (90.186%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.200 (0.200)\tLoss 0.5136 (0.5136)\tPrec 85.156% (85.156%)\n",
      " * Prec 85.730% \n",
      "best acc: 86.270000\n",
      "Epoch 7 — LR: [0.0004]\n",
      "Epoch: [8][0/391]\tTime 0.328 (0.328)\tData 0.296 (0.296)\tLoss 0.3213 (0.3213)\tPrec 87.500% (87.500%)\n",
      "Epoch: [8][100/391]\tTime 0.026 (0.026)\tData 0.001 (0.004)\tLoss 0.2153 (0.2943)\tPrec 94.531% (89.735%)\n",
      "Epoch: [8][200/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.3309 (0.2875)\tPrec 89.062% (89.984%)\n",
      "Epoch: [8][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.002)\tLoss 0.2216 (0.2857)\tPrec 92.188% (90.114%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 0.4359 (0.4359)\tPrec 84.375% (84.375%)\n",
      " * Prec 85.580% \n",
      "best acc: 86.270000\n",
      "Epoch 8 — LR: [0.00045000000000000004]\n",
      "Epoch: [9][0/391]\tTime 0.373 (0.373)\tData 0.343 (0.343)\tLoss 0.2434 (0.2434)\tPrec 89.844% (89.844%)\n",
      "Epoch: [9][100/391]\tTime 0.027 (0.027)\tData 0.001 (0.005)\tLoss 0.2391 (0.2873)\tPrec 90.625% (89.944%)\n",
      "Epoch: [9][200/391]\tTime 0.022 (0.025)\tData 0.001 (0.003)\tLoss 0.1562 (0.2853)\tPrec 95.312% (90.038%)\n",
      "Epoch: [9][300/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.2953 (0.2878)\tPrec 91.406% (89.966%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.188 (0.188)\tLoss 0.3880 (0.3880)\tPrec 89.062% (89.062%)\n",
      " * Prec 84.900% \n",
      "best acc: 86.270000\n",
      "Epoch 9 — LR: [0.0005]\n",
      "Epoch: [10][0/391]\tTime 0.383 (0.383)\tData 0.354 (0.354)\tLoss 0.3596 (0.3596)\tPrec 91.406% (91.406%)\n",
      "Epoch: [10][100/391]\tTime 0.026 (0.026)\tData 0.001 (0.005)\tLoss 0.2739 (0.2827)\tPrec 90.625% (89.898%)\n",
      "Epoch: [10][200/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.2924 (0.2829)\tPrec 89.062% (89.968%)\n",
      "Epoch: [10][300/391]\tTime 0.023 (0.024)\tData 0.002 (0.003)\tLoss 0.2888 (0.2868)\tPrec 91.406% (89.903%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.248 (0.248)\tLoss 0.3790 (0.3790)\tPrec 89.062% (89.062%)\n",
      " * Prec 85.030% \n",
      "best acc: 86.270000\n",
      "Epoch 10 — LR: [0.0005]\n",
      "Epoch: [11][0/391]\tTime 0.355 (0.355)\tData 0.324 (0.324)\tLoss 0.2921 (0.2921)\tPrec 89.844% (89.844%)\n",
      "Epoch: [11][100/391]\tTime 0.028 (0.027)\tData 0.001 (0.005)\tLoss 0.2012 (0.2930)\tPrec 92.188% (89.426%)\n",
      "Epoch: [11][200/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.3054 (0.2924)\tPrec 89.062% (89.529%)\n",
      "Epoch: [11][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.003)\tLoss 0.2580 (0.2888)\tPrec 93.750% (89.727%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 0.3949 (0.3949)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.590% \n",
      "best acc: 86.270000\n",
      "Epoch 11 — LR: [0.0005]\n",
      "Epoch: [12][0/391]\tTime 0.394 (0.394)\tData 0.357 (0.357)\tLoss 0.2894 (0.2894)\tPrec 85.938% (85.938%)\n",
      "Epoch: [12][100/391]\tTime 0.022 (0.027)\tData 0.001 (0.005)\tLoss 0.2513 (0.2820)\tPrec 90.625% (90.068%)\n",
      "Epoch: [12][200/391]\tTime 0.022 (0.025)\tData 0.001 (0.003)\tLoss 0.2169 (0.2823)\tPrec 92.188% (90.015%)\n",
      "Epoch: [12][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.003)\tLoss 0.3972 (0.2863)\tPrec 88.281% (90.064%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.204 (0.204)\tLoss 0.4059 (0.4059)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.280% \n",
      "best acc: 86.270000\n",
      "Epoch 12 — LR: [0.0005]\n",
      "Epoch: [13][0/391]\tTime 0.402 (0.402)\tData 0.372 (0.372)\tLoss 0.3310 (0.3310)\tPrec 86.719% (86.719%)\n",
      "Epoch: [13][100/391]\tTime 0.024 (0.027)\tData 0.001 (0.005)\tLoss 0.2952 (0.2896)\tPrec 87.500% (89.619%)\n",
      "Epoch: [13][200/391]\tTime 0.022 (0.026)\tData 0.001 (0.003)\tLoss 0.4387 (0.2914)\tPrec 85.938% (89.552%)\n",
      "Epoch: [13][300/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.3451 (0.2907)\tPrec 88.281% (89.641%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.209 (0.209)\tLoss 0.4379 (0.4379)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.050% \n",
      "best acc: 86.270000\n",
      "Epoch 13 — LR: [0.0005]\n",
      "Epoch: [14][0/391]\tTime 0.385 (0.385)\tData 0.356 (0.356)\tLoss 0.3157 (0.3157)\tPrec 89.844% (89.844%)\n",
      "Epoch: [14][100/391]\tTime 0.023 (0.026)\tData 0.001 (0.005)\tLoss 0.3335 (0.2889)\tPrec 88.281% (89.821%)\n",
      "Epoch: [14][200/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.2861 (0.2889)\tPrec 89.062% (89.859%)\n",
      "Epoch: [14][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.003)\tLoss 0.3509 (0.2899)\tPrec 85.156% (89.781%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.179 (0.179)\tLoss 0.4744 (0.4744)\tPrec 83.594% (83.594%)\n",
      " * Prec 84.650% \n",
      "best acc: 86.270000\n",
      "Epoch 14 — LR: [0.0005]\n",
      "Epoch: [15][0/391]\tTime 0.398 (0.398)\tData 0.368 (0.368)\tLoss 0.3102 (0.3102)\tPrec 90.625% (90.625%)\n",
      "Epoch: [15][100/391]\tTime 0.023 (0.027)\tData 0.001 (0.005)\tLoss 0.2984 (0.2806)\tPrec 90.625% (90.130%)\n",
      "Epoch: [15][200/391]\tTime 0.022 (0.025)\tData 0.001 (0.003)\tLoss 0.3800 (0.2871)\tPrec 85.938% (89.817%)\n",
      "Epoch: [15][300/391]\tTime 0.022 (0.024)\tData 0.001 (0.003)\tLoss 0.3187 (0.2855)\tPrec 89.844% (89.906%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.205 (0.205)\tLoss 0.3818 (0.3818)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.120% \n",
      "best acc: 86.270000\n",
      "Epoch 15 — LR: [0.0005]\n",
      "Epoch: [16][0/391]\tTime 0.358 (0.358)\tData 0.328 (0.328)\tLoss 0.2919 (0.2919)\tPrec 90.625% (90.625%)\n",
      "Epoch: [16][100/391]\tTime 0.023 (0.027)\tData 0.001 (0.005)\tLoss 0.3329 (0.2857)\tPrec 86.719% (90.331%)\n",
      "Epoch: [16][200/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.1923 (0.2833)\tPrec 92.969% (90.256%)\n",
      "Epoch: [16][300/391]\tTime 0.025 (0.024)\tData 0.001 (0.003)\tLoss 0.2733 (0.2858)\tPrec 90.625% (90.096%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.201 (0.201)\tLoss 0.4390 (0.4390)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.330% \n",
      "best acc: 86.270000\n",
      "Epoch 16 — LR: [0.0005]\n",
      "Epoch: [17][0/391]\tTime 0.419 (0.419)\tData 0.389 (0.389)\tLoss 0.1938 (0.1938)\tPrec 92.969% (92.969%)\n",
      "Epoch: [17][100/391]\tTime 0.023 (0.026)\tData 0.001 (0.005)\tLoss 0.2414 (0.2847)\tPrec 89.844% (89.851%)\n",
      "Epoch: [17][200/391]\tTime 0.027 (0.024)\tData 0.007 (0.003)\tLoss 0.3472 (0.2807)\tPrec 89.844% (90.229%)\n",
      "Epoch: [17][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.003)\tLoss 0.3188 (0.2807)\tPrec 89.062% (90.147%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.213 (0.213)\tLoss 0.4794 (0.4794)\tPrec 83.594% (83.594%)\n",
      " * Prec 85.500% \n",
      "best acc: 86.270000\n",
      "Epoch 17 — LR: [0.0005]\n",
      "Epoch: [18][0/391]\tTime 0.374 (0.374)\tData 0.340 (0.340)\tLoss 0.2877 (0.2877)\tPrec 88.281% (88.281%)\n",
      "Epoch: [18][100/391]\tTime 0.023 (0.027)\tData 0.001 (0.005)\tLoss 0.2638 (0.2813)\tPrec 92.969% (90.045%)\n",
      "Epoch: [18][200/391]\tTime 0.022 (0.025)\tData 0.001 (0.003)\tLoss 0.1558 (0.2860)\tPrec 96.094% (89.937%)\n",
      "Epoch: [18][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.003)\tLoss 0.2790 (0.2858)\tPrec 86.719% (89.862%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.192 (0.192)\tLoss 0.4155 (0.4155)\tPrec 85.156% (85.156%)\n",
      " * Prec 85.010% \n",
      "best acc: 86.270000\n",
      "Epoch 18 — LR: [0.0005]\n",
      "Epoch: [19][0/391]\tTime 0.374 (0.374)\tData 0.344 (0.344)\tLoss 0.2653 (0.2653)\tPrec 92.969% (92.969%)\n",
      "Epoch: [19][100/391]\tTime 0.023 (0.027)\tData 0.001 (0.005)\tLoss 0.1945 (0.2819)\tPrec 95.312% (90.099%)\n",
      "Epoch: [19][200/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.2647 (0.2862)\tPrec 90.625% (89.976%)\n",
      "Epoch: [19][300/391]\tTime 0.022 (0.024)\tData 0.001 (0.003)\tLoss 0.2179 (0.2841)\tPrec 92.969% (90.077%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.200 (0.200)\tLoss 0.4062 (0.4062)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.550% \n",
      "best acc: 86.270000\n",
      "Epoch 19 — LR: [0.0005]\n",
      "Epoch: [20][0/391]\tTime 0.386 (0.386)\tData 0.356 (0.356)\tLoss 0.2696 (0.2696)\tPrec 87.500% (87.500%)\n",
      "Epoch: [20][100/391]\tTime 0.023 (0.027)\tData 0.001 (0.005)\tLoss 0.3550 (0.2858)\tPrec 87.500% (89.658%)\n",
      "Epoch: [20][200/391]\tTime 0.022 (0.025)\tData 0.001 (0.003)\tLoss 0.2185 (0.2865)\tPrec 92.188% (89.809%)\n",
      "Epoch: [20][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.003)\tLoss 0.3544 (0.2855)\tPrec 85.938% (89.852%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.4387 (0.4387)\tPrec 85.938% (85.938%)\n",
      " * Prec 85.330% \n",
      "best acc: 86.270000\n",
      "Epoch 20 — LR: [0.0005]\n",
      "Epoch: [21][0/391]\tTime 0.472 (0.472)\tData 0.431 (0.431)\tLoss 0.2464 (0.2464)\tPrec 91.406% (91.406%)\n",
      "Epoch: [21][100/391]\tTime 0.025 (0.028)\tData 0.002 (0.006)\tLoss 0.1904 (0.2861)\tPrec 93.750% (90.215%)\n",
      "Epoch: [21][200/391]\tTime 0.022 (0.026)\tData 0.001 (0.004)\tLoss 0.3153 (0.2828)\tPrec 89.062% (90.248%)\n",
      "Epoch: [21][300/391]\tTime 0.025 (0.025)\tData 0.001 (0.003)\tLoss 0.3021 (0.2824)\tPrec 88.281% (90.215%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.200 (0.200)\tLoss 0.4988 (0.4988)\tPrec 85.156% (85.156%)\n",
      " * Prec 85.660% \n",
      "best acc: 86.270000\n",
      "Epoch 21 — LR: [0.0005]\n",
      "Epoch: [22][0/391]\tTime 0.400 (0.400)\tData 0.368 (0.368)\tLoss 0.3957 (0.3957)\tPrec 87.500% (87.500%)\n",
      "Epoch: [22][100/391]\tTime 0.022 (0.026)\tData 0.001 (0.005)\tLoss 0.2880 (0.2850)\tPrec 90.625% (89.867%)\n",
      "Epoch: [22][200/391]\tTime 0.022 (0.025)\tData 0.001 (0.003)\tLoss 0.4107 (0.2864)\tPrec 85.938% (89.995%)\n",
      "Epoch: [22][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.003)\tLoss 0.2997 (0.2865)\tPrec 86.719% (89.916%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 0.4925 (0.4925)\tPrec 84.375% (84.375%)\n",
      " * Prec 85.270% \n",
      "best acc: 86.270000\n",
      "Epoch 22 — LR: [0.0005]\n",
      "Epoch: [23][0/391]\tTime 0.428 (0.428)\tData 0.394 (0.394)\tLoss 0.2185 (0.2185)\tPrec 92.969% (92.969%)\n",
      "Epoch: [23][100/391]\tTime 0.023 (0.027)\tData 0.001 (0.005)\tLoss 0.2518 (0.2740)\tPrec 89.844% (90.424%)\n",
      "Epoch: [23][200/391]\tTime 0.022 (0.025)\tData 0.001 (0.003)\tLoss 0.2491 (0.2762)\tPrec 90.625% (90.333%)\n",
      "Epoch: [23][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.003)\tLoss 0.2351 (0.2792)\tPrec 95.312% (90.215%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 0.4347 (0.4347)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.100% \n",
      "best acc: 86.270000\n",
      "Epoch 23 — LR: [0.0005]\n",
      "Epoch: [24][0/391]\tTime 0.400 (0.400)\tData 0.336 (0.336)\tLoss 0.4518 (0.4518)\tPrec 82.812% (82.812%)\n",
      "Epoch: [24][100/391]\tTime 0.023 (0.027)\tData 0.001 (0.005)\tLoss 0.2847 (0.2806)\tPrec 88.281% (90.060%)\n",
      "Epoch: [24][200/391]\tTime 0.022 (0.025)\tData 0.001 (0.003)\tLoss 0.2666 (0.2752)\tPrec 92.969% (90.392%)\n",
      "Epoch: [24][300/391]\tTime 0.022 (0.025)\tData 0.001 (0.002)\tLoss 0.3210 (0.2753)\tPrec 88.281% (90.412%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.181 (0.181)\tLoss 0.5717 (0.5717)\tPrec 82.812% (82.812%)\n",
      " * Prec 85.210% \n",
      "best acc: 86.270000\n",
      "Epoch 24 — LR: [0.0005]\n",
      "Epoch: [25][0/391]\tTime 0.301 (0.301)\tData 0.269 (0.269)\tLoss 0.2419 (0.2419)\tPrec 92.188% (92.188%)\n",
      "Epoch: [25][100/391]\tTime 0.022 (0.025)\tData 0.001 (0.004)\tLoss 0.3727 (0.2793)\tPrec 85.156% (89.998%)\n",
      "Epoch: [25][200/391]\tTime 0.022 (0.024)\tData 0.001 (0.003)\tLoss 0.3891 (0.2789)\tPrec 87.500% (90.112%)\n",
      "Epoch: [25][300/391]\tTime 0.022 (0.024)\tData 0.001 (0.002)\tLoss 0.2631 (0.2803)\tPrec 89.844% (90.085%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.203 (0.203)\tLoss 0.4117 (0.4117)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.170% \n",
      "best acc: 86.270000\n",
      "Epoch 25 — LR: [0.0005]\n",
      "Epoch: [26][0/391]\tTime 0.385 (0.385)\tData 0.355 (0.355)\tLoss 0.3376 (0.3376)\tPrec 87.500% (87.500%)\n",
      "Epoch: [26][100/391]\tTime 0.023 (0.027)\tData 0.001 (0.005)\tLoss 0.2245 (0.2806)\tPrec 91.406% (90.114%)\n",
      "Epoch: [26][200/391]\tTime 0.027 (0.025)\tData 0.005 (0.003)\tLoss 0.2580 (0.2787)\tPrec 92.188% (90.213%)\n",
      "Epoch: [26][300/391]\tTime 0.024 (0.024)\tData 0.001 (0.002)\tLoss 0.3582 (0.2806)\tPrec 87.500% (90.140%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.294 (0.294)\tLoss 0.5229 (0.5229)\tPrec 85.938% (85.938%)\n",
      " * Prec 84.530% \n",
      "best acc: 86.270000\n",
      "Epoch 26 — LR: [0.0005]\n",
      "Epoch: [27][0/391]\tTime 0.379 (0.379)\tData 0.349 (0.349)\tLoss 0.2467 (0.2467)\tPrec 90.625% (90.625%)\n",
      "Epoch: [27][100/391]\tTime 0.024 (0.027)\tData 0.001 (0.005)\tLoss 0.3630 (0.2788)\tPrec 87.500% (90.068%)\n",
      "Epoch: [27][200/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.2679 (0.2761)\tPrec 89.844% (90.271%)\n",
      "Epoch: [27][300/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.1975 (0.2764)\tPrec 92.969% (90.350%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.269 (0.269)\tLoss 0.4773 (0.4773)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.610% \n",
      "best acc: 86.270000\n",
      "Epoch 27 — LR: [0.0005]\n",
      "Epoch: [28][0/391]\tTime 0.393 (0.393)\tData 0.355 (0.355)\tLoss 0.2965 (0.2965)\tPrec 89.844% (89.844%)\n",
      "Epoch: [28][100/391]\tTime 0.023 (0.027)\tData 0.001 (0.005)\tLoss 0.2722 (0.2602)\tPrec 92.188% (90.942%)\n",
      "Epoch: [28][200/391]\tTime 0.022 (0.025)\tData 0.001 (0.003)\tLoss 0.2668 (0.2687)\tPrec 89.062% (90.633%)\n",
      "Epoch: [28][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.002)\tLoss 0.3259 (0.2747)\tPrec 87.500% (90.417%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.216 (0.216)\tLoss 0.4112 (0.4112)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.200% \n",
      "best acc: 86.270000\n",
      "Epoch 28 — LR: [0.0005]\n",
      "Epoch: [29][0/391]\tTime 0.458 (0.458)\tData 0.427 (0.427)\tLoss 0.3256 (0.3256)\tPrec 89.062% (89.062%)\n",
      "Epoch: [29][100/391]\tTime 0.022 (0.027)\tData 0.001 (0.006)\tLoss 0.3562 (0.2680)\tPrec 87.500% (90.617%)\n",
      "Epoch: [29][200/391]\tTime 0.022 (0.025)\tData 0.001 (0.004)\tLoss 0.2200 (0.2732)\tPrec 90.625% (90.419%)\n",
      "Epoch: [29][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.003)\tLoss 0.1998 (0.2757)\tPrec 91.406% (90.345%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.299 (0.299)\tLoss 0.5361 (0.5361)\tPrec 83.594% (83.594%)\n",
      " * Prec 85.100% \n",
      "best acc: 86.270000\n",
      "Epoch 29 — LR: [0.0005]\n",
      "Epoch: [30][0/391]\tTime 0.341 (0.341)\tData 0.311 (0.311)\tLoss 0.3505 (0.3505)\tPrec 85.156% (85.156%)\n",
      "Epoch: [30][100/391]\tTime 0.023 (0.027)\tData 0.001 (0.004)\tLoss 0.4487 (0.2739)\tPrec 84.375% (90.207%)\n",
      "Epoch: [30][200/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.2579 (0.2716)\tPrec 90.625% (90.295%)\n",
      "Epoch: [30][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.002)\tLoss 0.2944 (0.2729)\tPrec 89.062% (90.342%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.191 (0.191)\tLoss 0.3981 (0.3981)\tPrec 85.938% (85.938%)\n",
      " * Prec 85.260% \n",
      "best acc: 86.270000\n",
      "Epoch 30 — LR: [0.0005]\n",
      "Epoch: [31][0/391]\tTime 0.393 (0.393)\tData 0.363 (0.363)\tLoss 0.2125 (0.2125)\tPrec 92.188% (92.188%)\n",
      "Epoch: [31][100/391]\tTime 0.022 (0.027)\tData 0.001 (0.005)\tLoss 0.2968 (0.2718)\tPrec 88.281% (90.679%)\n",
      "Epoch: [31][200/391]\tTime 0.022 (0.025)\tData 0.001 (0.003)\tLoss 0.3211 (0.2744)\tPrec 89.844% (90.427%)\n",
      "Epoch: [31][300/391]\tTime 0.022 (0.024)\tData 0.001 (0.003)\tLoss 0.3104 (0.2739)\tPrec 89.062% (90.443%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.182 (0.182)\tLoss 0.4069 (0.4069)\tPrec 85.938% (85.938%)\n",
      " * Prec 85.390% \n",
      "best acc: 86.270000\n",
      "Epoch 31 — LR: [0.0005]\n",
      "Epoch: [32][0/391]\tTime 0.370 (0.370)\tData 0.339 (0.339)\tLoss 0.2969 (0.2969)\tPrec 89.844% (89.844%)\n",
      "Epoch: [32][100/391]\tTime 0.022 (0.027)\tData 0.001 (0.005)\tLoss 0.3096 (0.2687)\tPrec 89.844% (90.532%)\n",
      "Epoch: [32][200/391]\tTime 0.024 (0.025)\tData 0.001 (0.003)\tLoss 0.2970 (0.2699)\tPrec 88.281% (90.388%)\n",
      "Epoch: [32][300/391]\tTime 0.023 (0.025)\tData 0.001 (0.002)\tLoss 0.3477 (0.2711)\tPrec 89.062% (90.399%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.197 (0.197)\tLoss 0.4659 (0.4659)\tPrec 82.031% (82.031%)\n",
      " * Prec 85.690% \n",
      "best acc: 86.270000\n",
      "Epoch 32 — LR: [0.0005]\n",
      "Epoch: [33][0/391]\tTime 0.393 (0.393)\tData 0.362 (0.362)\tLoss 0.3135 (0.3135)\tPrec 90.625% (90.625%)\n",
      "Epoch: [33][100/391]\tTime 0.022 (0.027)\tData 0.001 (0.005)\tLoss 0.2628 (0.2768)\tPrec 90.625% (90.377%)\n",
      "Epoch: [33][200/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.2039 (0.2786)\tPrec 92.969% (90.275%)\n",
      "Epoch: [33][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.003)\tLoss 0.2198 (0.2772)\tPrec 92.188% (90.311%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.188 (0.188)\tLoss 0.4158 (0.4158)\tPrec 85.938% (85.938%)\n",
      " * Prec 85.080% \n",
      "best acc: 86.270000\n",
      "Epoch 33 — LR: [0.0005]\n",
      "Epoch: [34][0/391]\tTime 0.292 (0.292)\tData 0.262 (0.262)\tLoss 0.1472 (0.1472)\tPrec 94.531% (94.531%)\n",
      "Epoch: [34][100/391]\tTime 0.023 (0.025)\tData 0.001 (0.004)\tLoss 0.2766 (0.2679)\tPrec 89.844% (90.710%)\n",
      "Epoch: [34][200/391]\tTime 0.021 (0.024)\tData 0.001 (0.003)\tLoss 0.3438 (0.2643)\tPrec 87.500% (90.695%)\n",
      "Epoch: [34][300/391]\tTime 0.022 (0.024)\tData 0.001 (0.002)\tLoss 0.2615 (0.2684)\tPrec 91.406% (90.560%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.227 (0.227)\tLoss 0.3993 (0.3993)\tPrec 85.938% (85.938%)\n",
      " * Prec 84.930% \n",
      "best acc: 86.270000\n",
      "Epoch 34 — LR: [0.0005]\n",
      "Epoch: [35][0/391]\tTime 0.399 (0.399)\tData 0.368 (0.368)\tLoss 0.3305 (0.3305)\tPrec 89.062% (89.062%)\n",
      "Epoch: [35][100/391]\tTime 0.022 (0.027)\tData 0.001 (0.005)\tLoss 0.2631 (0.2560)\tPrec 89.844% (91.050%)\n",
      "Epoch: [35][200/391]\tTime 0.022 (0.025)\tData 0.001 (0.003)\tLoss 0.3230 (0.2618)\tPrec 91.406% (90.827%)\n",
      "Epoch: [35][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.002)\tLoss 0.2602 (0.2648)\tPrec 92.188% (90.703%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.186 (0.186)\tLoss 0.4589 (0.4589)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.870% \n",
      "best acc: 86.270000\n",
      "Epoch 35 — LR: [0.0005]\n",
      "Epoch: [36][0/391]\tTime 0.388 (0.388)\tData 0.357 (0.357)\tLoss 0.2567 (0.2567)\tPrec 90.625% (90.625%)\n",
      "Epoch: [36][100/391]\tTime 0.022 (0.027)\tData 0.001 (0.005)\tLoss 0.2582 (0.2694)\tPrec 90.625% (90.509%)\n",
      "Epoch: [36][200/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.2012 (0.2651)\tPrec 94.531% (90.676%)\n",
      "Epoch: [36][300/391]\tTime 0.029 (0.024)\tData 0.005 (0.003)\tLoss 0.2656 (0.2652)\tPrec 91.406% (90.641%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.205 (0.205)\tLoss 0.3954 (0.3954)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.710% \n",
      "best acc: 86.270000\n",
      "Epoch 36 — LR: [0.0005]\n",
      "Epoch: [37][0/391]\tTime 0.363 (0.363)\tData 0.332 (0.332)\tLoss 0.2777 (0.2777)\tPrec 92.969% (92.969%)\n",
      "Epoch: [37][100/391]\tTime 0.023 (0.027)\tData 0.001 (0.005)\tLoss 0.3220 (0.2657)\tPrec 88.281% (90.865%)\n",
      "Epoch: [37][200/391]\tTime 0.021 (0.025)\tData 0.001 (0.003)\tLoss 0.2637 (0.2692)\tPrec 92.188% (90.641%)\n",
      "Epoch: [37][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.002)\tLoss 0.2098 (0.2682)\tPrec 90.625% (90.641%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.200 (0.200)\tLoss 0.4559 (0.4559)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.420% \n",
      "best acc: 86.270000\n",
      "Epoch 37 — LR: [0.0005]\n",
      "Epoch: [38][0/391]\tTime 0.346 (0.346)\tData 0.315 (0.315)\tLoss 0.4908 (0.4908)\tPrec 83.594% (83.594%)\n",
      "Epoch: [38][100/391]\tTime 0.023 (0.026)\tData 0.001 (0.004)\tLoss 0.2251 (0.2602)\tPrec 90.625% (91.012%)\n",
      "Epoch: [38][200/391]\tTime 0.022 (0.024)\tData 0.001 (0.003)\tLoss 0.2891 (0.2641)\tPrec 90.625% (90.920%)\n",
      "Epoch: [38][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.002)\tLoss 0.1987 (0.2638)\tPrec 96.094% (90.861%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.248 (0.248)\tLoss 0.4488 (0.4488)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.960% \n",
      "best acc: 86.270000\n",
      "Epoch 38 — LR: [0.0005]\n",
      "Epoch: [39][0/391]\tTime 0.364 (0.364)\tData 0.333 (0.333)\tLoss 0.2013 (0.2013)\tPrec 92.188% (92.188%)\n",
      "Epoch: [39][100/391]\tTime 0.023 (0.027)\tData 0.001 (0.005)\tLoss 0.2579 (0.2633)\tPrec 92.188% (90.795%)\n",
      "Epoch: [39][200/391]\tTime 0.022 (0.025)\tData 0.001 (0.003)\tLoss 0.2053 (0.2657)\tPrec 92.188% (90.633%)\n",
      "Epoch: [39][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.002)\tLoss 0.2912 (0.2613)\tPrec 90.625% (90.835%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.219 (0.219)\tLoss 0.4638 (0.4638)\tPrec 84.375% (84.375%)\n",
      " * Prec 86.030% \n",
      "best acc: 86.270000\n",
      "Epoch 39 — LR: [0.0005]\n",
      "Epoch: [40][0/391]\tTime 0.328 (0.328)\tData 0.299 (0.299)\tLoss 0.2684 (0.2684)\tPrec 89.062% (89.062%)\n",
      "Epoch: [40][100/391]\tTime 0.023 (0.026)\tData 0.001 (0.004)\tLoss 0.3670 (0.2692)\tPrec 87.500% (90.285%)\n",
      "Epoch: [40][200/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.2805 (0.2677)\tPrec 89.844% (90.489%)\n",
      "Epoch: [40][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.002)\tLoss 0.3459 (0.2697)\tPrec 85.156% (90.384%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.204 (0.204)\tLoss 0.5327 (0.5327)\tPrec 85.938% (85.938%)\n",
      " * Prec 85.410% \n",
      "best acc: 86.270000\n",
      "Epoch 40 — LR: [0.0005]\n",
      "Epoch: [41][0/391]\tTime 0.382 (0.382)\tData 0.351 (0.351)\tLoss 0.2404 (0.2404)\tPrec 87.500% (87.500%)\n",
      "Epoch: [41][100/391]\tTime 0.022 (0.027)\tData 0.001 (0.005)\tLoss 0.2578 (0.2617)\tPrec 92.188% (90.687%)\n",
      "Epoch: [41][200/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.2890 (0.2669)\tPrec 92.969% (90.590%)\n",
      "Epoch: [41][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.003)\tLoss 0.2549 (0.2644)\tPrec 92.188% (90.674%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.195 (0.195)\tLoss 0.4252 (0.4252)\tPrec 85.156% (85.156%)\n",
      " * Prec 85.090% \n",
      "best acc: 86.270000\n",
      "Epoch 41 — LR: [0.0005]\n",
      "Epoch: [42][0/391]\tTime 0.393 (0.393)\tData 0.335 (0.335)\tLoss 0.3980 (0.3980)\tPrec 85.156% (85.156%)\n",
      "Epoch: [42][100/391]\tTime 0.023 (0.027)\tData 0.001 (0.005)\tLoss 0.2587 (0.2636)\tPrec 91.406% (90.865%)\n",
      "Epoch: [42][200/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.2330 (0.2631)\tPrec 93.750% (90.784%)\n",
      "Epoch: [42][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.002)\tLoss 0.2122 (0.2633)\tPrec 93.750% (90.731%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.194 (0.194)\tLoss 0.4513 (0.4513)\tPrec 85.156% (85.156%)\n",
      " * Prec 86.030% \n",
      "best acc: 86.270000\n",
      "Epoch 42 — LR: [0.0005]\n",
      "Epoch: [43][0/391]\tTime 0.354 (0.354)\tData 0.324 (0.324)\tLoss 0.2807 (0.2807)\tPrec 89.062% (89.062%)\n",
      "Epoch: [43][100/391]\tTime 0.022 (0.026)\tData 0.001 (0.005)\tLoss 0.3127 (0.2564)\tPrec 87.500% (90.818%)\n",
      "Epoch: [43][200/391]\tTime 0.022 (0.025)\tData 0.001 (0.003)\tLoss 0.1928 (0.2624)\tPrec 95.312% (90.555%)\n",
      "Epoch: [43][300/391]\tTime 0.027 (0.024)\tData 0.005 (0.003)\tLoss 0.3265 (0.2632)\tPrec 90.625% (90.576%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.286 (0.286)\tLoss 0.4586 (0.4586)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.850% \n",
      "best acc: 86.270000\n",
      "Epoch 43 — LR: [0.0005]\n",
      "Epoch: [44][0/391]\tTime 0.313 (0.313)\tData 0.283 (0.283)\tLoss 0.2959 (0.2959)\tPrec 89.062% (89.062%)\n",
      "Epoch: [44][100/391]\tTime 0.022 (0.026)\tData 0.001 (0.004)\tLoss 0.3275 (0.2513)\tPrec 88.281% (91.120%)\n",
      "Epoch: [44][200/391]\tTime 0.023 (0.024)\tData 0.001 (0.003)\tLoss 0.3403 (0.2567)\tPrec 89.062% (91.103%)\n",
      "Epoch: [44][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.002)\tLoss 0.2368 (0.2588)\tPrec 92.188% (90.960%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.208 (0.208)\tLoss 0.4481 (0.4481)\tPrec 85.938% (85.938%)\n",
      " * Prec 85.610% \n",
      "best acc: 86.270000\n",
      "Epoch 44 — LR: [0.0005]\n",
      "Epoch: [45][0/391]\tTime 0.369 (0.369)\tData 0.338 (0.338)\tLoss 0.2231 (0.2231)\tPrec 92.969% (92.969%)\n",
      "Epoch: [45][100/391]\tTime 0.023 (0.027)\tData 0.001 (0.005)\tLoss 0.3295 (0.2552)\tPrec 88.281% (91.151%)\n",
      "Epoch: [45][200/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.2877 (0.2583)\tPrec 87.500% (90.959%)\n",
      "Epoch: [45][300/391]\tTime 0.022 (0.024)\tData 0.001 (0.002)\tLoss 0.3720 (0.2588)\tPrec 85.938% (90.942%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.205 (0.205)\tLoss 0.4242 (0.4242)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.760% \n",
      "best acc: 86.270000\n",
      "Epoch 45 — LR: [0.0005]\n",
      "Epoch: [46][0/391]\tTime 0.373 (0.373)\tData 0.344 (0.344)\tLoss 0.3145 (0.3145)\tPrec 89.062% (89.062%)\n",
      "Epoch: [46][100/391]\tTime 0.022 (0.027)\tData 0.001 (0.005)\tLoss 0.2446 (0.2592)\tPrec 89.062% (90.787%)\n",
      "Epoch: [46][200/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.1619 (0.2588)\tPrec 93.750% (91.041%)\n",
      "Epoch: [46][300/391]\tTime 0.022 (0.024)\tData 0.001 (0.002)\tLoss 0.1812 (0.2599)\tPrec 93.750% (90.978%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.192 (0.192)\tLoss 0.4913 (0.4913)\tPrec 85.156% (85.156%)\n",
      " * Prec 85.430% \n",
      "best acc: 86.270000\n",
      "Epoch 46 — LR: [0.0005]\n",
      "Epoch: [47][0/391]\tTime 0.406 (0.406)\tData 0.375 (0.375)\tLoss 0.2084 (0.2084)\tPrec 92.188% (92.188%)\n",
      "Epoch: [47][100/391]\tTime 0.022 (0.027)\tData 0.001 (0.005)\tLoss 0.2243 (0.2626)\tPrec 92.188% (90.625%)\n",
      "Epoch: [47][200/391]\tTime 0.022 (0.025)\tData 0.001 (0.003)\tLoss 0.2139 (0.2613)\tPrec 92.969% (90.745%)\n",
      "Epoch: [47][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.003)\tLoss 0.1727 (0.2626)\tPrec 95.312% (90.672%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.186 (0.186)\tLoss 0.3588 (0.3588)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.340% \n",
      "best acc: 86.340000\n",
      "Epoch 47 — LR: [0.0005]\n",
      "Epoch: [48][0/391]\tTime 0.383 (0.383)\tData 0.353 (0.353)\tLoss 0.2430 (0.2430)\tPrec 91.406% (91.406%)\n",
      "Epoch: [48][100/391]\tTime 0.023 (0.027)\tData 0.001 (0.005)\tLoss 0.2169 (0.2631)\tPrec 93.750% (90.679%)\n",
      "Epoch: [48][200/391]\tTime 0.023 (0.025)\tData 0.001 (0.003)\tLoss 0.2974 (0.2601)\tPrec 90.625% (90.924%)\n",
      "Epoch: [48][300/391]\tTime 0.022 (0.024)\tData 0.001 (0.002)\tLoss 0.2706 (0.2563)\tPrec 89.844% (91.123%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.199 (0.199)\tLoss 0.4187 (0.4187)\tPrec 84.375% (84.375%)\n",
      " * Prec 86.290% \n",
      "best acc: 86.340000\n",
      "Epoch 48 — LR: [0.0005]\n",
      "Epoch: [49][0/391]\tTime 0.375 (0.375)\tData 0.345 (0.345)\tLoss 0.3671 (0.3671)\tPrec 82.812% (82.812%)\n",
      "Epoch: [49][100/391]\tTime 0.023 (0.026)\tData 0.001 (0.005)\tLoss 0.2669 (0.2624)\tPrec 89.844% (90.780%)\n",
      "Epoch: [49][200/391]\tTime 0.022 (0.025)\tData 0.001 (0.003)\tLoss 0.2123 (0.2582)\tPrec 92.969% (91.068%)\n",
      "Epoch: [49][300/391]\tTime 0.023 (0.024)\tData 0.001 (0.003)\tLoss 0.3052 (0.2582)\tPrec 91.406% (91.038%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.190 (0.190)\tLoss 0.4299 (0.4299)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.060% \n",
      "best acc: 86.340000\n",
      "Epoch 49 — LR: [0.0005]\n"
     ]
    }
   ],
   "source": [
    "lr = .0005\n",
    "weight_decay = .0001\n",
    "epochs = 50\n",
    "best_prec = 0\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "base_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=epochs\n",
    ")\n",
    "\n",
    "scheduler = GradualWarmupScheduler(\n",
    "    optimizer,\n",
    "    multiplier=1,  \n",
    "    total_epoch=10,  \n",
    "    after_scheduler=base_scheduler\n",
    ")\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'+str(model_name)\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "        \n",
    "for epoch in range(0, epochs):\n",
    "    \n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)\n",
    "\n",
    "    # NEW\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch} — LR: {scheduler.get_last_lr()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 8634/10000 (86%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/project_resnet/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceramic-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send an input and grap the value by using prehook like HW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "spoken-worst",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# choose the first 8x8 QuantConv2d in layer3\n",
    "layer = model.layer3[0].conv2\n",
    "# print(layer)\n",
    "\n",
    "w_bit = 4\n",
    "weight_q     = layer.weight_q.detach()                           # quantized weights\n",
    "weight_alpha = layer.weight_quant.state_dict()['wgt_alpha']      # alpha from layer\n",
    "weight_delta = weight_alpha / ((2**(w_bit - 1)) - 1)\n",
    "weight_int   = torch.round(weight_q / weight_delta).to(torch.int8)\n",
    "\n",
    "# optional: sanity check\n",
    "print(weight_int.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "interior-oxygen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([128, 8, 8, 8])\n",
      "x_int.shape: torch.Size([128, 8, 8, 8])\n",
      "x_int min/max: 0 15\n"
     ]
    }
   ],
   "source": [
    "# === grab activation of the 1st 8x8 QuantConv2d in layer3 and quantize it ===\n",
    "x_bit = 4\n",
    "activation = {}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    # input is a tuple: (x,)\n",
    "    activation[\"x\"] = input[0].detach()   # keep on CPU for now\n",
    "\n",
    "# register hook on the chosen conv layer\n",
    "hook_handle = layer.register_forward_hook(hook_fn)\n",
    "\n",
    "# run one batch through the model to populate activation[\"x\"]\n",
    "data_iter = iter(testloader)\n",
    "images, targets = next(data_iter)        # CIFAR-10 batch\n",
    "images = images.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(images)\n",
    "\n",
    "# remove hook so it doesn't affect later runs\n",
    "hook_handle.remove()\n",
    "\n",
    "# ----- quantize the captured activation -----\n",
    "x = activation[\"x\"].to(device)           # input of the 2nd conv in layer3[0]\n",
    "print(\"x.shape:\", x.shape)\n",
    "\n",
    "x_alpha = layer.act_alpha.item()\n",
    "x_delta = x_alpha / (2 ** x_bit - 1)\n",
    "\n",
    "act_quant_fn = act_quantization(x_bit)   # from quant_layer.py\n",
    "x_q = act_quant_fn(x, x_alpha)           # quantized activation (float)\n",
    "x_int = torch.round(x_q / x_delta).to(torch.int32)\n",
    "\n",
    "print(\"x_int.shape:\", x_int.shape)\n",
    "print(\"x_int min/max:\", x_int.min().item(), x_int.max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ranging-porter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "output_recovered: torch.Size([128, 8, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "in_ch, out_ch = layer.in_channels, layer.out_channels\n",
    "k = layer.kernel_size[0]\n",
    "pad = layer.padding[0]\n",
    "stride = layer.stride[0]\n",
    "\n",
    "conv_int = torch.nn.Conv2d(in_ch, out_ch, kernel_size=k, stride=stride, padding=pad, bias=False).cuda()\n",
    "print(conv_int)\n",
    "conv_int.weight = torch.nn.Parameter(weight_int.to(torch.float32))\n",
    "\n",
    "output_int = conv_int(x_int.to(torch.float32))           # output_int can be calculated with conv_int and x_int\n",
    "output_recovered = output_int * (x_delta * weight_delta) # recover with x_delta and w_delta\n",
    "\n",
    "print(\"output_recovered:\", output_recovered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "designed-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### input floating number / weight quantized version\n",
    "conv_ref = torch.nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False).cuda()\n",
    "conv_ref.weight = torch.nn.Parameter(weight_q)\n",
    "output_ref = conv_ref(x)\n",
    "# print(output_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "157dffd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6927454471588135\n"
     ]
    }
   ],
   "source": [
    "difference = abs(output_ref - output_recovered)\n",
    "print(difference.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e706f963-d3d7-4634-9d9f-632ca2f782fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
